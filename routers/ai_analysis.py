from fastapi import APIRouter, Request, Query, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone, timedelta
from pymongo import MongoClient
import json
import logging
from docs.investment_ai.data_scheduler import get_data_status, get_recovery_status, get_ai_api_status_summary

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="/app/trading_bot/templates")

class AIAnalysisViewer:
    """AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client = MongoClient("mongodb://mongodb:27017")
        self.database = self.client["bitcoin"]
        self.cache_collection = self.database["data_cache"]
    
    def get_analysis_tasks(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ë¶„ì„ ì‘ì—… ëª©ë¡ ë°˜í™˜"""
        return [
            "ai_sentiment_analysis",
            "ai_technical_analysis", 
            "ai_macro_analysis",
            "ai_onchain_analysis",
            "ai_institutional_analysis",
            "final_decision"
        ]

    def sort_by_latest_first(self, analyses: List[Dict]) -> List[Dict]:
        """ë¶„ì„ê¸°ë³„ ìµœì‹  ë°ì´í„°ë¥¼ ë§¨ ìœ„ë¡œ ì´ë™"""
        if not analyses:
            return analyses
        
        # ê° ë¶„ì„ê¸°ë³„ ìµœì‹  ë°ì´í„° ì°¾ê¸°
        latest_by_task = {}
        other_analyses = []
        
        for analysis in analyses:
            task_name = analysis.get('task_name')
            if task_name not in latest_by_task:
                # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ê²ƒì´ ìµœì‹  (ì´ë¯¸ ì‹œê°„ìˆœ ì—­ìˆœ ì •ë ¬ë˜ì–´ ìˆìŒ)
                latest_by_task[task_name] = analysis
            else:
                # ìµœì‹ ì´ ì•„ë‹Œ ë‚˜ë¨¸ì§€ë“¤
                other_analyses.append(analysis)
        
        # ìµœì‹  ë°ì´í„°ë“¤ì„ ë§¨ ìœ„ë¡œ, ë‚˜ë¨¸ì§€ëŠ” ì‹œê°„ìˆœìœ¼ë¡œ ìœ ì§€
        latest_analyses = list(latest_by_task.values())
        # ìµœì‹  ë°ì´í„°ë“¤ë„ ì‹œê°„ìˆœ ì •ë ¬
        latest_analyses.sort(key=lambda x: x.get('created_at', datetime.min), reverse=True)
        
        return latest_analyses + other_analyses

    def get_task_display_name(self, task_name: str) -> str:
        """ì‘ì—…ëª…ì„ í‘œì‹œìš© ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        name_mapping = {
            "ai_sentiment_analysis": "ì‹œì¥ ê°ì • ë¶„ì„",
            "ai_technical_analysis": "ê¸°ìˆ ì  ë¶„ì„",
            "ai_macro_analysis": "ê±°ì‹œê²½ì œ ë¶„ì„", 
            "ai_onchain_analysis": "ì˜¨ì²´ì¸ ë¶„ì„",
            "ai_institutional_analysis": "ê¸°ê´€íˆ¬ì ë¶„ì„",
            "final_decision": "ìµœì¢… AI íˆ¬ì ê²°ì •"
        }
        return name_mapping.get(task_name, task_name)
    
    def get_recent_analyses(self, task_name: Optional[str] = None, hours: int = 24, limit: int = 50, for_blog: bool = False) -> List[Dict]:
        """ìµœê·¼ AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        try:
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            since_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            
            # ì¿¼ë¦¬ ì¡°ê±´ êµ¬ì„±
            query = {
                "created_at": {"$gte": since_time}
            }
            
            if task_name:
                query["task_name"] = task_name
            else:
                # AI ë¶„ì„ ì‘ì—…ë§Œ ì¡°íšŒ
                query["task_name"] = {"$in": self.get_analysis_tasks()}
            
            # ë°ì´í„° ì¡°íšŒ
            cursor = self.cache_collection.find(query).sort("created_at", -1).limit(limit)
            
            results = []
            for doc in cursor:
                try:
                    # ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                    analysis_data = doc.get("data", {})

                    has_success = analysis_data.get("success", False)
                    has_result = "result" in analysis_data and analysis_data["result"] is not None

                    # ê¸°ë³¸ ì •ë³´
                    result = {
                        "task_name": doc.get("task_name"),
                        "display_name": self.get_task_display_name(doc.get("task_name", "")),
                        "created_at": doc.get("created_at"),
                        "expire_at": doc.get("expire_at"),
                        "success": has_success and has_result,  # ë‘˜ ë‹¤ ìˆì–´ì•¼ ì„±ê³µ
                        "analysis_result": analysis_data,                # âœ… ì „ì²´ ë°ì´í„° ì €ì¥
                        "raw_data": analysis_data
                    }

                    # ì„±ê³µí•œ ë¶„ì„ì˜ ê²½ìš° ìš”ì•½ ì •ë³´ ì¶”ì¶œ  
                    if result["success"] and "result" in analysis_data:
                        # ë¸”ë¡œê·¸ìš©ì¸ì§€ ì¼ë°˜ìš©ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ìš”ì•½ í•¨ìˆ˜ ì‚¬ìš©
                        if for_blog:
                            result["summary"] = self._extract_blog_summary(doc.get("task_name"), analysis_data.get("result", {}))
                        else:
                            result["summary"] = self._extract_summary(doc.get("task_name"), analysis_data.get("result", {}))
                    else:
                        # ì‹¤íŒ¨í•œ ë¶„ì„ì˜ ê²½ìš° ì‹¤íŒ¨ ì •ë³´ ì¶”ì¶œ
                        result["failure_info"] = self._extract_failure_info(analysis_data)

                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return self.sort_by_latest_first(results)
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_summary(self, task_name: str, analysis_result: Dict) -> Dict:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
        summary = {
            "confidence": analysis_result.get("confidence", 0),
            "key_points": [],
            "recommendation": "ë¶„ì„ ë¶ˆê°€"
        }
        
        try:
            if task_name == "ai_sentiment_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "sentiment_score": analysis_result.get("market_sentiment_score", 50),
                    "sentiment_state": analysis_result.get("sentiment_state", "ì¤‘ë¦½"),
                    "recommendation": analysis_result.get("investment_recommendation", "ì¤‘ë¦½ì  ì ‘ê·¼")
                })
                
            elif task_name == "ai_technical_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "signal": analysis_result.get("overall_signal", "Hold"),
                    "trend": analysis_result.get("trend_analysis", {}).get("trend_direction", "ì¤‘ë¦½"),
                    "momentum": analysis_result.get("momentum_analysis", {}).get("momentum_direction", "ì¤‘ë¦½"),
                    "recommendation": f"{analysis_result.get('overall_signal', 'Hold')} - {analysis_result.get('trend_analysis', {}).get('trend_direction', 'ì¤‘ë¦½')} ì¶”ì„¸"
                })
                
            elif task_name == "ai_macro_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "macro_score": analysis_result.get("macro_environment_score", 50),
                    "investment_environment": analysis_result.get("investment_environment", "ì¤‘ë¦½"),
                    "recommendation": analysis_result.get("btc_recommendation", "ì‹ ì¤‘í•œ ì ‘ê·¼")
                })
                
            elif task_name == "ai_onchain_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "health_score": analysis_result.get("onchain_health_score", 50),
                    "signal": analysis_result.get("investment_signal", "Hold"),
                    "network_security": analysis_result.get("network_security_analysis", "ì •ìƒ"),
                    "recommendation": analysis_result.get("investment_signal", "ì¤‘ë¦½ì ")
                })
                
            elif task_name == "ai_institutional_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "flow_score": analysis_result.get("institutional_flow_score", 50),
                    "signal": analysis_result.get("investment_signal", "Hold"),
                    "flow_direction": "ë¶„ì‚°" if "Sell" in analysis_result.get("investment_signal", "") else "ì¤‘ë¦½",
                    "recommendation": analysis_result.get("investment_signal", "ê´€ë§")
                })
                
            elif task_name == "final_decision":
                def safe_slice(text, length=100):
                    if text and isinstance(text, str):
                        return text[:length] + "..." if len(text) > length else text
                    return "ë¶„ì„ ì¤‘"
                summary.update({
                    "confidence": analysis_result.get("decision_confidence", 0),
                    "decision": analysis_result.get("final_decision", "Hold"),
                    "action": analysis_result.get("recommended_action", {}).get("action_type", "Hold"),
                    "recommendation": safe_slice(analysis_result.get("decision_reasoning")),
                    "needs_human_review": analysis_result.get("needs_human_review", False),
                    "human_review_reason": safe_slice(analysis_result.get("human_review_reason"))
                })
            
            # ê³µí†µ í‚¤ í¬ì¸íŠ¸ ì¶”ì¶œ
            if "analysis_summary" in analysis_result:
                summary["key_points"] = [analysis_result["analysis_summary"][:200] + "..."]
                
        except Exception as e:
            logger.error(f"ìš”ì•½ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return summary
    
    def _extract_blog_summary(self, task_name: str, analysis_result: Dict) -> Dict:
        """ë¸”ë¡œê·¸ìš© ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ì—†ìŒ)"""
        summary = {
            "confidence": analysis_result.get("confidence", 0),
            "key_points": [],
            "recommendation": "ë¶„ì„ ë¶ˆê°€"
        }
        
        try:
            if task_name == "ai_sentiment_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "sentiment_score": analysis_result.get("market_sentiment_score", 50),
                    "sentiment_state": analysis_result.get("sentiment_state", "ì¤‘ë¦½"),
                    "recommendation": analysis_result.get("investment_recommendation", "ì¤‘ë¦½ì  ì ‘ê·¼")
                })
                
            elif task_name == "ai_technical_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "signal": analysis_result.get("overall_signal", "Hold"),
                    "trend": analysis_result.get("trend_analysis", {}).get("trend_direction", "ì¤‘ë¦½"),
                    "momentum": analysis_result.get("momentum_analysis", {}).get("momentum_direction", "ì¤‘ë¦½"),
                    "recommendation": f"{analysis_result.get('overall_signal', 'Hold')} - {analysis_result.get('trend_analysis', {}).get('trend_direction', 'ì¤‘ë¦½')} ì¶”ì„¸"
                })
                
            elif task_name == "ai_macro_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "macro_score": analysis_result.get("macro_environment_score", 50),
                    "investment_environment": analysis_result.get("investment_environment", "ì¤‘ë¦½"),
                    "recommendation": analysis_result.get("btc_recommendation", "ì‹ ì¤‘í•œ ì ‘ê·¼")
                })
                
            elif task_name == "ai_onchain_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "health_score": analysis_result.get("onchain_health_score", 50),
                    "signal": analysis_result.get("investment_signal", "Hold"),
                    "network_security": analysis_result.get("network_security_analysis", "ì •ìƒ"),
                    "recommendation": analysis_result.get("investment_signal", "ì¤‘ë¦½ì ")
                })
                
            elif task_name == "ai_institutional_analysis":
                summary.update({
                    "confidence": analysis_result.get("confidence", 0),
                    "flow_score": analysis_result.get("institutional_flow_score", 50),
                    "signal": analysis_result.get("investment_signal", "Hold"),
                    "flow_direction": "ë¶„ì‚°" if "Sell" in analysis_result.get("investment_signal", "") else "ì¤‘ë¦½",
                    "recommendation": analysis_result.get("investment_signal", "ê´€ë§")
                })
                
            elif task_name == "final_decision":
                summary.update({
                    "confidence": analysis_result.get("decision_confidence", 0),
                    "decision": analysis_result.get("final_decision", "Hold"),
                    "action": analysis_result.get("recommended_action", {}).get("action_type", "Hold"),
                    "recommendation": analysis_result.get("decision_reasoning", "ë¶„ì„ ì¤‘"),  # ê¸¸ì´ ì œí•œ ì œê±°
                    "needs_human_review": analysis_result.get("needs_human_review", False),
                    "human_review_reason": analysis_result.get("human_review_reason", "")  # ê¸¸ì´ ì œí•œ ì œê±°
                })
            
            # ê³µí†µ í‚¤ í¬ì¸íŠ¸ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
            if "analysis_summary" in analysis_result:
                summary["key_points"] = [analysis_result["analysis_summary"]]  # ì „ì²´ ë‚´ìš© ì‚¬ìš©
                
        except Exception as e:
            logger.error(f"ë¸”ë¡œê·¸ìš© ìš”ì•½ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return summary


    def _extract_failure_info(self, analysis_result: Dict) -> Dict:
        """ì‹¤íŒ¨í•œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤íŒ¨ ì •ë³´ ì¶”ì¶œ"""
        failure_info = {
            "error_type": "unknown",
            "error_message": "ë¶„ì„ ì‹¤íŒ¨",
            "skip_reason": None,
            "error_count": 0,
            "max_errors": 3,
            "retry_available": True
        }
        
        try:
            if isinstance(analysis_result, dict):
                failure_info.update({
                    "error_type": analysis_result.get("skip_reason", "unknown"),
                    "error_message": analysis_result.get("error", "ë¶„ì„ ì‹¤íŒ¨"),
                    "skip_reason": analysis_result.get("skip_reason"),
                    "error_count": analysis_result.get("error_count", 0),
                    "max_errors": analysis_result.get("max_errors", 3),
                    "retry_available": analysis_result.get("error_count", 0) < analysis_result.get("max_errors", 3)
                })
                
                # ì‹¤íŒ¨ ìœ í˜•ë³„ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€
                skip_reason = analysis_result.get("skip_reason", "")
                if skip_reason == "insufficient_raw_data":
                    failure_info["user_message"] = "ì›ì‹œ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"
                elif skip_reason == "ai_analysis_failed":
                    failure_info["user_message"] = "AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨"
                elif skip_reason == "exception":
                    failure_info["user_message"] = "ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ"
                elif skip_reason == "execution_failed":
                    failure_info["user_message"] = "ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨"
                elif skip_reason == "analyzer_disabled":
                    failure_info["user_message"] = "ë¶„ì„ê¸° ë¹„í™œì„±í™” (ì—°ì† ì‹¤íŒ¨)"
                else:
                    failure_info["user_message"] = failure_info["error_message"]
                    
        except Exception as e:
            logger.error(f"ì‹¤íŒ¨ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return failure_info
    
    def get_analysis_detail(self, task_name: str, created_at: datetime) -> Optional[Dict]:
        """íŠ¹ì • ë¶„ì„ ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì •í™•í•œ ì‹œê°„ ë§¤ì¹­ì„ ìœ„í•´ ë²”ìœ„ ê²€ìƒ‰
            time_range = timedelta(seconds=30)
            start_time = created_at - time_range
            end_time = created_at + time_range
            
            doc = self.cache_collection.find_one({
                "task_name": task_name,
                "created_at": {
                    "$gte": start_time,
                    "$lte": end_time
                }
            })
            
            if not doc:
                return None
            
            return {
                "task_name": task_name,
                "display_name": self.get_task_display_name(task_name),
                "created_at": doc.get("created_at"),
                "data": doc.get("data", {}),
                "formatted_data": json.dumps(doc.get("data", {}), indent=2, ensure_ascii=False, default=str)
            }
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def get_analysis_statistics(self, hours: int = 24) -> Dict:
        """AI ë¶„ì„ í†µê³„ ì •ë³´"""
        try:
            since_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            
            stats = {
                "total_analyses": 0,
                "success_count": 0,
                "failure_count": 0,
                "by_analyzer": {},
                "success_rate": 0
            }
            
            # ê° ë¶„ì„ê¸°ë³„ í†µê³„
            for task_name in self.get_analysis_tasks():
                cursor = self.cache_collection.find({
                    "task_name": task_name,
                    "created_at": {"$gte": since_time}
                })
                
                task_total = 0
                task_success = 0
                
                for doc in cursor:
                    task_total += 1
                    analysis_data = doc.get("data", {})
                    
                    # ğŸ”§ ìˆ˜ì •: ì˜¬ë°”ë¥¸ success ì²´í¬
                    if analysis_data.get("success", False):  # analysis_result ì œê±°!
                        task_success += 1
                
                stats["by_analyzer"][task_name] = {
                    "display_name": self.get_task_display_name(task_name),
                    "total": task_total,
                    "success": task_success,
                    "failure": task_total - task_success,
                    "success_rate": (task_success / task_total * 100) if task_total > 0 else 0
                }
                
                stats["total_analyses"] += task_total
                stats["success_count"] += task_success
            
            stats["failure_count"] = stats["total_analyses"] - stats["success_count"]
            stats["success_rate"] = (stats["success_count"] / stats["total_analyses"] * 100) if stats["total_analyses"] > 0 else 0
            
            return stats
            
        except Exception as e:
            logger.error(f"ë¶„ì„ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {"total_analyses": 0, "success_count": 0, "failure_count": 0, "by_analyzer": {}, "success_rate": 0}

    # ì™¸ë¶€ ì „ì†¡ìš© ì°¨íŠ¸ ë°ì´í„° (72ì‹œê°„ë´‰)
    def get_chart_data(self, hours: int = 300) -> List[Dict]:
        """ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ì¡°íšŒ (1ì‹œê°„ë´‰ ê¸°ì¤€, ë¶ˆì™„ì „í•œ ë§ˆì§€ë§‰ ë°ì´í„° ì œì™¸)"""
        try:
            # ì‹œê°„ ë²”ìœ„ ì„¤ì • (73ì‹œê°„ ê°€ì ¸ì™€ì„œ ë§ˆì§€ë§‰ 1ê°œ ì œì™¸)
            since_time = datetime.now(timezone.utc) - timedelta(hours=hours + 1)
            
            # chart_60m ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ì¡°íšŒ
            chart_collection = self.database["chart_60m"]
            
            cursor = chart_collection.find({
                "timestamp": {"$gte": since_time}
            }).sort("timestamp", 1).limit(hours + 1)  # 73ê°œ ê°€ì ¸ì˜¤ê¸°
            
            chart_data = []
            for doc in cursor:
                chart_data.append({
                    "timestamp": doc.get("timestamp"),
                    "open": float(doc.get("open", 0)),
                    "high": float(doc.get("high", 0)), 
                    "low": float(doc.get("low", 0)),
                    "close": float(doc.get("close", 0)),
                    "volume": float(doc.get("volume", 0))
                })
            
            # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°ì´í„° ì œê±°
            if len(chart_data) > hours:
                chart_data = chart_data[:-1]  # ë§ˆì§€ë§‰ 1ê°œ ì œê±°
            
            return chart_data
            
        except Exception as e:
            logger.error(f"ì°¨íŠ¸ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []


    def _filter_analysis_for_blog(self, analysis_data):
        """ë¸”ë¡œê·¸ìš©ìœ¼ë¡œ ë¶„ì„ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜"""
        filtered_data = {}
        
        # ê¸°ë³¸ ì •ë³´ ìœ ì§€
        basic_fields = ['task_name', 'display_name', 'created_at', 'success']
        for field in basic_fields:
            if field in analysis_data:
                filtered_data[field] = analysis_data[field]
        
        # analysis_resultê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if 'analysis_result' in analysis_data and analysis_data['analysis_result'].get('success'):
            result = analysis_data['analysis_result']['result']
            filtered_result = {}
            
            # ë¶„ì„ íƒ€ì…ë³„ë¡œ í•„í„°ë§
            task_name = analysis_data.get('task_name', '')
            
            if task_name == 'final_decision':
                # ìµœì¢… ê²°ì • - í•µì‹¬ ì •ë³´ë§Œ
                filtered_result = {
                    'final_decision': result.get('final_decision'),
                    'decision_confidence': result.get('decision_confidence'),
                    'market_outlook': result.get('market_outlook', {}),
                    'decision_reasoning': result.get('decision_reasoning'),
                    'signal_consensus': {
                        'consensus_level': result.get('signal_consensus', {}).get('consensus_level'),
                        'conflicting_signals': result.get('signal_consensus', {}).get('conflicting_signals', []),
                        'dominant_signal_source': result.get('signal_consensus', {}).get('dominant_signal_source')
                    },
                    'confidence_factors': {
                        'supporting_factors': result.get('confidence_factors', {}).get('supporting_factors', []),
                        'risk_factors': result.get('confidence_factors', {}).get('risk_factors', [])
                    }
                }
                
            elif task_name == 'ai_technical_analysis':
                # ê¸°ìˆ ì  ë¶„ì„ - ìƒì„¸ ìˆ˜ì¹˜ ì œì™¸
                filtered_result = {
                    'overall_signal': result.get('overall_signal'),
                    'confidence': result.get('confidence'),
                    'trend_analysis': result.get('trend_analysis', {}),
                    'momentum_analysis': result.get('momentum_analysis', {}),
                    'timeframe_analysis': result.get('timeframe_analysis', {}),
                    'analysis_summary': result.get('analysis_summary')
                }
                
            elif task_name == 'ai_sentiment_analysis':
                # ì‹œì¥ ê°ì • ë¶„ì„
                filtered_result = {
                    'market_sentiment_score': result.get('market_sentiment_score'),
                    'sentiment_state': result.get('sentiment_state'),
                    'confidence': result.get('confidence'),
                    'investment_recommendation': result.get('investment_recommendation'),
                    'market_impact': result.get('market_impact'),
                    'analysis_summary': result.get('analysis_summary')
                }
                
            elif task_name == 'ai_macro_analysis':
                # ê±°ì‹œê²½ì œ ë¶„ì„
                filtered_result = {
                    'macro_environment_score': result.get('macro_environment_score'),
                    'investment_environment': result.get('investment_environment'),
                    'confidence': result.get('confidence'),
                    'btc_recommendation': result.get('btc_recommendation'),
                    'analysis_summary': result.get('analysis_summary'),
                    'key_insights': result.get('key_insights', [])
                }
                
            elif task_name == 'ai_onchain_analysis':
                # ì˜¨ì²´ì¸ ë¶„ì„
                filtered_result = {
                    'onchain_health_score': result.get('onchain_health_score'),
                    'investment_signal': result.get('investment_signal'),
                    'confidence': result.get('confidence'),
                    'network_security_analysis': result.get('network_security_analysis'),
                    'holder_sentiment': result.get('holder_sentiment'),
                    'mining_outlook': result.get('mining_outlook'),
                    'analysis_summary': result.get('analysis_summary'),
                    'key_insights': result.get('key_insights', [])
                }
                
            elif task_name == 'ai_institutional_analysis':
                # ê¸°ê´€íˆ¬ì ë¶„ì„
                filtered_result = {
                    'institutional_flow_score': result.get('institutional_flow_score'),
                    'investment_signal': result.get('investment_signal'),
                    'confidence': result.get('confidence'),
                    'analysis_summary': result.get('analysis_summary'),
                    'key_insights': result.get('key_insights', [])
                }
            
            # í•„í„°ë§ëœ ê²°ê³¼ ì €ì¥
            filtered_data['analysis_result'] = {
                'success': True,
                'result': filtered_result
            }
        
        return filtered_data

    def _filter_blog_analysis_data(self, latest_detailed, historical_summaries):
        """ë¸”ë¡œê·¸ ë¶„ì„ ë°ì´í„° ì „ì²´ë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜"""
        
        # ìµœì‹  ìƒì„¸ ë°ì´í„° í•„í„°ë§
        filtered_latest = []
        for analysis in latest_detailed:
            filtered_analysis = self._filter_analysis_for_blog(analysis)
            filtered_latest.append(filtered_analysis)
        
        # ê³¼ê±° ìš”ì•½ ë°ì´í„°ëŠ” ì´ë¯¸ ìš”ì•½ëœ í˜•íƒœì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
        # ë‹¨, ë„ˆë¬´ ê¸´ summary ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¼ë¶€ ì œí•œ
        filtered_historical = []
        for summary in historical_summaries:
            filtered_summary = summary.copy()
            
            # summary ë‚´ key_pointsê°€ ë„ˆë¬´ ê¸¸ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
            if 'summary' in filtered_summary and 'key_points' in filtered_summary['summary']:
                key_points = filtered_summary['summary']['key_points']
                if isinstance(key_points, list) and len(key_points) > 0:
                    # ì²« ë²ˆì§¸ key_pointê°€ ë„ˆë¬´ ê¸¸ë©´ 1000ìë¡œ ì œí•œ
                    first_point = key_points[0]
                    if len(first_point) > 1000:
                        filtered_summary['summary']['key_points'] = [first_point[:1000] + "..."]
                    else:
                        filtered_summary['summary']['key_points'] = [first_point]
            
            filtered_historical.append(filtered_summary)
        
        return filtered_latest, filtered_historical



# ì „ì—­ ë·°ì–´ ì¸ìŠ¤í„´ìŠ¤
ai_viewer = AIAnalysisViewer()

@router.get("/ai-analysis", response_class=HTMLResponse)
async def ai_analysis_page(
    request: Request,
    analyzer: Optional[str] = Query(None, description="ë¶„ì„ê¸° í•„í„°"),
    hours: int = Query(24, description="ì¡°íšŒ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)"),
    limit: int = Query(50, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
):
    """AI ë¶„ì„ ê²°ê³¼ í˜ì´ì§€"""
    try:
        # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        analyses = ai_viewer.get_recent_analyses(task_name=analyzer, hours=hours, limit=limit)
        
        # í†µê³„ ì •ë³´ ì¡°íšŒ
        statistics = ai_viewer.get_analysis_statistics(hours=hours)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ê¸° ëª©ë¡
        analyzers = [
            {"value": task, "name": ai_viewer.get_task_display_name(task)}
            for task in ai_viewer.get_analysis_tasks()
        ]
        
        return templates.TemplateResponse(
            "ai_analysis.html",
            {
                "request": request,
                "analyses": analyses,
                "statistics": statistics,
                "analyzers": analyzers,
                "current_analyzer": analyzer,
                "hours": hours,
                "limit": limit,
                "total_results": len(analyses)
            }
        )
        
    except Exception as e:
        logger.error(f"AI ë¶„ì„ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

@router.get("/api/ai-analysis")
async def get_ai_analysis_api(
    analyzer: Optional[str] = Query(None),
    hours: int = Query(24),
    limit: int = Query(50)
):
    """AI ë¶„ì„ ê²°ê³¼ API"""
    try:
        analyses = ai_viewer.get_recent_analyses(task_name=analyzer, hours=hours, limit=limit)
        statistics = ai_viewer.get_analysis_statistics(hours=hours)
        
        return {
            "success": True,
            "data": {
                "analyses": analyses,
                "statistics": statistics,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"AI ë¶„ì„ API ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@router.get("/api/ai-status-quick")
async def get_ai_status_quick():
    """AI ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ëŠ” API (AI API í…ŒìŠ¤íŠ¸ ì œì™¸)"""
    try:
        # AI API í…ŒìŠ¤íŠ¸ ì—†ì´ ê¸°ë³¸ ìƒíƒœë§Œ ì¡°íšŒ
        ai_data_status = get_data_status()
        ai_recovery_status = get_recovery_status()
        ai_api_status = get_ai_api_status_summary()
        
        return {
            "success": True,
            "api_test_result": None,  # ë¹ ë¥¸ ì¡°íšŒì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ì•ˆí•¨
            "ai_data_status": ai_data_status,
            "ai_recovery_status": ai_recovery_status,
            "ai_api_status": ai_api_status,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "note": "ë¹ ë¥¸ ì¡°íšŒ ëª¨ë“œ - AI API í…ŒìŠ¤íŠ¸ ì œì™¸"
        }
        
    except Exception as e:
        logger.error(f"AI ìƒíƒœ ë¹ ë¥¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "ai_api_status": {"is_working": False, "status_text": f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}"},
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@router.get("/ai-analysis/detail", response_class=HTMLResponse)
async def ai_analysis_detail(
    request: Request,
    task_name: str = Query(..., description="ë¶„ì„ ì‘ì—…ëª…"),
    timestamp: str = Query(..., description="ë¶„ì„ ì‹œê° (ISO í˜•ì‹)")
):
    """AI ë¶„ì„ ìƒì„¸ ê²°ê³¼ í˜ì´ì§€"""
    try:
        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
        created_at = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        
        # ìƒì„¸ ì •ë³´ ì¡°íšŒ
        detail = ai_viewer.get_analysis_detail(task_name, created_at)
        
        if not detail:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return templates.TemplateResponse(
            "ai_analysis_detail.html",
            {
                "request": request,
                "detail": detail
            }
        )
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹: {str(e)}")
    except Exception as e:
        logger.error(f"AI ë¶„ì„ ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")


@router.get("/api/chart-data")
async def get_chart_data_api(
    hours: int = Query(300, description="ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„)", ge=1, le=720)
):
    """ì°¨íŠ¸ ë°ì´í„° ì¡°íšŒ API (1ì‹œê°„ë´‰ ê¸°ì¤€)"""
    try:
        chart_data = ai_viewer.get_chart_data(hours=hours)
        
        if not chart_data:
            return {
                "success": False,
                "error": "ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "data": [],
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        return {
            "success": True,
            "data": {
                "chart_data": chart_data,
                "data_count": len(chart_data),
                "hours_requested": hours,
                "first_timestamp": chart_data[0]["timestamp"].isoformat() if chart_data else None,
                "last_timestamp": chart_data[-1]["timestamp"].isoformat() if chart_data else None
            },
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"ì°¨íŠ¸ ë°ì´í„° API ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": [],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@router.get("/api/blog-analysis")
async def get_blog_analysis_api(
    analysis_hours: int = Query(6, description="AI ë¶„ì„ ë°ì´í„° ì‹œê°„ ë²”ìœ„", ge=1, le=168)
):
    """ë¸”ë¡œê·¸ ìƒì„±ìš© AI ë¶„ì„ ë°ì´í„° API (í•„í„°ë§ ì ìš©)"""
    try:
        # AI ë¶„ì„ ë°ì´í„° ì¡°íšŒ - ìµœì‹  ìƒì„¸ ë°ì´í„° (ê° ë¶„ì„ê¸°ë³„ ìµœì‹  1ê°œì”©)
        latest_detailed = ai_viewer.get_recent_analyses(hours=analysis_hours, for_blog=True, limit=6)
        
        # AI ë¶„ì„ ë°ì´í„° ì¡°íšŒ - ê³¼ê±° ìš”ì•½ ë°ì´í„° (6ì‹œê°„ ì „ì²´)
        all_analyses = ai_viewer.get_recent_analyses(hours=analysis_hours, for_blog=True, limit=50)
        historical_summaries = []
        
        for analysis in all_analyses:
            if 'summary' in analysis:
                historical_summaries.append({
                    "task_name": analysis["task_name"],
                    "display_name": analysis["display_name"],
                    "created_at": analysis["created_at"].isoformat(),  # UTC ISO í˜•íƒœ
                    "summary": analysis["summary"],
                    "success": analysis.get("success", False)
                })
        
        # ë¸”ë¡œê·¸ìš© ë°ì´í„° í•„í„°ë§ ì ìš©
        filtered_latest, filtered_historical = ai_viewer._filter_blog_analysis_data(latest_detailed, historical_summaries)
        
        return {
            "success": True,
            "data": {
                "latest_detailed_analyses": filtered_latest,
                "historical_summaries": filtered_historical,
                "metadata": {
                    "latest_detailed_count": len(filtered_latest),
                    "historical_summaries_count": len(filtered_historical),
                    "analysis_hours": analysis_hours,
                    "latest_analysis_timestamp": filtered_latest[0]["created_at"].isoformat() if filtered_latest else None,
                    "filtered": True  # í•„í„°ë§ ì ìš©ë¨ì„ í‘œì‹œ
                }
            },
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ ë¶„ì„ ë°ì´í„° API ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": {
                "latest_detailed_analyses": [],
                "historical_summaries": [],
                "metadata": {}
            },
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
