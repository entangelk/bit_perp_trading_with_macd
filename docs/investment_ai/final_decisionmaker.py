import json
import re
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timezone
from google import genai
from google.genai import types
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../'))

from docs.investment_ai.config import CONFIG, API_KEY, MODEL_PRIORITY

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("final_decision_maker")

class FinalDecisionMaker:
    """ìµœì¢… íˆ¬ì ê²°ì • AI - ëª¨ë“  ë¶„ì„ í†µí•©"""
    
    def __init__(self):
        # AI ëª¨ë¸ ì´ˆê¸°í™” ì œê±° - ì‹¤ì œ í˜¸ì¶œ ì‹œì—ë§Œ ì´ˆê¸°í™”
        self.client = None
        self.model_name = None
        
        # ë¶„ì„ë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ìƒí™©ì— ë”°ë¼ ë™ì  ì¡°ì •)
        self.default_weights = {
            'position_analysis': 25,      # í˜„ì¬ í¬ì§€ì…˜ ìƒíƒœ (ìµœìš°ì„ )
            'technical_analysis': 20,     # ê¸°ìˆ ì  ë¶„ì„ (ë‹¨ê¸° ì‹ í˜¸)
            'sentiment_analysis': 15,     # ì‹œì¥ ì‹¬ë¦¬ (ì‹œì¥ ë¶„ìœ„ê¸°)
            'macro_analysis': 15,         # ê±°ì‹œê²½ì œ (ì¤‘ì¥ê¸° í™˜ê²½)
            'onchain_analysis': 15,       # ì˜¨ì²´ì¸ ë°ì´í„° (í€ë”ë©˜í„¸)
            'institutional_analysis': 10  # ê¸°ê´€ íˆ¬ì íë¦„ (ì¥ê¸° íŠ¸ë Œë“œ)
        }
        
        # ê²°ì • ì„ê³„ê°’ ì„¤ì •
        self.decision_thresholds = {
            'strong_buy': 75,
            'buy': 60,
            'hold': 40,
            'sell': 25,
            'strong_sell': 0
        }
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì •
        self.risk_params = {
            'max_position_size': 50,      # ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸° (%)
            'max_leverage': 10,           # ìµœëŒ€ ë ˆë²„ë¦¬ì§€
            'min_confidence': 60,         # ìµœì†Œ ì‹ ë¢°ë„
            'stop_loss_range': (2, 8),    # ìŠ¤í†±ë¡œìŠ¤ ë²”ìœ„ (%)
            'take_profit_range': (4, 15)  # í…Œì´í¬í”„ë¡œí• ë²”ìœ„ (%)
        }

        self.scheduler_to_analysis_mapping = {
            # ìŠ¤ì¼€ì¤„ëŸ¬ í‚¤ â†’ ìµœì¢…ê²°ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤
            'ai_technical_analysis': 'technical_analysis',
            'ai_sentiment_analysis': 'sentiment_analysis', 
            'ai_macro_analysis': 'macro_analysis',
            'ai_onchain_analysis': 'onchain_analysis',
            'ai_institutional_analysis': 'institutional_analysis',
            'position_data': 'position_analysis',  # í¬ì§€ì…˜ì€ ì›ì‹œ ë°ì´í„°
            # í˜„ì¬ í¬ì§€ì…˜ì€ ë³„ë„ ì²˜ë¦¬
        }
        logger.info("ìµœì¢… ê²°ì • ë©”ì´ì»¤ ì´ˆê¸°í™” ì™„ë£Œ - ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ì§€ì›")

    def get_analysis_data_from_scheduler(self, scheduler) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ AI ë¶„ì„ ê²°ê³¼ë“¤ì„ ê°€ì ¸ì™€ì„œ ìµœì¢…ê²°ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë¶„ì„ ë°ì´í„° ë§¤í•‘ ì‹œì‘")
            mapped_results = {}
            
            # 1. AI ë¶„ì„ ê²°ê³¼ë“¤ ë§¤í•‘
            for scheduler_key, analysis_key in self.scheduler_to_analysis_mapping.items():
                try:
                    logger.debug(f"ë§¤í•‘ ì‹œë„: {scheduler_key} â†’ {analysis_key}")
                    
                    # ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ìºì‹œëœ ë°ì´í„° ì¡°íšŒ
                    cached_data = scheduler.get_cached_data(scheduler_key)
                    
                    if cached_data:
                        if scheduler_key.startswith('ai_'):
                            # AI ë¶„ì„ ê²°ê³¼ëŠ” analysis_result ì•ˆì— ìˆìŒ
                            if 'analysis_result' in cached_data:
                                analysis_result = cached_data['analysis_result']
                                mapped_results[analysis_key] = analysis_result
                                
                                # ì„±ê³µ/ì‹¤íŒ¨ ë¡œê¹…
                                if analysis_result.get('success', False):
                                    logger.debug(f"âœ… {scheduler_key} â†’ {analysis_key} ë§¤í•‘ ì„±ê³µ (AI ë¶„ì„)")
                                else:
                                    skip_reason = analysis_result.get('skip_reason', 'unknown')
                                    logger.warning(f"âš ï¸ {scheduler_key} â†’ {analysis_key} ì‹¤íŒ¨í•œ ë¶„ì„ ë§¤í•‘ (ì´ìœ : {skip_reason})")
                            else:
                                logger.error(f"âŒ {scheduler_key}: analysis_result í‚¤ê°€ ì—†ìŒ")
                                mapped_results[analysis_key] = {
                                    'success': False,
                                    'error': f'{scheduler_key}: analysis_result í‚¤ ëˆ„ë½',
                                    'skip_reason': 'malformed_cache_data'
                                }
                        else:
                            # ì›ì‹œ ë°ì´í„° (position_data ë“±)ëŠ” ì§ì ‘ ì‚¬ìš©
                            mapped_results[analysis_key] = cached_data
                            logger.debug(f"âœ… {scheduler_key} â†’ {analysis_key} ì›ì‹œë°ì´í„° ë§¤í•‘")
                    else:
                        logger.warning(f"âŒ {scheduler_key} ìºì‹œ ë°ì´í„° ì—†ìŒ")
                        mapped_results[analysis_key] = {
                            'success': False,
                            'error': f'{scheduler_key} ìºì‹œ ë°ì´í„° ì—†ìŒ',
                            'skip_reason': 'no_cached_data'
                        }
                        
                except Exception as e:
                    logger.error(f"âŒ {scheduler_key} ë§¤í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
                    mapped_results[analysis_key] = {
                        'success': False,
                        'error': f'{scheduler_key} ë§¤í•‘ ì‹¤íŒ¨: {str(e)}',
                        'skip_reason': 'mapping_error'
                    }
            
            # 2. í¬ì§€ì…˜ ë¶„ì„ ë³„ë„ ì²˜ë¦¬ (í¬ì§€ì…˜ ë°ì´í„°ë¡œë¶€í„° ì‹¤ì‹œê°„ ë¶„ì„ ìˆ˜í–‰)
            try:
                logger.debug("í¬ì§€ì…˜ ë¶„ì„ ì‹¤ì‹œê°„ ìˆ˜í–‰ ì‹œì‘")
                position_data = scheduler.get_cached_data('position_data')
                
                if position_data:
                    # í¬ì§€ì…˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ì§€ì…˜ ë¶„ì„ ìˆ˜í–‰
                    from docs.investment_ai.analyzers.position_analyzer import analyze_position_status
                    
                    # í¬ì§€ì…˜ ë°ì´í„°ì—ì„œ í˜„ì¬ í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ
                    current_position_info = self._extract_current_position_from_data(position_data)
                    
                    # ì‹¤ì‹œê°„ í¬ì§€ì…˜ ë¶„ì„ ìˆ˜í–‰
                    position_analysis = analyze_position_status()
                    
                    if position_analysis and position_analysis.get('success', False):
                        mapped_results['position_analysis'] = position_analysis
                        logger.info("âœ… í¬ì§€ì…˜ ë¶„ì„ ì‹¤ì‹œê°„ ìˆ˜í–‰ ì„±ê³µ")
                    else:
                        logger.warning("âŒ í¬ì§€ì…˜ ë¶„ì„ ì‹¤ì‹œê°„ ìˆ˜í–‰ ì‹¤íŒ¨")
                        mapped_results['position_analysis'] = {
                            'success': False,
                            'error': 'í¬ì§€ì…˜ ë¶„ì„ ì‹¤íŒ¨',
                            'skip_reason': 'position_analysis_failed'
                        }
                else:
                    logger.warning("âŒ í¬ì§€ì…˜ ë°ì´í„° ì—†ìŒ")
                    mapped_results['position_analysis'] = {
                        'success': False,
                        'error': 'í¬ì§€ì…˜ ë°ì´í„° ì—†ìŒ',
                        'skip_reason': 'no_position_data'
                    }
                    
            except Exception as e:
                logger.error(f"âŒ í¬ì§€ì…˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                mapped_results['position_analysis'] = {
                    'success': False,
                    'error': f'í¬ì§€ì…˜ ë¶„ì„ ì˜¤ë¥˜: {str(e)}',
                    'skip_reason': 'position_analysis_error'
                }
            
            # 3. í˜„ì¬ í¬ì§€ì…˜ ì •ë³´ ì¶”ê°€ (ìµœì¢… ê²°ì •ì—ì„œ í•„ìš”)
            try:
                logger.debug("í˜„ì¬ í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ ì‹œì‘")
                position_data = scheduler.get_cached_data('position_data')
                
                if position_data and 'balance' in position_data:
                    current_position = self._extract_current_position_from_data(position_data)
                    mapped_results['current_position'] = current_position
                    logger.debug("âœ… í˜„ì¬ í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
                else:
                    # ê¸°ë³¸ í¬ì§€ì…˜ ì •ë³´
                    mapped_results['current_position'] = {
                        'has_position': False,
                        'side': 'none',
                        'size': 0,
                        'entry_price': 0
                    }
                    logger.warning("âŒ í¬ì§€ì…˜ ë°ì´í„° ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                    
            except Exception as e:
                logger.error(f"âŒ í˜„ì¬ í¬ì§€ì…˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                mapped_results['current_position'] = {
                    'has_position': False,
                    'side': 'none',
                    'size': 0,
                    'entry_price': 0,
                    'error': str(e)
                }
            
            # 4. ë§¤í•‘ ê²°ê³¼ ìš”ì•½ ë° ê²€ì¦
            success_count = 0
            failed_count = 0
            skipped_count = 0
            
            for key, value in mapped_results.items():
                if key == 'current_position':
                    continue  # í˜„ì¬ í¬ì§€ì…˜ì€ ë³„ë„ ì²˜ë¦¬
                    
                if isinstance(value, dict):
                    if value.get('success', False):
                        success_count += 1
                    elif value.get('skip_reason'):
                        skipped_count += 1
                    else:
                        failed_count += 1
                else:
                    success_count += 1  # ì›ì‹œ ë°ì´í„°
            
            total_analyses = success_count + failed_count + skipped_count
            
            logger.info(f"ë°ì´í„° ë§¤í•‘ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {failed_count}, ìŠ¤í‚µ {skipped_count} / ì´ {total_analyses}ê°œ")
            
            # ì‹¤íŒ¨í•œ ë¶„ì„ë“¤ ìƒì„¸ ë¡œê¹…
            failed_analyses = []
            for key, value in mapped_results.items():
                if isinstance(value, dict) and not value.get('success', False) and key != 'current_position':
                    reason = value.get('skip_reason', value.get('error', 'unknown'))
                    failed_analyses.append(f"{key}({reason})")
            
            if failed_analyses:
                logger.warning(f"ì‹¤íŒ¨í•œ ë¶„ì„ë“¤: {', '.join(failed_analyses)}")
            
            # ë§¤í•‘ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            mapped_results['_mapping_metadata'] = {
                'mapping_timestamp': datetime.now(timezone.utc).isoformat(),
                'total_analyses': total_analyses,
                'success_count': success_count,
                'failed_count': failed_count,
                'skipped_count': skipped_count,
                'success_rate': (success_count / total_analyses * 100) if total_analyses > 0 else 0,
                'failed_analyses': failed_analyses,
                'scheduler_keys_processed': list(self.scheduler_to_analysis_mapping.keys())
            }
            
            return mapped_results
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë§¤í•‘ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return {
                'error': f'ë§¤í•‘ ì „ì²´ ì‹¤íŒ¨: {str(e)}',
                '_mapping_metadata': {
                    'mapping_timestamp': datetime.now(timezone.utc).isoformat(),
                    'total_error': True,
                    'error_details': str(e)
                }
            }

    def _extract_current_position_from_data(self, position_data: Dict) -> Dict:
        """í¬ì§€ì…˜ ë°ì´í„°ì—ì„œ í˜„ì¬ í¬ì§€ì…˜ ìƒíƒœ ì¶”ì¶œ"""
        try:
            current_position = {
                'has_position': False,
                'side': 'none',
                'size': 0,
                'entry_price': 0,
                'unrealized_pnl': 0,
                'margin_ratio': 0,
                'total_equity': 0,
                'available_balance': 0
            }
            
            # ì”ê³  ì •ë³´ ì¶”ì¶œ
            balance = position_data.get('balance', {})
            if isinstance(balance, dict) and 'USDT' in balance:
                usdt_balance = balance['USDT']
                current_position.update({
                    'total_equity': float(usdt_balance.get('total', 0)),
                    'available_balance': float(usdt_balance.get('free', 0))
                })
            
            # positions í•„ë“œì—ì„œ BTC í¬ì§€ì…˜ ì°¾ê¸°
            positions = position_data.get('positions', [])
            if isinstance(positions, str):
                import json
                try:
                    positions = json.loads(positions)
                except:
                    positions = []
            
            btc_position = None
            if isinstance(positions, list):
                for pos in positions:
                    if isinstance(pos, dict):
                        symbol = pos.get('symbol', '').upper()
                        if 'BTC' in symbol:
                            btc_position = pos
                            break
            
            if btc_position:
                size = float(btc_position.get('size', btc_position.get('contracts', 0)))
                if abs(size) > 0:
                    current_position.update({
                        'has_position': True,
                        'side': 'long' if size > 0 else 'short',
                        'size': abs(size),
                        'entry_price': float(btc_position.get('avgPrice', btc_position.get('entryPrice', 0))),
                        'unrealized_pnl': float(btc_position.get('unrealizedPnl', 0)),
                        'margin_ratio': float(btc_position.get('marginRatio', 0))
                    })
            
            logger.debug(f"í¬ì§€ì…˜ ìƒíƒœ ì¶”ì¶œ ì™„ë£Œ: {current_position['side']} {current_position['size']}")
            return current_position
            
        except Exception as e:
            logger.error(f"í¬ì§€ì…˜ ìƒíƒœ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {
                'has_position': False,
                'side': 'none',
                'size': 0,
                'entry_price': 0,
                'error': str(e)
            }



    async def make_final_decision_with_scheduler(self, scheduler) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•œ ìµœì¢… íˆ¬ì ê²°ì • (ìƒˆë¡œìš´ ë©”ì¸ í•¨ìˆ˜)"""
        try:
            logger.info("ìµœì¢… íˆ¬ì ê²°ì • ë¶„ì„ ì‹œì‘ (ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™)")
            
            # 1. ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë¶„ì„ ê²°ê³¼ë“¤ ê°€ì ¸ì˜¤ê¸° ë° ë§¤í•‘
            all_analysis_results = self.get_analysis_data_from_scheduler(scheduler)
            
            # ë§¤í•‘ ì‹¤íŒ¨ í™•ì¸
            if 'error' in all_analysis_results:
                logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë¶„ì„ ê²°ê³¼ ë§¤í•‘ ì‹¤íŒ¨: {all_analysis_results['error']}")
                return {
                    "success": False,
                    "error": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ì‹¤íŒ¨: {all_analysis_results['error']}",
                    "analysis_type": "final_decision",
                    "skip_reason": "scheduler_mapping_failed"
                }
            
            if not all_analysis_results:
                logger.error("ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")
                return {
                    "success": False,
                    "error": "ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ì‹¤íŒ¨ - ë¶„ì„ ê²°ê³¼ ì—†ìŒ",
                    "analysis_type": "final_decision",
                    "skip_reason": "scheduler_integration_failed"
                }
            
            # 2. ë§¤í•‘ í’ˆì§ˆ ê²€ì¦
            mapping_metadata = all_analysis_results.get('_mapping_metadata', {})
            success_rate = mapping_metadata.get('success_rate', 0)
            
            if success_rate < 40:  # 40% ë¯¸ë§Œ ì„±ê³µë¥ ì´ë©´ ìœ„í—˜
                logger.warning(f"ë§¤í•‘ ì„±ê³µë¥ ì´ ë‚®ìŒ ({success_rate:.1f}%) - ì‹ ì¤‘í•œ ê²°ì • í•„ìš”")
                
            # 3. ê¸°ì¡´ ìµœì¢… ê²°ì • ë¡œì§ ì‹¤í–‰ (ë§¤í•‘ëœ ë°ì´í„° ì‚¬ìš©)
            logger.info(f"ìµœì¢… ê²°ì • ë¡œì§ ì‹¤í–‰ (ë§¤í•‘ ì„±ê³µë¥ : {success_rate:.1f}%)")
            final_decision_result = await self.make_final_decision(all_analysis_results)
            
            # 4. ê²°ê³¼ì— ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ì •ë³´ ì¶”ê°€
            if final_decision_result.get('success', False):
                result_data = final_decision_result.get('result', {})
                if 'analysis_metadata' not in result_data:
                    result_data['analysis_metadata'] = {}
                
                result_data['analysis_metadata'].update({
                    'scheduler_integration': True,
                    'mapping_success_rate': success_rate,
                    'mapping_timestamp': mapping_metadata.get('mapping_timestamp'),
                    'scheduler_keys_used': mapping_metadata.get('scheduler_keys_processed', []),
                    'failed_mappings': mapping_metadata.get('failed_analyses', [])
                })
            
            logger.info("ìµœì¢… íˆ¬ì ê²°ì • ì™„ë£Œ (ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™)")
            return final_decision_result
            
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ìµœì¢… ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "analysis_type": "final_decision",
                "skip_reason": "scheduler_integration_error"
            }

    def debug_scheduler_data_mapping(self, scheduler) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë°ì´í„° ë§¤í•‘ ë””ë²„ê¹… ì •ë³´ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            debug_info = {
                'scheduler_cache_status': {},
                'mapping_test_results': {},
                'raw_cache_data_preview': {},
                'debug_timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            # ê° ìŠ¤ì¼€ì¤„ëŸ¬ í‚¤ì˜ ìºì‹œ ìƒíƒœ í™•ì¸
            for scheduler_key, analysis_key in self.scheduler_to_analysis_mapping.items():
                try:
                    cached_data = scheduler.get_cached_data(scheduler_key)
                    
                    if cached_data:
                        debug_info['scheduler_cache_status'][scheduler_key] = {
                            'has_cache': True,
                            'data_type': str(type(cached_data)),
                            'keys': list(cached_data.keys()) if isinstance(cached_data, dict) else 'not_dict',
                            'size_estimate': len(str(cached_data))
                        }
                        
                        # ì²« 100ìë§Œ ë¯¸ë¦¬ë³´ê¸°
                        preview = str(cached_data)[:100] + "..." if len(str(cached_data)) > 100 else str(cached_data)
                        debug_info['raw_cache_data_preview'][scheduler_key] = preview
                        
                        # ë§¤í•‘ í…ŒìŠ¤íŠ¸
                        if scheduler_key.startswith('ai_') and isinstance(cached_data, dict):
                            if 'analysis_result' in cached_data:
                                analysis_result = cached_data['analysis_result']
                                debug_info['mapping_test_results'][analysis_key] = {
                                    'mapping_possible': True,
                                    'analysis_success': analysis_result.get('success', False),
                                    'analysis_keys': list(analysis_result.keys()) if isinstance(analysis_result, dict) else 'not_dict'
                                }
                            else:
                                debug_info['mapping_test_results'][analysis_key] = {
                                    'mapping_possible': False,
                                    'issue': 'no_analysis_result_key'
                                }
                        else:
                            debug_info['mapping_test_results'][analysis_key] = {
                                'mapping_possible': True,
                                'note': 'raw_data_direct_mapping'
                            }
                    else:
                        debug_info['scheduler_cache_status'][scheduler_key] = {
                            'has_cache': False
                        }
                        debug_info['mapping_test_results'][analysis_key] = {
                            'mapping_possible': False,
                            'issue': 'no_cache_data'
                        }
                        
                except Exception as e:
                    debug_info['scheduler_cache_status'][scheduler_key] = {
                        'has_cache': False,
                        'error': str(e)
                    }
                    debug_info['mapping_test_results'][analysis_key] = {
                        'mapping_possible': False,
                        'issue': f'debug_error: {str(e)}'
                    }
            
            return debug_info
            
        except Exception as e:
            return {
                'debug_error': str(e),
                'debug_timestamp': datetime.now(timezone.utc).isoformat()
            }

    def get_model(self):
        """AI ëª¨ë¸ì„ í•„ìš”í•  ë•Œë§Œ ì´ˆê¸°í™”"""
        if not API_KEY:
            return None, None
            
        try:
            client = genai.Client(api_key=API_KEY)
            
            for model_name in MODEL_PRIORITY:
                try:
                    return client, model_name
                except Exception as e:
                    logger.warning(f"ìµœì¢… ê²°ì • ëª¨ë¸ {model_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue
            
            return None, None
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ì • ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None
    
    def validate_analysis_results(self, analysis_results: Dict) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦ ë° ì •ì œ - NoneType ì—ëŸ¬ ìˆ˜ì •"""
        try:
            validated = {}
            
            # ğŸ”§ ìˆ˜ì •: analysis_results None ì²´í¬ ì¶”ê°€
            if not analysis_results or not isinstance(analysis_results, dict):
                logger.error("ë¶„ì„ ê²°ê³¼ê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
                return {}
            
            # ê° ë¶„ì„ ê²°ê³¼ ê²€ì¦
            required_analyses = [
                'position_analysis', 'technical_analysis', 'sentiment_analysis',
                'macro_analysis', 'onchain_analysis', 'institutional_analysis'
            ]
            
            for analysis_type in required_analyses:
                try:
                    if analysis_type in analysis_results:
                        result = analysis_results[analysis_type]
                        
                        # ğŸ”§ ìˆ˜ì •: resultê°€ Noneì¸ ê²½ìš° ì²´í¬
                        if result is None:
                            logger.warning(f"{analysis_type} ê²°ê³¼ê°€ None")
                            validated[analysis_type] = {
                                'result': {},
                                'confidence': 0,
                                'signal': 'Hold',
                                'timestamp': datetime.now().isoformat(),
                                'data_quality': 0,
                                'error': f'{analysis_type} ê²°ê³¼ê°€ None',
                                'timing_metadata': {'status': 'none_result', 'analysis_type': analysis_type}
                            }
                            continue
                        
                        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
                        if isinstance(result, dict) and result.get('success', False):
                            validated[analysis_type] = {
                                'result': result.get('result', {}),
                                'confidence': self._extract_confidence(result.get('result', {})),
                                'signal': self._extract_signal(result.get('result', {})),
                                'timestamp': result.get('result', {}).get('analysis_metadata', {}).get('data_timestamp', datetime.now().isoformat()),
                                'data_quality': result.get('data_quality', {}).get('success_rate', 0) if result.get('data_quality') else 0,
                                'timing_metadata': self._extract_timing_metadata(result, analysis_type)
                            }
                        else:
                            # ì‹¤íŒ¨í•œ ë¶„ì„ì€ ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                            error_msg = result.get('error', 'ë¶„ì„ ì‹¤íŒ¨') if isinstance(result, dict) else 'ì˜ëª»ëœ ê²°ê³¼ í˜•ì‹'
                            validated[analysis_type] = {
                                'result': {},
                                'confidence': 0,
                                'signal': 'Hold',
                                'timestamp': datetime.now().isoformat(),
                                'data_quality': 0,
                                'error': error_msg,
                                'timing_metadata': self._extract_timing_metadata(result if isinstance(result, dict) else {}, analysis_type)
                            }
                    else:
                        # ëˆ„ë½ëœ ë¶„ì„ë„ ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                        validated[analysis_type] = {
                            'result': {},
                            'confidence': 0,
                            'signal': 'Hold',
                            'timestamp': datetime.now().isoformat(),
                            'data_quality': 0,
                            'error': 'ë¶„ì„ ëˆ„ë½',
                            'timing_metadata': {'status': 'missing', 'analysis_type': analysis_type}
                        }
                except Exception as e:
                    logger.error(f"{analysis_type} ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                    validated[analysis_type] = {
                        'result': {},
                        'confidence': 0,
                        'signal': 'Hold',
                        'timestamp': datetime.now().isoformat(),
                        'data_quality': 0,
                        'error': f'ê²€ì¦ ì˜¤ë¥˜: {str(e)}',
                        'timing_metadata': {'status': 'validation_error', 'analysis_type': analysis_type}
                    }
            
            return validated
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ê²€ì¦ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return {}

    
    def _extract_confidence(self, result: Dict) -> float:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ - None ì²´í¬ ê°•í™”"""
        try:
            # ğŸ”§ ìˆ˜ì •: resultê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
            if not result or not isinstance(result, dict):
                return 50.0
            
            # ë‹¤ì–‘í•œ ì‹ ë¢°ë„ í‚¤ ì‹œë„
            confidence_keys = ['confidence', 'analysis_confidence', 'reliability_score']
            
            for key in confidence_keys:
                if key in result:
                    confidence = result[key]
                    if isinstance(confidence, (int, float)) and confidence is not None:
                        return min(100, max(0, float(confidence)))
            
            # ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            return 50.0
            
        except Exception as e:
            logger.warning(f"ì‹ ë¢°ë„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 50.0

    def _extract_signal(self, result: Dict) -> str:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ íˆ¬ì ì‹ í˜¸ ì¶”ì¶œ - None ì²´í¬ ê°•í™”"""
        try:
            # ğŸ”§ ìˆ˜ì •: resultê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
            if not result or not isinstance(result, dict):
                return 'Hold'
            
            # ë‹¤ì–‘í•œ ì‹ í˜¸ í‚¤ ì‹œë„
            signal_keys = [
                'investment_signal', 'final_decision', 'btc_signal', 
                'institution_signal', 'recommended_action', 'signal'
            ]
            
            for key in signal_keys:
                if key in result and result[key] is not None:
                    signal = str(result[key]).strip()
                    # ì‹ í˜¸ ì •ê·œí™”
                    return self._normalize_signal(signal)
            
            # ì‹ í˜¸ê°€ ì—†ìœ¼ë©´ Hold
            return 'Hold'
            
        except Exception as e:
            logger.warning(f"ì‹ í˜¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 'Hold'


    def _normalize_signal(self, signal: str) -> str:
        """íˆ¬ì ì‹ í˜¸ ì •ê·œí™”"""
        signal_lower = signal.lower()
        
        # Strong Buy íŒ¨í„´
        if any(keyword in signal_lower for keyword in ['strong buy', 'very bullish', 'aggressive buy']):
            return 'Strong Buy'
        
        # Buy íŒ¨í„´
        elif any(keyword in signal_lower for keyword in ['buy', 'bullish', 'long']):
            return 'Buy'
        
        # Strong Sell íŒ¨í„´
        elif any(keyword in signal_lower for keyword in ['strong sell', 'very bearish', 'aggressive sell']):
            return 'Strong Sell'
        
        # Sell íŒ¨í„´
        elif any(keyword in signal_lower for keyword in ['sell', 'bearish', 'short']):
            return 'Sell'
        
        # Hold íŒ¨í„´ (ê¸°ë³¸ê°’)
        else:
            return 'Hold'
    
    def _extract_timing_metadata(self, result: Dict, analysis_type: str) -> Dict:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ íƒ€ì´ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - None ì²´í¬ ê°•í™”"""
        try:
            timing_metadata = {
                'analysis_type': analysis_type,
                'extraction_time': datetime.now(timezone.utc).isoformat()
            }
            
            # ğŸ”§ ìˆ˜ì •: resultê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
            if not result or not isinstance(result, dict):
                timing_metadata.update({
                    'status': 'no_result_data',
                    'error_reason': 'Result is None or not dict'
                })
                return timing_metadata
            
            # ê¸°ë³¸ ë¶„ì„ ì •ë³´
            if result.get('success', False):
                analysis_result = result.get('result', {})
                if isinstance(analysis_result, dict):
                    analysis_metadata = analysis_result.get('analysis_metadata', {})
                    
                    timing_metadata.update({
                        'status': 'success',
                        'analysis_timestamp': analysis_metadata.get('data_timestamp', analysis_metadata.get('analysis_timestamp')),
                        'model_used': analysis_metadata.get('model_used'),
                        'analysis_duration': analysis_metadata.get('analysis_duration'),
                        'data_collection_time': analysis_metadata.get('data_collection_time')
                    })
                else:
                    timing_metadata.update({
                        'status': 'success_but_no_metadata',
                        'note': 'Result exists but no metadata'
                    })
            else:
                timing_metadata.update({
                    'status': 'failed',
                    'error_reason': result.get('error', 'Unknown'),
                    'skip_reason': result.get('skip_reason')
                })
            
            # ìºì‹œ ê´€ë ¨ ì •ë³´ (ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì˜¨ ê²½ìš°)
            if 'analysis_timestamp' in result:
                timing_metadata['cached_analysis_time'] = result['analysis_timestamp']
            
            if 'data_freshness' in result:
                timing_metadata['data_freshness'] = result['data_freshness']
            
            if 'raw_data_used' in result:
                timing_metadata['raw_data_status'] = result['raw_data_used']
            
            # ë¹„í™œì„±í™” ì •ë³´
            if result.get('disabled', False):
                timing_metadata['disabled'] = True
                timing_metadata['status'] = 'disabled'
            
            if result.get('skipped', False):
                timing_metadata['skipped'] = True
                timing_metadata['status'] = 'skipped'
            
            return timing_metadata
            
        except Exception as e:
            return {
                'analysis_type': analysis_type,
                'status': 'extraction_error',
                'error': str(e),
                'extraction_time': datetime.now(timezone.utc).isoformat()
            }

    def calculate_dynamic_weights(self, validated_results: Dict) -> Dict:
        """ë¶„ì„ë³„ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            weights = self.default_weights.copy()
            
            # ë°ì´í„° í’ˆì§ˆì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
            for analysis_type, data in validated_results.items():
                confidence = data.get('confidence', 50)
                data_quality = data.get('data_quality', 50)
                
                # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê°€ì¤‘ì¹˜ ê°ì†Œ
                if confidence < 30:
                    weights[analysis_type] *= 0.5
                elif confidence < 50:
                    weights[analysis_type] *= 0.7
                elif confidence > 80:
                    weights[analysis_type] *= 1.2
                
                # ë°ì´í„° í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€ ê°ì†Œ
                if data_quality < 50:
                    weights[analysis_type] *= 0.8
            
            # í¬ì§€ì…˜ ë¶„ì„ì€ í•­ìƒ ë†’ì€ ê°€ì¤‘ì¹˜ ìœ ì§€
            if 'position_analysis' in weights:
                weights['position_analysis'] = max(weights['position_analysis'], 20)
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (ì´í•© 100)
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: (v / total_weight) * 100 for k, v in weights.items()}
            
            return weights
            
        except Exception as e:
            logger.error(f"ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return self.default_weights
    
    def calculate_composite_score(self, validated_results: Dict, weights: Dict) -> Dict:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì‹ í˜¸ë³„ ì ìˆ˜ ë§¤í•‘
            signal_scores = {
                'Strong Buy': 90,
                'Buy': 70,
                'Hold': 50,
                'Sell': 30,
                'Strong Sell': 10
            }
            
            weighted_score = 0
            total_weight = 0
            signal_distribution = {'Strong Buy': 0, 'Buy': 0, 'Hold': 0, 'Sell': 0, 'Strong Sell': 0}
            confidence_weighted_avg = 0
            
            # ê° ë¶„ì„ì˜ ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
            for analysis_type, data in validated_results.items():
                if analysis_type in weights:
                    weight = weights[analysis_type]
                    signal = data.get('signal', 'Hold')
                    confidence = data.get('confidence', 50)
                    
                    score = signal_scores.get(signal, 50)
                    weighted_score += score * weight / 100
                    total_weight += weight
                    
                    # ì‹ í˜¸ ë¶„í¬ ê¸°ë¡
                    signal_distribution[signal] += weight
                    confidence_weighted_avg += confidence * weight / 100
            
            # ìµœì¢… ì ìˆ˜ ì •ê·œí™”
            composite_score = weighted_score if total_weight > 0 else 50
            
            # ìµœì¢… ê²°ì • ë„ì¶œ
            final_decision = self._determine_final_decision(composite_score, signal_distribution)
            
            return {
                'composite_score': round(composite_score, 2),
                'final_decision': final_decision,
                'signal_distribution': {k: round(v, 1) for k, v in signal_distribution.items()},
                'weighted_confidence': round(confidence_weighted_avg, 1),
                'decision_strength': self._calculate_decision_strength(composite_score, signal_distribution)
            }
            
        except Exception as e:
            logger.error(f"ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'composite_score': 50.0,
                'final_decision': 'Hold',
                'signal_distribution': {'Hold': 100.0},
                'weighted_confidence': 50.0,
                'decision_strength': 'Weak'
            }
    
    def _determine_final_decision(self, score: float, signal_distribution: Dict) -> str:
        """ì¢…í•© ì ìˆ˜ì™€ ì‹ í˜¸ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ê²°ì •"""
        try:
            # ì ìˆ˜ ê¸°ë°˜ 1ì°¨ ê²°ì •
            if score >= self.decision_thresholds['strong_buy']:
                primary_decision = 'Strong Buy'
            elif score >= self.decision_thresholds['buy']:
                primary_decision = 'Buy'
            elif score >= self.decision_thresholds['hold']:
                primary_decision = 'Hold'
            elif score >= self.decision_thresholds['sell']:
                primary_decision = 'Sell'
            else:
                primary_decision = 'Strong Sell'
            
            # ì‹ í˜¸ ë¶„í¬ ê¸°ë°˜ ê²€ì¦
            max_signal = max(signal_distribution, key=signal_distribution.get)
            max_weight = signal_distribution[max_signal]
            
            # ê³¼ë°˜ìˆ˜ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„ 
            if max_weight > 50:
                return max_signal
            
            # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì ìˆ˜ ê¸°ë°˜ ê²°ì • ì‚¬ìš©
            return primary_decision
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ì • ë„ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return 'Hold'
    
    def _calculate_decision_strength(self, score: float, signal_distribution: Dict) -> str:
        """ê²°ì • ê°•ë„ ê³„ì‚°"""
        try:
            # ì ìˆ˜ ê·¹ë‹¨ì„±
            score_extremity = max(abs(score - 50), 0) / 50  # 0~1
            
            # ì‹ í˜¸ ì¼ì¹˜ë„
            max_signal_weight = max(signal_distribution.values())
            signal_consensus = max_signal_weight / 100  # 0~1
            
            # ì¢…í•© ê°•ë„
            strength = (score_extremity + signal_consensus) / 2
            
            if strength > 0.7:
                return 'Very Strong'
            elif strength > 0.5:
                return 'Strong'
            elif strength > 0.3:
                return 'Moderate'
            else:
                return 'Weak'
                
        except Exception:
            return 'Weak'
    
    def generate_risk_management(self, final_decision: str, composite_score: float, 
                                current_position: Dict, market_data: Dict) -> Dict:
        """ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        try:
            current_price = market_data.get('current_price', 100000)
            
            # í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
            if final_decision in ['Strong Buy', 'Buy']:
                position_size = self._calculate_position_size(composite_score, 'long')
                leverage = self._calculate_leverage(composite_score, 'long')
                
                # ìŠ¤í†±ë¡œìŠ¤/í…Œì´í¬í”„ë¡œí•
                stop_loss_pct = self._calculate_stop_loss_percentage(composite_score)
                take_profit_pct = self._calculate_take_profit_percentage(composite_score)
                
                stop_loss_price = current_price * (1 - stop_loss_pct / 100)
                take_profit_price = current_price * (1 + take_profit_pct / 100)
                
            elif final_decision in ['Strong Sell', 'Sell']:
                position_size = self._calculate_position_size(composite_score, 'short')
                leverage = self._calculate_leverage(composite_score, 'short')
                
                stop_loss_pct = self._calculate_stop_loss_percentage(100 - composite_score)
                take_profit_pct = self._calculate_take_profit_percentage(100 - composite_score)
                
                stop_loss_price = current_price * (1 + stop_loss_pct / 100)
                take_profit_price = current_price * (1 - take_profit_pct / 100)
                
            else:  # Hold
                position_size = 0
                leverage = 1
                stop_loss_price = None
                take_profit_price = None
                stop_loss_pct = 0
                take_profit_pct = 0
            
            return {
                'position_size_percent': position_size,
                'recommended_leverage': leverage,
                'stop_loss_price': round(stop_loss_price, 2) if stop_loss_price else None,
                'take_profit_price': round(take_profit_price, 2) if take_profit_price else None,
                'stop_loss_percentage': round(stop_loss_pct, 2),
                'take_profit_percentage': round(take_profit_pct, 2),
                'max_loss_amount': round(position_size * stop_loss_pct / 100, 2) if position_size > 0 else 0,
                'risk_reward_ratio': round(take_profit_pct / stop_loss_pct, 2) if stop_loss_pct > 0 else 0,
                'liquidation_buffer': 15,  # ì²­ì‚°ê°€ ë²„í¼ 15%
                'position_monitoring': self._generate_monitoring_rules(final_decision, composite_score)
            }
            
        except Exception as e:
            logger.error(f"ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_risk_management()
    
    def _calculate_position_size(self, score: float, direction: str) -> float:
        """ì ìˆ˜ ê¸°ë°˜ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°"""
        try:
            # ì ìˆ˜ë¥¼ í¬ì§€ì…˜ í¬ê¸°ë¡œ ë³€í™˜ (50ì  ê¸°ì¤€)
            if direction == 'long':
                strength = max(0, score - 50) / 50  # 0~1
            else:  # short
                strength = max(0, 50 - score) / 50  # 0~1
            
            # ìµœì†Œ 5%, ìµœëŒ€ 50%
            min_size, max_size = 5, self.risk_params['max_position_size']
            position_size = min_size + (max_size - min_size) * strength
            
            return round(position_size, 1)
            
        except Exception:
            return 10.0  # ê¸°ë³¸ê°’
    
    def _calculate_leverage(self, score: float, direction: str) -> int:
        """ì ìˆ˜ ê¸°ë°˜ ë ˆë²„ë¦¬ì§€ ê³„ì‚°"""
        try:
            # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ë ˆë²„ë¦¬ì§€
            if direction == 'long':
                strength = max(0, score - 50) / 50
            else:
                strength = max(0, 50 - score) / 50
            
            # ìµœì†Œ 1ë°°, ìµœëŒ€ ì„¤ì •ê°’
            min_lev, max_lev = 1, self.risk_params['max_leverage']
            leverage = min_lev + (max_lev - min_lev) * strength
            
            return max(1, min(max_lev, int(leverage)))
            
        except Exception:
            return 3  # ê¸°ë³¸ê°’
    
    def _calculate_stop_loss_percentage(self, strength_score: float) -> float:
        """ê°•ë„ ì ìˆ˜ ê¸°ë°˜ ìŠ¤í†±ë¡œìŠ¤ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            # ê°•ë„ê°€ ë†’ì„ìˆ˜ë¡ íƒ€ì´íŠ¸í•œ ìŠ¤í†±ë¡œìŠ¤
            strength = strength_score / 100  # 0~1
            min_sl, max_sl = self.risk_params['stop_loss_range']
            
            # ì—­ë¹„ë¡€: ê°•ë„ ë†’ìœ¼ë©´ ìŠ¤í†±ë¡œìŠ¤ ì‘ê²Œ
            stop_loss = max_sl - (max_sl - min_sl) * strength
            return round(stop_loss, 1)
            
        except Exception:
            return 5.0  # ê¸°ë³¸ê°’
    
    def _calculate_take_profit_percentage(self, strength_score: float) -> float:
        """ê°•ë„ ì ìˆ˜ ê¸°ë°˜ í…Œì´í¬í”„ë¡œí• ë¹„ìœ¨ ê³„ì‚°"""
        try:
            strength = strength_score / 100
            min_tp, max_tp = self.risk_params['take_profit_range']
            
            # ì •ë¹„ë¡€: ê°•ë„ ë†’ìœ¼ë©´ í…Œì´í¬í”„ë¡œí• í¬ê²Œ
            take_profit = min_tp + (max_tp - min_tp) * strength
            return round(take_profit, 1)
            
        except Exception:
            return 8.0  # ê¸°ë³¸ê°’
    
    def _generate_monitoring_rules(self, decision: str, score: float) -> List[str]:
        """í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ ê·œì¹™ ìƒì„±"""
        rules = []
        
        if decision in ['Strong Buy', 'Buy']:
            rules.append("ë¡± í¬ì§€ì…˜ ì§„ì… í›„ ìŠ¤í†±ë¡œìŠ¤ ì¤€ìˆ˜ í•„ìˆ˜")
            rules.append("ê¸°ìˆ ì  ì§€í‘œ ë³€í™” ëª¨ë‹ˆí„°ë§")
            if score > 80:
                rules.append("ê°•í•œ ì‹ í˜¸ì´ë¯€ë¡œ ëª©í‘œê°€ ë„ë‹¬ ì‹œ ì¼ë¶€ ìµì ˆ ê³ ë ¤")
        
        elif decision in ['Strong Sell', 'Sell']:
            rules.append("ìˆ í¬ì§€ì…˜ ì§„ì… í›„ ìŠ¤í†±ë¡œìŠ¤ ì¤€ìˆ˜ í•„ìˆ˜")
            rules.append("ì‹œì¥ ì‹¬ë¦¬ ë³€í™” ì£¼ì‹œ")
            if score < 20:
                rules.append("ê°•í•œ ì•½ì„¸ ì‹ í˜¸ì´ë¯€ë¡œ ë°˜ë“± ì‹œ ì¶”ê°€ ì§„ì… ê³ ë ¤")
        
        else:  # Hold
            rules.append("í˜„ì¬ í¬ì§€ì…˜ ìœ ì§€ ë˜ëŠ” ê´€ë§")
            rules.append("ëª…í™•í•œ ì‹ í˜¸ ë“±ì¥ê¹Œì§€ ëŒ€ê¸°")
        
        rules.append("15ë¶„ë§ˆë‹¤ ì‹ í˜¸ ì—…ë°ì´íŠ¸ í™•ì¸")
        rules.append("ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê·œì¹™ ì—„ìˆ˜")
        
        return rules
    
    def _get_default_risk_management(self) -> Dict:
        """ê¸°ë³¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì •"""
        return {
            'position_size_percent': 10.0,
            'recommended_leverage': 3,
            'stop_loss_price': None,
            'take_profit_price': None,
            'stop_loss_percentage': 5.0,
            'take_profit_percentage': 8.0,
            'max_loss_amount': 0.5,
            'risk_reward_ratio': 1.6,
            'liquidation_buffer': 15,
            'position_monitoring': ["ê¸°ë³¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê·œì¹™ ì ìš©"]
        }
    
    def detect_signal_conflicts(self, validated_results: Dict) -> Dict:
        """ì‹ í˜¸ ì¶©ëŒ ê°ì§€ ë° í•´ê²°"""
        try:
            signals = [data.get('signal', 'Hold') for data in validated_results.values()]
            
            # ì‹ í˜¸ ì¹´ìš´íŠ¸
            signal_count = {}
            for signal in signals:
                signal_count[signal] = signal_count.get(signal, 0) + 1
            
            # ì¶©ëŒ ê°ì§€
            buy_signals = signal_count.get('Strong Buy', 0) + signal_count.get('Buy', 0)
            sell_signals = signal_count.get('Strong Sell', 0) + signal_count.get('Sell', 0)
            hold_signals = signal_count.get('Hold', 0)
            
            conflicts = []
            resolution_strategy = 'consensus'
            
            if buy_signals > 0 and sell_signals > 0:
                conflicts.append("ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸ ì¶©ëŒ")
                resolution_strategy = 'weighted_average'
            
            if abs(buy_signals - sell_signals) <= 1 and hold_signals == 0:
                conflicts.append("ê°•í•œ ì‹ í˜¸ ê°„ íŒ½íŒ½í•œ ëŒ€ë¦½")
                resolution_strategy = 'conservative_hold'
            
            return {
                'conflicts_detected': len(conflicts) > 0,
                'conflict_types': conflicts,
                'signal_distribution': signal_count,
                'resolution_strategy': resolution_strategy,
                'confidence_adjustment': -10 if len(conflicts) > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"ì‹ í˜¸ ì¶©ëŒ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'conflicts_detected': False,
                'conflict_types': [],
                'signal_distribution': {},
                'resolution_strategy': 'hold',
                'confidence_adjustment': 0
            }
    
    async def analyze_with_ai(self, integrated_data: Dict) -> Dict:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… íˆ¬ì ê²°ì •"""
        # í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ ì´ˆê¸°í™”
        if self.client is None:
            self.client, self.model_name = self.get_model()
        
        if self.client is None:
            logger.warning("AI ëª¨ë¸ì´ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.rule_based_final_decision(integrated_data)
        
        try:
            # ìµœì¢… ê²°ì • í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = CONFIG["prompts"]["final_decision"].format(
                position_analysis=json.dumps(integrated_data.get('position_analysis', {}), ensure_ascii=False, indent=2),
                sentiment_analysis=json.dumps(integrated_data.get('sentiment_analysis', {}), ensure_ascii=False, indent=2),
                technical_analysis=json.dumps(integrated_data.get('technical_analysis', {}), ensure_ascii=False, indent=2),
                macro_analysis=json.dumps(integrated_data.get('macro_analysis', {}), ensure_ascii=False, indent=2),
                onchain_analysis=json.dumps(integrated_data.get('onchain_analysis', {}), ensure_ascii=False, indent=2),
                institution_analysis=json.dumps(integrated_data.get('institutional_analysis', {}), ensure_ascii=False, indent=2),
                current_position=json.dumps(integrated_data.get('current_position', {}), ensure_ascii=False, indent=2)
            )
            
            # AI ëª¨ë¸ì— ì§ˆì˜
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            
            # ğŸ”§ ìˆ˜ì •: AI ì‘ë‹µ ì²˜ë¦¬ ê°•í™”
            if not response or not hasattr(response, 'text') or not response.text:
                logger.error("AI ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return self.rule_based_final_decision(integrated_data)
            
            result_text = response.text.strip()
            logger.info(f"ğŸ” DEBUG: AI ì‘ë‹µ ê¸¸ì´: {len(result_text)}")
            logger.info(f"ğŸ” DEBUG: AI ì‘ë‹µ ì²« 100ì: {result_text[:100]}")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # 1ì°¨: ì „ì²´ ì‘ë‹µì´ JSONì¸ì§€ í™•ì¸
                result_json = json.loads(result_text)
                logger.info("ğŸ” DEBUG: ì „ì²´ ì‘ë‹µì´ JSONìœ¼ë¡œ íŒŒì‹±ë¨")
            except json.JSONDecodeError:
                # 2ì°¨: JSON ë¸”ë¡ ì°¾ê¸°
                json_match = re.search(r'\{[\s\S]*\}', result_text)
                if json_match:
                    try:
                        result_json = json.loads(json_match.group(0))
                        logger.info("ğŸ” DEBUG: JSON ë¸”ë¡ ì¶”ì¶œ í›„ íŒŒì‹± ì„±ê³µ")
                    except json.JSONDecodeError as e:
                        logger.error(f"ğŸ” DEBUG: JSON ë¸”ë¡ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        logger.error(f"ğŸ” DEBUG: ì¶”ì¶œëœ JSON: {json_match.group(0)[:200]}")
                        return self.rule_based_final_decision(integrated_data)
                else:
                    logger.error("ğŸ” DEBUG: AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    logger.error(f"ğŸ” DEBUG: ì „ì²´ ì‘ë‹µ: {result_text[:500]}")
                    return self.rule_based_final_decision(integrated_data)
            
            # ğŸ”§ ìˆ˜ì •: íŒŒì‹±ëœ ê²°ê³¼ íƒ€ì… í™•ì¸
            if not isinstance(result_json, dict):
                logger.error(f"ğŸ” DEBUG: íŒŒì‹±ëœ ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(result_json)}")
                logger.error(f"ğŸ” DEBUG: íŒŒì‹±ëœ ê²°ê³¼: {result_json}")
                return self.rule_based_final_decision(integrated_data)
            
            logger.info(f"ğŸ” DEBUG: AI ê²°ê³¼ í‚¤ë“¤: {list(result_json.keys())}")
            
            # ğŸ”§ ìˆ˜ì •: í•„ìˆ˜ í‚¤ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            required_keys = ['final_decision', 'decision_confidence', 'recommended_action']
            for key in required_keys:
                if key not in result_json:
                    logger.warning(f"ğŸ” DEBUG: í•„ìˆ˜ í‚¤ ëˆ„ë½: {key}")
                    if key == 'final_decision':
                        result_json[key] = 'Hold'
                    elif key == 'decision_confidence':
                        result_json[key] = 50
                    elif key == 'recommended_action':
                        result_json[key] = {'action_type': 'Wait'}
            
            # ë¶„ì„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result_json['analysis_metadata'] = {
                'analysis_type': 'ai_based',
                'decision_timestamp': datetime.now(timezone.utc).isoformat(),
                'model_used': self.model_name,
                'integrated_analyses': list(integrated_data.keys()),
                'raw_data': integrated_data
            }
            
            logger.info(f"ğŸ” DEBUG: ìµœì¢… AI ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ - {result_json.get('final_decision', 'Unknown')}")
            return result_json
            
        except Exception as e:
            logger.error(f"AI ìµœì¢… ê²°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ğŸ” DEBUG: ì˜¤ë¥˜ ë°œìƒ ì§€ì ì—ì„œ integrated_data í‚¤ë“¤: {list(integrated_data.keys()) if isinstance(integrated_data, dict) else 'Not dict'}")
            return self.rule_based_final_decision(integrated_data)
    
    def rule_based_final_decision(self, integrated_data: Dict) -> Dict:
        """ê·œì¹™ ê¸°ë°˜ ìµœì¢… íˆ¬ì ê²°ì • (AI ëª¨ë¸ ì—†ì„ ë•Œ ë°±ì—…)"""
        try:
            # 1. ë¶„ì„ ê²°ê³¼ ê²€ì¦
            validated_results = self.validate_analysis_results(integrated_data)
            
            # 2. ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
            weights = self.calculate_dynamic_weights(validated_results)
            
            # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            composite_analysis = self.calculate_composite_score(validated_results, weights)
            
            # 4. ì‹ í˜¸ ì¶©ëŒ ê°ì§€
            conflict_analysis = self.detect_signal_conflicts(validated_results)
            
            # 5. í˜„ì¬ í¬ì§€ì…˜ ê³ ë ¤
            current_position = integrated_data.get('current_position', {})
            
            # 6. ì‹œì¥ ë°ì´í„°
            market_data = {
                'current_price': 100000,  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” í˜„ì¬ê°€ ì „ë‹¬ë°›ì•„ì•¼ í•¨
                'volatility': 'medium'
            }
            
            # 7. ë¦¬ìŠ¤í¬ ê´€ë¦¬
            risk_management = self.generate_risk_management(
                composite_analysis['final_decision'],
                composite_analysis['composite_score'],
                current_position,
                market_data
            )
            
            # 8. ì‹ ë¢°ë„ ì¡°ì • (ì¶©ëŒ ì‹œ ê°ì†Œ)
            final_confidence = max(0, min(100, 
                composite_analysis['weighted_confidence'] + conflict_analysis['confidence_adjustment']
            ))
            
            # 9. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "final_decision": composite_analysis['final_decision'],
                "decision_confidence": round(final_confidence, 1),
                "recommended_action": {
                    "action_type": self._map_decision_to_action(composite_analysis['final_decision'], current_position),
                    "entry_price": market_data['current_price'],
                    "position_size": risk_management['position_size_percent'],
                    "leverage": risk_management['recommended_leverage'],
                    "mandatory_stop_loss": risk_management['stop_loss_price'],
                    "mandatory_take_profit": risk_management['take_profit_price']
                },
                "analysis_weight": {k: round(v, 1) for k, v in weights.items()},
                "composite_score": composite_analysis['composite_score'],
                "signal_distribution": composite_analysis['signal_distribution'],
                "decision_strength": composite_analysis['decision_strength'],
                "risk_assessment": {
                    "overall_risk": self._assess_overall_risk(composite_analysis['composite_score'], final_confidence),
                    "max_loss_potential": risk_management['max_loss_amount'],
                    "profit_potential": risk_management['take_profit_percentage'],
                    "risk_reward_ratio": risk_management['risk_reward_ratio']
                },
                "conflict_analysis": conflict_analysis,
                "execution_plan": {
                    "immediate_action": self._generate_immediate_action(composite_analysis['final_decision']),
                    "sl_tp_mandatory": True if composite_analysis['final_decision'] != 'Hold' else False,
                    "monitoring_points": risk_management['position_monitoring'],
                    "exit_conditions": self._generate_exit_conditions(composite_analysis['final_decision'])
                },
                "market_outlook": {
                    "short_term": "15ë¶„-1ì‹œê°„ ì „ë§",
                    "medium_term": "1-4ì‹œê°„ ì „ë§",
                    "trend_change_probability": self._calculate_trend_change_probability(composite_analysis)
                },
                "individual_analysis_summary": self._summarize_individual_analyses(validated_results),
                "confidence": round(final_confidence, 1),
                "decision_reasoning": self._generate_decision_reasoning(composite_analysis, weights, conflict_analysis),
                "needs_human_review": final_confidence < self.risk_params['min_confidence'] or conflict_analysis['conflicts_detected'],
                "human_review_reason": self._generate_review_reason(final_confidence, conflict_analysis)
            }
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['analysis_metadata'] = {
                'analysis_type': 'rule_based',
                'decision_timestamp': datetime.now(timezone.utc).isoformat(),
                'model_used': 'rule_based_final_decision',
                'integrated_analyses': list(validated_results.keys()),
                'weights_used': weights,
                'raw_data': integrated_data
            }
            
            # ì¢…í•© íƒ€ì´ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['timing_summary'] = self._generate_timing_summary(validated_results)
            
            return result
            
        except Exception as e:
            logger.error(f"ê·œì¹™ ê¸°ë°˜ ìµœì¢… ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_emergency_decision()
    
    def _map_decision_to_action(self, decision: str, current_position: Dict) -> str:
        """ê²°ì •ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ìœ¼ë¡œ ë§¤í•‘"""
        try:
            has_position = current_position.get('has_position', False)
            position_side = current_position.get('side', 'none')
            
            if decision == 'Strong Buy':
                if not has_position:
                    return "Open Long Position"
                elif position_side == 'short':
                    return "Reverse to Long"
                else:
                    return "Add to Long Position"
                    
            elif decision == 'Buy':
                if not has_position:
                    return "Open Long Position"
                elif position_side == 'short':
                    return "Close Short Position"
                else:
                    return "Hold Long Position"
                    
            elif decision == 'Strong Sell':
                if not has_position:
                    return "Open Short Position"
                elif position_side == 'long':
                    return "Reverse to Short"
                else:
                    return "Add to Short Position"
                    
            elif decision == 'Sell':
                if not has_position:
                    return "Open Short Position"
                elif position_side == 'long':
                    return "Close Long Position"
                else:
                    return "Hold Short Position"
                    
            else:  # Hold
                if has_position:
                    return "Hold Current Position"
                else:
                    return "Wait for Signal"
                    
        except Exception:
            return "Hold Current Position"
    
    def _assess_overall_risk(self, score: float, confidence: float) -> str:
        """ì „ì²´ ë¦¬ìŠ¤í¬ í‰ê°€"""
        try:
            # ì ìˆ˜ ê·¹ë‹¨ì„±ê³¼ ì‹ ë¢°ë„ ê³ ë ¤
            score_risk = abs(score - 50) / 50  # 0~1 (ê·¹ë‹¨ì ì¼ìˆ˜ë¡ ìœ„í—˜)
            confidence_safety = confidence / 100  # 0~1 (ë†’ì„ìˆ˜ë¡ ì•ˆì „)
            
            overall_risk_score = (score_risk * 0.6) + ((1 - confidence_safety) * 0.4)
            
            if overall_risk_score > 0.7:
                return "Very High"
            elif overall_risk_score > 0.5:
                return "High"
            elif overall_risk_score > 0.3:
                return "Medium"
            else:
                return "Low"
                
        except Exception:
            return "Medium"
    
    def _generate_immediate_action(self, decision: str) -> str:
        """ì¦‰ì‹œ ì‹¤í–‰í•  í–‰ë™ ìƒì„±"""
        action_map = {
            'Strong Buy': "ì¦‰ì‹œ ë¡± í¬ì§€ì…˜ ì§„ì… ë˜ëŠ” í™•ëŒ€",
            'Buy': "ì ì • ì‹œì ì— ë¡± í¬ì§€ì…˜ ì§„ì…",
            'Hold': "í˜„ì¬ ìƒíƒœ ìœ ì§€ ë° ê´€ì°°",
            'Sell': "ì ì • ì‹œì ì— ìˆ í¬ì§€ì…˜ ì§„ì… ë˜ëŠ” ë¡± ì²­ì‚°",
            'Strong Sell': "ì¦‰ì‹œ ìˆ í¬ì§€ì…˜ ì§„ì… ë˜ëŠ” ë¡± ì „ëŸ‰ ì²­ì‚°"
        }
        return action_map.get(decision, "ê´€ì°° ì§€ì†")
    
    def _generate_exit_conditions(self, decision: str) -> List[str]:
        """ì²­ì‚° ì¡°ê±´ ìƒì„±"""
        if decision in ['Strong Buy', 'Buy']:
            return [
                "ìŠ¤í†±ë¡œìŠ¤ ê°€ê²© í„°ì¹˜ ì‹œ ì¦‰ì‹œ ì²­ì‚°",
                "í…Œì´í¬í”„ë¡œí• ëª©í‘œê°€ ë„ë‹¬ ì‹œ ì¼ë¶€ ë˜ëŠ” ì „ëŸ‰ ì²­ì‚°",
                "ê¸°ìˆ ì  ì‹ í˜¸ ë°˜ì „ ì‹œ ì²­ì‚° ê³ ë ¤",
                "ì‹œì¥ ì‹¬ë¦¬ ê¸‰ë³€ ì‹œ ì¬í‰ê°€"
            ]
        elif decision in ['Strong Sell', 'Sell']:
            return [
                "ìŠ¤í†±ë¡œìŠ¤ ê°€ê²© í„°ì¹˜ ì‹œ ì¦‰ì‹œ ì²­ì‚°",
                "í…Œì´í¬í”„ë¡œí• ëª©í‘œê°€ ë„ë‹¬ ì‹œ ì¼ë¶€ ë˜ëŠ” ì „ëŸ‰ ì²­ì‚°",
                "ì§€ì§€ì„  ê°•ë ¥ ì§€ì§€ ì‹œ ì²­ì‚° ê³ ë ¤",
                "ê±°ì‹œê²½ì œ í˜¸ì¬ ë°œìƒ ì‹œ ì¬í‰ê°€"
            ]
        else:
            return [
                "ëª…í™•í•œ ë°©í–¥ì„± ì‹ í˜¸ ë“±ì¥ ì‹œ í¬ì§€ì…˜ ê²€í† ",
                "15ë¶„ë§ˆë‹¤ ì‹ í˜¸ ì¬í‰ê°€"
            ]
    
    def _calculate_trend_change_probability(self, composite_analysis: Dict) -> str:
        """ì¶”ì„¸ ì „í™˜ ê°€ëŠ¥ì„± ê³„ì‚°"""
        try:
            score = composite_analysis['composite_score']
            strength = composite_analysis['decision_strength']
            
            # ì¤‘ë¦½ êµ¬ê°„ì—ì„œëŠ” ì „í™˜ ê°€ëŠ¥ì„± ë†’ìŒ
            if 40 <= score <= 60:
                return "High (ì¤‘ë¦½ êµ¬ê°„)"
            
            # ê·¹ë‹¨ êµ¬ê°„ì—ì„œ ê°•í•œ ì‹ í˜¸ë©´ ì „í™˜ ê°€ëŠ¥ì„± ë‚®ìŒ
            if strength in ['Very Strong', 'Strong'] and (score > 75 or score < 25):
                return "Low (ê°•í•œ ì¶”ì„¸)"
            
            # ê·¸ ì™¸ëŠ” ì¤‘ê°„
            return "Medium"
            
        except Exception:
            return "Medium"
    
    def _summarize_individual_analyses(self, validated_results: Dict) -> Dict:
        """ê°œë³„ ë¶„ì„ ìš”ì•½"""
        summary = {}
        
        for analysis_type, data in validated_results.items():
            summary[analysis_type] = {
                'signal': data.get('signal', 'Hold'),
                'confidence': data.get('confidence', 50),
                'status': 'Success' if 'error' not in data else 'Failed',
                'key_point': self._extract_key_point(analysis_type, data)
            }
        
        return summary
    
    def _extract_key_point(self, analysis_type: str, data: Dict) -> str:
        """ê° ë¶„ì„ì˜ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        try:
            result = data.get('result', {})
            
            if analysis_type == 'position_analysis':
                return f"í¬ì§€ì…˜ ìƒíƒœ: {data.get('signal', 'N/A')}"
            elif analysis_type == 'technical_analysis':
                return f"ê¸°ìˆ ì  ì‹ í˜¸: {data.get('signal', 'N/A')}"
            elif analysis_type == 'sentiment_analysis':
                sentiment_score = result.get('market_sentiment_score', 50)
                return f"ì‹œì¥ ì‹¬ë¦¬: {sentiment_score}ì "
            elif analysis_type == 'macro_analysis':
                macro_score = result.get('macro_environment_score', 50)
                return f"ê±°ì‹œê²½ì œ: {macro_score}ì "
            elif analysis_type == 'onchain_analysis':
                onchain_score = result.get('onchain_health_score', 50)
                return f"ì˜¨ì²´ì¸: {onchain_score}ì "
            elif analysis_type == 'institutional_analysis':
                institutional_score = result.get('institutional_flow_score', 50)
                return f"ê¸°ê´€íˆ¬ì: {institutional_score}ì "
            else:
                return f"ì‹ í˜¸: {data.get('signal', 'N/A')}"
                
        except Exception:
            return "ë¶„ì„ ë°ì´í„° ì—†ìŒ"
    
    def _generate_decision_reasoning(self, composite_analysis: Dict, weights: Dict, conflict_analysis: Dict) -> str:
        """ê²°ì • ì´ìœ  ìƒì„±"""
        try:
            decision = composite_analysis['final_decision']
            score = composite_analysis['composite_score']
            strength = composite_analysis['decision_strength']
            
            reasoning = f"ì¢…í•© ì ìˆ˜ {score:.1f}ì ì„ ê¸°ë°˜ìœ¼ë¡œ '{decision}' ê²°ì •. "
            reasoning += f"ì‹ í˜¸ ê°•ë„: {strength}. "
            
            # ì£¼ìš” ê°€ì¤‘ì¹˜ ì–¸ê¸‰
            max_weight_analysis = max(weights, key=weights.get)
            max_weight = weights[max_weight_analysis]
            reasoning += f"ì£¼ìš” ê·¼ê±°: {max_weight_analysis} ({max_weight:.1f}% ê°€ì¤‘ì¹˜). "
            
            # ì¶©ëŒ ìƒí™© ì–¸ê¸‰
            if conflict_analysis['conflicts_detected']:
                reasoning += f"ì‹ í˜¸ ì¶©ëŒ ê°ì§€: {', '.join(conflict_analysis['conflict_types'])}. "
                reasoning += f"í•´ê²° ì „ëµ: {conflict_analysis['resolution_strategy']}."
            
            return reasoning
            
        except Exception:
            return "ì¢…í•© ë¶„ì„ì„ í†µí•œ ê²°ì •"
    
    def _generate_review_reason(self, confidence: float, conflict_analysis: Dict) -> str:
        """ì¸ê°„ ê²€í†  í•„ìš” ì´ìœ """
        reasons = []
        
        if confidence < self.risk_params['min_confidence']:
            reasons.append(f"ë‚®ì€ ì‹ ë¢°ë„ ({confidence:.1f}%)")
        
        if conflict_analysis['conflicts_detected']:
            reasons.append("ë¶„ì„ ê°„ ì‹ í˜¸ ì¶©ëŒ")
        
        if len(reasons) == 0:
            return None
        
        return "; ".join(reasons)
    
    def _generate_timing_summary(self, validated_results: Dict) -> Dict:
        """ëª¨ë“  ë¶„ì„ì˜ íƒ€ì´ë° ì •ë³´ ì¢…í•©"""
        try:
            summary = {
                'total_analyses': len(validated_results),
                'successful_analyses': 0,
                'failed_analyses': 0,
                'skipped_analyses': 0,
                'disabled_analyses': 0,
                'cached_analyses': 0,
                'real_time_analyses': 0,
                'oldest_data_age_minutes': 0,
                'newest_data_age_minutes': float('inf'),
                'analysis_freshness': {},
                'data_quality_summary': {},
                'timing_details': {}
            }
            
            for analysis_type, data in validated_results.items():
                timing_meta = data.get('timing_metadata', {})
                status = timing_meta.get('status', 'unknown')
                
                # ìƒíƒœë³„ ì¹´ìš´íŠ¸
                if status == 'success':
                    summary['successful_analyses'] += 1
                elif status == 'failed':
                    summary['failed_analyses'] += 1
                elif status == 'skipped':
                    summary['skipped_analyses'] += 1
                elif status == 'disabled':
                    summary['disabled_analyses'] += 1
                
                # ìºì‹œ vs ì‹¤ì‹œê°„ ë¶„ì„
                if 'cached_analysis_time' in timing_meta:
                    summary['cached_analyses'] += 1
                else:
                    summary['real_time_analyses'] += 1
                
                # ë°ì´í„° ì‹ ì„ ë„ ë¶„ì„
                data_freshness = timing_meta.get('data_freshness', {})
                if data_freshness:
                    ages = [age for age in data_freshness.values() if isinstance(age, (int, float))]
                    if ages:
                        max_age = max(ages)
                        min_age = min(ages)
                        summary['oldest_data_age_minutes'] = max(summary['oldest_data_age_minutes'], max_age)
                        summary['newest_data_age_minutes'] = min(summary['newest_data_age_minutes'], min_age)
                
                # ë¶„ì„ë³„ ì‹ ì„ ë„ ë“±ê¸‰
                if data_freshness:
                    avg_age = sum(age for age in data_freshness.values() if isinstance(age, (int, float))) / len([age for age in data_freshness.values() if isinstance(age, (int, float))])
                    if avg_age <= 30:
                        freshness_grade = 'fresh'
                    elif avg_age <= 120:
                        freshness_grade = 'moderate'
                    else:
                        freshness_grade = 'stale'
                    summary['analysis_freshness'][analysis_type] = freshness_grade
                
                # ë°ì´í„° í’ˆì§ˆ ìš”ì•½
                raw_data_status = timing_meta.get('raw_data_status', {})
                if raw_data_status:
                    available_sources = raw_data_status.get('available_sources', 0)
                    total_sources = len([k for k, v in raw_data_status.items() if k.startswith('has_')])
                    if total_sources > 0:
                        quality_score = (available_sources / total_sources) * 100
                        summary['data_quality_summary'][analysis_type] = {
                            'quality_score': round(quality_score, 1),
                            'available_sources': available_sources,
                            'total_sources': total_sources
                        }
                
                # ìƒì„¸ íƒ€ì´ë° ì •ë³´
                summary['timing_details'][analysis_type] = {
                    'status': status,
                    'timestamp': timing_meta.get('analysis_timestamp', timing_meta.get('cached_analysis_time')),
                    'is_cached': 'cached_analysis_time' in timing_meta,
                    'model_used': timing_meta.get('model_used'),
                    'error_reason': timing_meta.get('error_reason')
                }
            
            # ì‹ ì„ ë„ ì²˜ë¦¬ (ë¬´í•œëŒ€ ì²˜ë¦¬)
            if summary['newest_data_age_minutes'] == float('inf'):
                summary['newest_data_age_minutes'] = 0
            
            # ì „ì²´ ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰
            successful_rate = (summary['successful_analyses'] / summary['total_analyses']) * 100 if summary['total_analyses'] > 0 else 0
            if successful_rate >= 80:
                summary['overall_quality'] = 'excellent'
            elif successful_rate >= 60:
                summary['overall_quality'] = 'good'
            elif successful_rate >= 40:
                summary['overall_quality'] = 'moderate'
            else:
                summary['overall_quality'] = 'poor'
            
            # ìºì‹œ íš¨ìœ¨ì„±
            cache_efficiency = (summary['cached_analyses'] / summary['total_analyses']) * 100 if summary['total_analyses'] > 0 else 0
            summary['cache_efficiency_percent'] = round(cache_efficiency, 1)
            
            return summary
            
        except Exception as e:
            logger.error(f"íƒ€ì´ë° ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'total_analyses': len(validated_results) if validated_results else 0,
                'error': f'íƒ€ì´ë° ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}',
                'generation_time': datetime.now(timezone.utc).isoformat()
            }
    
    def _get_emergency_decision(self) -> Dict:
        """ê¸´ê¸‰ ìƒí™© ê¸°ë³¸ ê²°ì •"""
        return {
            "final_decision": "Hold",
            "decision_confidence": 0,
            "recommended_action": {
                "action_type": "Wait for Signal",
                "entry_price": None,
                "position_size": 0,
                "leverage": 1,
                "mandatory_stop_loss": None,
                "mandatory_take_profit": None
            },
            "error": "ìµœì¢… ê²°ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "needs_human_review": True,
            "human_review_reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸´ê¸‰ ìƒí™©"
        }
    
    def check_analysis_data_availability(self, all_analysis_results: Dict) -> Tuple[bool, Dict]:
        """ë¶„ì„ ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸ - í¬ì§€ì…˜ ìœ ë¬´ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¡œì§ ì¶”ê°€"""
        try:
            logger.info(f"ğŸ” DEBUG: check_analysis_data_availability ì‹œì‘")
            logger.info(f"ğŸ” DEBUG: all_analysis_results type: {type(all_analysis_results)}")
            logger.info(f"ğŸ” DEBUG: all_analysis_results is None: {all_analysis_results is None}")
            logger.info(f"ğŸ” DEBUG: all_analysis_results length: {len(all_analysis_results) if all_analysis_results else 'N/A'}")
            
            # all_analysis_resultsê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²´í¬
            if not all_analysis_results or not isinstance(all_analysis_results, dict):
                logger.error("ë¶„ì„ ê²°ê³¼ê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
                return False, {
                    'analysis_status': {},
                    'failed_due_to_data': 0,
                    'failed_due_to_disabled': 0,
                    'total_core_analyses': 0,
                    'core_success_count': 0,
                    'essential_success_count': 0,
                    'critical_failures': ['all_analysis_results_is_none'],
                    'data_availability_rate': 0,
                    'decision_viability': 'not_viable',
                    'failure_reasons': ['ë¶„ì„ ê²°ê³¼ê°€ None ë˜ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬']
                }
            
            # ğŸ”§ í¬ì§€ì…˜ ìœ ë¬´ í™•ì¸
            current_position = all_analysis_results.get('current_position', {})
            has_position = current_position.get('has_position', False)
            
            logger.info(f"ğŸ” DEBUG: í˜„ì¬ í¬ì§€ì…˜ ìƒíƒœ: {has_position}")
            
            # ğŸ”§ í¬ì§€ì…˜ ì—†ìœ¼ë©´ position_analysis ê¸°ë³¸ê°’ ì„¤ì •
            if not has_position and ('position_analysis' not in all_analysis_results or 
                                not isinstance(all_analysis_results.get('position_analysis'), dict)):
                logger.info("ğŸ” DEBUG: í¬ì§€ì…˜ ì—†ìŒ - position_analysis ê¸°ë³¸ê°’ ì„¤ì •")
                all_analysis_results['position_analysis'] = {
                    'success': True,
                    'result': {
                        'recommended_action': 'Wait',
                        'position_status': 'No Position',
                        'risk_level': 'None',
                        'confidence': 100
                    },
                    'analysis_type': 'position_analysis',
                    'note': 'No position - default analysis'
                }
            
            # ğŸ”§ ë³€ìˆ˜ë“¤ ì´ˆê¸°í™”
            analysis_status = {}
            failed_due_to_data = 0
            failed_due_to_disabled = 0
            total_analyses = 0
            critical_failures = []
            
            # í•µì‹¬ ë¶„ì„ë“¤ (ìµœì†Œ 2ê°œëŠ” ì„±ê³µí•´ì•¼ í•¨)
            core_analyses = ['sentiment_analysis', 'technical_analysis', 'macro_analysis', 'onchain_analysis', 'institutional_analysis']
            
            # ğŸ”§ í¬ì§€ì…˜ ìœ ë¬´ì— ë”°ë¥¸ í•„ìˆ˜ ë¶„ì„ ê²°ì •
            if has_position:
                essential_analyses = ['technical_analysis', 'position_analysis']
                logger.info("ğŸ” DEBUG: í¬ì§€ì…˜ ìˆìŒ - position_analysis í•„ìˆ˜")
            else:
                essential_analyses = ['technical_analysis']
                logger.info("ğŸ” DEBUG: í¬ì§€ì…˜ ì—†ìŒ - position_analysis í•„ìˆ˜ ì•„ë‹˜")
            
            # ğŸ” ë””ë²„ê¹…: ë¶„ì„ ëŒ€ìƒ ëª©ë¡
            logger.info(f"ğŸ” DEBUG: í•µì‹¬ ë¶„ì„ ëª©ë¡: {core_analyses}")
            logger.info(f"ğŸ” DEBUG: í•„ìˆ˜ ë¶„ì„ ëª©ë¡: {essential_analyses}")
            
            # ê° ë¶„ì„ ê²€ì‚¬
            for analysis_type in core_analyses + essential_analyses:
                logger.info(f"ğŸ” DEBUG: {analysis_type} ê²€ì‚¬ ì‹œì‘")
                
                if analysis_type in all_analysis_results:
                    total_analyses += 1
                    result = all_analysis_results[analysis_type]
                    
                    # ğŸ” ë””ë²„ê¹…: ê°œë³„ ë¶„ì„ ê²°ê³¼ ìƒì„¸ í™•ì¸
                    logger.info(f"ğŸ” DEBUG: {analysis_type} ê²°ê³¼ íƒ€ì…: {type(result)}")
                    logger.info(f"ğŸ” DEBUG: {analysis_type} ê²°ê³¼ê°€ None: {result is None}")
                    
                    if result is None:
                        logger.warning(f"ğŸ” DEBUG: {analysis_type} ê²°ê³¼ê°€ None")
                        analysis_status[analysis_type] = 'failed_none_result'
                        failed_due_to_data += 1
                        if analysis_type in essential_analyses:
                            critical_failures.append(f"{analysis_type}: ê²°ê³¼ê°€ None")
                        continue
                    
                    # ìºì‹œëœ ë¶„ì„ ê²°ê³¼ì¸ ê²½ìš° analysis_result ë‚´ë¶€ í™•ì¸
                    if isinstance(result, dict) and 'analysis_result' in result:
                        actual_result = result['analysis_result']
                        logger.info(f"ğŸ” DEBUG: {analysis_type} ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©, analysis_result íƒ€ì…: {type(actual_result)}")
                    else:
                        actual_result = result
                        logger.info(f"ğŸ” DEBUG: {analysis_type} ì§ì ‘ ê²°ê³¼ ì‚¬ìš©")
                    
                    # actual_resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
                    if not isinstance(actual_result, dict):
                        logger.warning(f"ğŸ” DEBUG: {analysis_type} actual_resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(actual_result)}")
                        analysis_status[analysis_type] = 'failed_invalid_format'
                        failed_due_to_data += 1
                        if analysis_type in essential_analyses:
                            critical_failures.append(f"{analysis_type}: ì˜ëª»ëœ ê²°ê³¼ í˜•ì‹")
                        continue
                    
                    # ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    success = actual_result.get('success', False)
                    logger.info(f"ğŸ” DEBUG: {analysis_type} success: {success}")
                    
                    if not success:
                        # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
                        skip_reason = actual_result.get('skip_reason', '')
                        error_msg = actual_result.get('error', '')
                        
                        logger.warning(f"ğŸ” DEBUG: {analysis_type} ì‹¤íŒ¨ - skip_reason: {skip_reason}, error: {error_msg}")
                        
                        if skip_reason in ['insufficient_raw_data', 'no_valid_data', 'insufficient_data']:
                            failed_due_to_data += 1
                            analysis_status[analysis_type] = 'failed_data_insufficient'
                            if analysis_type in essential_analyses:
                                critical_failures.append(f"{analysis_type}: ë°ì´í„° ë¶€ì¡±")
                        elif skip_reason == 'analyzer_disabled':
                            failed_due_to_disabled += 1
                            analysis_status[analysis_type] = 'failed_disabled'
                            if analysis_type in essential_analyses:
                                critical_failures.append(f"{analysis_type}: ë¶„ì„ê¸° ë¹„í™œì„±í™”")
                        else:
                            analysis_status[analysis_type] = 'failed_other'
                            if analysis_type in essential_analyses:
                                critical_failures.append(f"{analysis_type}: {error_msg}")
                    else:
                        analysis_status[analysis_type] = 'success'
                        logger.info(f"ğŸ” DEBUG: {analysis_type} ì„±ê³µìœ¼ë¡œ ë¶„ë¥˜")
                else:
                    # ë¶„ì„ ê²°ê³¼ ìì²´ê°€ ì—†ìŒ
                    logger.warning(f"ğŸ” DEBUG: {analysis_type} í‚¤ê°€ all_analysis_resultsì— ì—†ìŒ")
                    if analysis_type in essential_analyses:
                        critical_failures.append(f"{analysis_type}: ê²°ê³¼ ì—†ìŒ")
                    analysis_status[analysis_type] = 'missing'
            
            # ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ ë¡œì§
            core_success_count = sum(1 for analysis_type in core_analyses 
                                if analysis_status.get(analysis_type) == 'success')
            essential_success_count = sum(1 for analysis_type in essential_analyses 
                                        if analysis_status.get(analysis_type) == 'success')
            
            # ğŸ” ë””ë²„ê¹…: ì„±ê³µ ì¹´ìš´íŠ¸
            logger.info(f"ğŸ” DEBUG: í•µì‹¬ ë¶„ì„ ì„±ê³µ ì¹´ìš´íŠ¸: {core_success_count}/{len(core_analyses)}")
            logger.info(f"ğŸ” DEBUG: í•„ìˆ˜ ë¶„ì„ ì„±ê³µ ì¹´ìš´íŠ¸: {essential_success_count}/{len(essential_analyses)}")
            logger.info(f"ğŸ” DEBUG: ë°ì´í„° ë¶€ì¡± ì‹¤íŒ¨: {failed_due_to_data}")
            logger.info(f"ğŸ” DEBUG: ì¹˜ëª…ì  ì‹¤íŒ¨: {critical_failures}")
            
            # íŒë‹¨ ê¸°ì¤€
            data_sufficient = (
                len(critical_failures) == 0 and  # í•„ìˆ˜ ë¶„ì„ ëª¨ë‘ ì„±ê³µ
                core_success_count >= 2 and      # í•µì‹¬ ë¶„ì„ ìµœì†Œ 2ê°œ ì„±ê³µ
                failed_due_to_data < 4           # ë°ì´í„° ë¶€ì¡± ì‹¤íŒ¨ 4ê°œ ë¯¸ë§Œ
            )
            
            logger.info(f"ğŸ” DEBUG: ìµœì¢… ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨: {data_sufficient}")
            
            # ìƒì„¸ ì •ë³´
            availability_info = {
                'analysis_status': analysis_status,
                'failed_due_to_data': failed_due_to_data,
                'failed_due_to_disabled': failed_due_to_disabled,
                'total_core_analyses': len(core_analyses),
                'core_success_count': core_success_count,
                'essential_success_count': essential_success_count,
                'critical_failures': critical_failures,
                'data_availability_rate': (core_success_count / len(core_analyses) * 100) if core_analyses else 0,
                'decision_viability': 'viable' if data_sufficient else 'not_viable',
                'failure_reasons': []
            }
            
            # ì‹¤íŒ¨ ì´ìœ  ìƒì„¸ ë¶„ì„
            if not data_sufficient:
                if critical_failures:
                    availability_info['failure_reasons'].append(f"í•„ìˆ˜ ë¶„ì„ ì‹¤íŒ¨: {', '.join(critical_failures)}")
                if core_success_count < 2:
                    availability_info['failure_reasons'].append(f"í•µì‹¬ ë¶„ì„ ë¶€ì¡± (ì„±ê³µ: {core_success_count}/5)")
                if failed_due_to_data >= 4:
                    availability_info['failure_reasons'].append(f"ê´‘ë²”ìœ„í•œ ë°ì´í„° ë¶€ì¡± ({failed_due_to_data}ê°œ ë¶„ì„)")
            
            return data_sufficient, availability_info
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ë°ì´í„° ê°€ìš©ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ğŸ” DEBUG: ì—ëŸ¬ ë°œìƒ, all_analysis_results: {all_analysis_results}")
            return False, {
                'error': str(e),
                'decision_viability': 'not_viable',
                'failure_reasons': [f'ê°€ìš©ì„± í™•ì¸ ì˜¤ë¥˜: {str(e)}']
            }







    async def make_final_decision(self, all_analysis_results: Dict) -> Dict:
        """ìµœì¢… íˆ¬ì ê²°ì • ë©”ì¸ í•¨ìˆ˜ - ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€"""
        try:
            logger.info("ìµœì¢… íˆ¬ì ê²°ì • ë¶„ì„ ì‹œì‘")
            
            # ğŸ” ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° ìƒì„¸ í™•ì¸
            logger.info(f"ğŸ” DEBUG: ì „ë‹¬ë°›ì€ ë¶„ì„ ê²°ê³¼ í‚¤ë“¤: {list(all_analysis_results.keys()) if all_analysis_results else 'None'}")
            
            if all_analysis_results:
                for key, value in all_analysis_results.items():
                    if value is None:
                        logger.warning(f"ğŸ” DEBUG: {key} = None")
                    elif isinstance(value, dict):
                        logger.info(f"ğŸ” DEBUG: {key} = dict with keys: {list(value.keys())}")
                        if 'success' in value:
                            logger.info(f"ğŸ” DEBUG: {key}.success = {value.get('success')}")
                        if 'error' in value:
                            logger.warning(f"ğŸ” DEBUG: {key}.error = {value.get('error')}")
                    else:
                        logger.info(f"ğŸ” DEBUG: {key} = {type(value)} (not dict)")
            
            # ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
            data_sufficient, availability_info = self.check_analysis_data_availability(all_analysis_results)
            
            # ğŸ” ë””ë²„ê¹…: ê°€ìš©ì„± í™•ì¸ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
            logger.info(f"ğŸ” DEBUG: ë°ì´í„° ì¶©ë¶„ì„±: {data_sufficient}")
            logger.info(f"ğŸ” DEBUG: ê°€ìš©ì„± ì •ë³´: {availability_info}")
            
            if not data_sufficient:
                failure_summary = "; ".join(availability_info['failure_reasons'])
                logger.warning(f"ìµœì¢… ê²°ì •: ì¤‘ë‹¨ - {failure_summary}")
                
                return {
                    "success": False,
                    "error": f"íˆ¬ì ê²°ì • ì¤‘ë‹¨: {failure_summary}",
                    "analysis_type": "final_decision",
                    "skip_reason": "insufficient_analysis_data",
                    "data_availability": availability_info,
                    "debug_info": {
                        "input_keys": list(all_analysis_results.keys()) if all_analysis_results else [],
                        "input_types": {k: str(type(v)) for k, v in all_analysis_results.items()} if all_analysis_results else {}
                    },
                    "safety_protocol": {
                        "triggered": True,
                        "reason": "minimum_data_requirements_not_met",
                        "recommended_action": "wait_for_data_recovery",
                        "retry_conditions": [
                            "í•„ìˆ˜ ë¶„ì„ (ê¸°ìˆ ì  ë¶„ì„, í¬ì§€ì…˜ ë¶„ì„) ë³µêµ¬",
                            f"í•µì‹¬ ë¶„ì„ ìµœì†Œ 2ê°œ ì´ìƒ ì„±ê³µ (í˜„ì¬: {availability_info['core_success_count']}/5)",
                            "ë°ì´í„° ì†ŒìŠ¤ ë³µêµ¬ ë˜ëŠ” ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹"
                        ]
                    }
                }
            
            # 1. ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•© (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ)
            integrated_data = {
                'position_analysis': all_analysis_results.get('position_analysis', {}),
                'technical_analysis': all_analysis_results.get('technical_analysis', {}),
                'sentiment_analysis': all_analysis_results.get('sentiment_analysis', {}),
                'macro_analysis': all_analysis_results.get('macro_analysis', {}),
                'onchain_analysis': all_analysis_results.get('onchain_analysis', {}),
                'institutional_analysis': all_analysis_results.get('institutional_analysis', {}),
                'current_position': all_analysis_results.get('current_position', {}),
                'integration_timestamp': datetime.now(timezone.utc).isoformat(),
                'data_availability': availability_info
            }
            
            # ğŸ” ë””ë²„ê¹…: í†µí•© ë°ì´í„° í™•ì¸
            logger.info(f"ğŸ” DEBUG: í†µí•©ëœ ë°ì´í„° í‚¤ë“¤: {list(integrated_data.keys())}")
            
            # 2. AI ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ ìµœì¢… ë¶„ì„
            final_result = await self.analyze_with_ai(integrated_data)
            
            logger.info(f"ìµœì¢… íˆ¬ì ê²°ì • ì™„ë£Œ: {final_result.get('final_decision', 'Unknown')}")
            
            return {
                "success": True,
                "result": final_result,
                "analysis_type": "final_decision",
                "integration_summary": {
                    "total_analyses": len([k for k in integrated_data.keys() if k not in ['integration_timestamp', 'data_availability']]),
                    "successful_analyses": len([k for k, v in integrated_data.items() if v.get('success', False)]),
                    "integration_method": "weighted_composite_scoring",
                    "decision_framework": "multi_factor_analysis",
                    "data_availability_rate": availability_info['data_availability_rate']
                }
            }
            
        except Exception as e:
            logger.error(f"ìµœì¢… íˆ¬ì ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ğŸ” DEBUG: ì—ëŸ¬ ë°œìƒ ì‹œì ì˜ all_analysis_results: {all_analysis_results}")
            return {
                "success": False,
                "error": f"ìµœì¢… ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "result": self._get_emergency_decision(),
                "analysis_type": "final_decision"
            }

# ì „ì—­ ìµœì¢… ê²°ì • ì¸ìŠ¤í„´ìŠ¤
_global_decision_maker: Optional[FinalDecisionMaker] = None

def get_final_decision_maker() -> FinalDecisionMaker:
    """ì „ì—­ ìµœì¢… ê²°ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_decision_maker
    if _global_decision_maker is None:
        _global_decision_maker = FinalDecisionMaker()
    return _global_decision_maker

# ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜
async def make_final_investment_decision(all_analysis_results: Dict) -> Dict:
    """ìµœì¢… íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” í•¨ìˆ˜"""
    decision_maker = get_final_decision_maker()
    return await decision_maker.make_final_decision(all_analysis_results)

# ğŸ”§ ê¸°ì¡´ ì™¸ë¶€ í•¨ìˆ˜ì— ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ë²„ì „ ì¶”ê°€
async def make_final_investment_decision_with_scheduler(scheduler) -> Dict:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì—°ë™ ìµœì¢… íˆ¬ì ê²°ì • í•¨ìˆ˜"""
    decision_maker = get_final_decision_maker()
    return await decision_maker.make_final_decision_with_scheduler(scheduler)

def debug_final_decision_mapping(scheduler) -> Dict:
    """ìµœì¢… ê²°ì • ë§¤í•‘ ë””ë²„ê¹… í•¨ìˆ˜"""
    decision_maker = get_final_decision_maker()
    return decision_maker.debug_scheduler_data_mapping(scheduler)


# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    import asyncio
    
    async def test():
        print("ğŸ” ìµœì¢… íˆ¬ì ê²°ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ìš© ë¶„ì„ ê²°ê³¼ (ë”ë¯¸ ë°ì´í„°)
        test_analysis_results = {
            'position_analysis': {
                'success': True,
                'result': {
                    'recommended_action': 'Buy',
                    'confidence': 75
                }
            },
            'technical_analysis': {
                'success': True,
                'result': {
                    'overall_signal': 'Buy',
                    'confidence': 80
                }
            },
            'sentiment_analysis': {
                'success': True,
                'result': {
                    'market_sentiment_score': 65,
                    'investment_recommendation': 'Hold',
                    'confidence': 70
                }
            },
            'macro_analysis': {
                'success': True,
                'result': {
                    'macro_environment_score': 55,
                    'btc_recommendation': 'Hold',
                    'confidence': 65
                }
            },
            'onchain_analysis': {
                'success': True,
                'result': {
                    'onchain_health_score': 72,
                    'investment_signal': 'Buy',
                    'confidence': 78
                }
            },
            'institutional_analysis': {
                'success': True,
                'result': {
                    'institutional_flow_score': 68,
                    'investment_signal': 'Institutional Buy',
                    'confidence': 72
                }
            },
            'current_position': {
                'has_position': False,
                'side': 'none'
            }
        }
        
        result = await make_final_investment_decision(test_analysis_results)
        
        if result['success']:
            print("âœ… ìµœì¢… íˆ¬ì ê²°ì • ì„±ê³µ!")
            decision = result['result']
            print(f"ìµœì¢… ê²°ì •: {decision.get('final_decision', 'Unknown')}")
            print(f"ì‹ ë¢°ë„: {decision.get('decision_confidence', 0):.1f}%")
            print(f"ê¶Œì¥ ì•¡ì…˜: {decision.get('recommended_action', {}).get('action_type', 'N/A')}")
            print(f"í¬ì§€ì…˜ í¬ê¸°: {decision.get('recommended_action', {}).get('position_size', 0):.1f}%")
            print(f"ì¸ê°„ ê²€í†  í•„ìš”: {decision.get('needs_human_review', False)}")
        else:
            print("âŒ ìµœì¢… íˆ¬ì ê²°ì • ì‹¤íŒ¨:")
            print(result['error'])
        
        print("\n" + "="*50)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    asyncio.run(test())