import json
import re
import logging
import requests
from typing import Dict, List, Optional
from datetime import datetime, timezone, timedelta
from google import genai
from google.genai import types
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../'))

from docs.investment_ai.config import CONFIG, API_KEY, MODEL_PRIORITY

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("onchain_analyzer")

# CoinGecko API í‚¤ ì„¤ì •
COINGECKO_API_KEY = os.getenv("GEKCO_API_KEY")

def get_coingecko_headers():
    """CoinGecko API í—¤ë” ìƒì„± (API í‚¤ í¬í•¨)"""
    headers = {'User-Agent': 'trading-bot/1.0'}
    if COINGECKO_API_KEY:
        headers['x-cg-demo-api-key'] = COINGECKO_API_KEY
        logger.debug("CoinGecko API í‚¤ ì ìš©ë¨")
    else:
        logger.warning("CoinGecko API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ë¬´ë£Œ í•œë„ ì ìš©")
    return headers

class OnchainAnalyzer:
    """ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ AI - 4ë‹¨ê³„ (ë¬´ë£Œ API ê¸°ë°˜)"""
    
    def __init__(self):
        self.client = None
        self.model_name = None
        
        # ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì¶”ê°€
        self.error_counts = {
            'blockchain_stats': 0,
            'address_metrics': 0,
            'holder_behavior': 0,
            'mining_metrics': 0,
            'mempool_data': 0
        }
        self.max_errors = 3
        
        # ìºì‹± TTL ì„¤ì • (ì´ˆ ë‹¨ìœ„)
        self.cache_ttl = {
            'whale_movements': 1800,      # 30ë¶„ (ê³ ë˜ ê±°ë˜)
            'exchange_flows': 1800,       # 30ë¶„ (ê±°ë˜ì†Œ ìœ ì…/ìœ ì¶œ)
            'network_metrics': 3600,      # 1ì‹œê°„ (ë„¤íŠ¸ì›Œí¬ ì§€í‘œ)
            'holder_behavior': 3600,      # 1ì‹œê°„ (ë³´ìœ ì í–‰ë™)
            'mining_metrics': 7200,       # 2ì‹œê°„ (ì±„êµ´ ì§€í‘œ)
            'addresses_metrics': 3600,    # 1ì‹œê°„ (ì£¼ì†Œ í™œì„±ë„)
        }
        
        # ë¬´ë£Œ ì˜¨ì²´ì¸ ë°ì´í„° ì†ŒìŠ¤ë“¤
        self.data_sources = {
            # Blockchain.info API - ë¬´ë£Œ (ì œí•œ ìˆìŒ)
            'blockchain_info': {
                'base_url': 'https://api.blockchain.info',
                'endpoints': {
                    'stats': '/stats',
                    'pools': '/pools',
                    'unconfirmed': '/q/unconfirmedcount',
                    'difficulty': '/q/getdifficulty',
                    'hashrate': '/q/hashrate',
                    'market_cap': '/q/marketcap',
                    'total_btc': '/q/totalbc',
                    'avg_block_size': '/q/avgtxsize',
                }
            },
            
            # CoinGecko API - ë¬´ë£Œ (ì˜¨ì²´ì¸ ë°ì´í„° ì¼ë¶€)
            'coingecko': {
                'base_url': 'https://api.coingecko.com/api/v3',
                'endpoints': {
                    'global_data': '/global',
                    'btc_data': '/coins/bitcoin',
                    'btc_market_data': '/coins/bitcoin/market_chart',
                }
            },
            
            # BitcoinVisuals API - ë¬´ë£Œ (ì œí•œì )
            'bitcoin_visuals': {
                'base_url': 'https://api.bitcoinvisuals.com',
                'endpoints': {
                    'addresses': '/addresses',
                    'transactions': '/transactions',
                }
            }
        }
    
    def get_model(self):
        """AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
        if not API_KEY:
            logger.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”ë¯¸ ë¶„ì„ê¸°ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.")
            return None, None
            
        try:
            client = genai.Client(api_key=API_KEY)
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ëª¨ë¸ ì‹œë„
            for model_name in MODEL_PRIORITY:
                try:
                    logger.info(f"ì˜¨ì²´ì¸ ë¶„ì„ ëª¨ë¸ {model_name} ì´ˆê¸°í™” ì„±ê³µ")
                    return client, model_name
                    
                except Exception as e:
                    logger.warning(f"ì˜¨ì²´ì¸ ë¶„ì„ ëª¨ë¸ {model_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue
            
            logger.error("ì˜¨ì²´ì¸ ë¶„ì„ìš© ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return None, None
            
        except Exception as e:
            logger.error(f"ì˜¨ì²´ì¸ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return None, None
    
    def get_blockchain_info_stats(self) -> Dict:
        """Blockchain.infoì—ì„œ ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘ (ìºì‹± ê¶Œì¥: 1ì‹œê°„)"""
        try:
            url = f"{self.data_sources['blockchain_info']['base_url']}/stats"
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                'total_btc_supply': data.get('totalbc', 0) / 100000000,  # Satoshi -> BTC
                'market_cap_usd': data.get('market_price_usd', 0) * (data.get('totalbc', 0) / 100000000),
                'hash_rate': data.get('hash_rate', 0),
                'difficulty': data.get('difficulty', 0),
                'mempool_size': data.get('n_btc_mined', 0),
                'total_transactions': data.get('n_tx', 0),
                'blocks_count': data.get('n_blocks_total', 0),
                'avg_block_size': data.get('avg_block_size', 0),
                'minutes_between_blocks': data.get('minutes_between_blocks', 0),
                'trade_volume_btc': data.get('trade_volume_btc', 0),
                'trade_volume_usd': data.get('trade_volume_usd', 0),
                'timestamp': datetime.now().isoformat(),
                'source': 'blockchain_info',
                'cache_ttl': self.cache_ttl['network_metrics']
            }
            
        except Exception as e:
            logger.error(f"Blockchain.info í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_counts['blockchain_stats'] += 1
            return self._get_cached_blockchain_stats()
    
    def get_mempool_data(self) -> Dict:
        """ë©”ëª¨ë¦¬í’€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹± ê¶Œì¥: 30ë¶„)"""
        try:
            # ë¯¸í™•ì¸ ê±°ë˜ ìˆ˜
            unconfirmed_url = f"{self.data_sources['blockchain_info']['base_url']}/q/unconfirmedcount"
            unconfirmed_response = requests.get(unconfirmed_url, timeout=10)
            unconfirmed_count = int(unconfirmed_response.text) if unconfirmed_response.status_code == 200 else 0
            
            return {
                'unconfirmed_transactions': unconfirmed_count,
                'congestion_level': 'High' if unconfirmed_count > 50000 else 'Medium' if unconfirmed_count > 20000 else 'Low',
                'timestamp': datetime.now().isoformat(),
                'source': 'blockchain_info',
                'cache_ttl': self.cache_ttl['network_metrics']
            }
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬í’€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_counts['mempool_data'] += 1
            return None
    
    def get_address_metrics(self) -> Dict:
        """ì£¼ì†Œ í™œì„±ë„ ì§€í‘œ ìˆ˜ì§‘ (ìºì‹± ê¶Œì¥: 1ì‹œê°„)"""
        try:
            # ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ APIë¥¼ ì¡°í•©í•´ì•¼ í•˜ì§€ë§Œ, ë¬´ë£Œ ì œí•œìœ¼ë¡œ ì¸í•´ ì¶”ì •ì¹˜ ì‚¬ìš©
            # í–¥í›„ ë” ë‚˜ì€ ë¬´ë£Œ API ë°œê²¬ì‹œ êµì²´ ê°€ëŠ¥
            
            # CoinGeckoì—ì„œ Bitcoin ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            url = f"{self.data_sources['coingecko']['base_url']}/coins/bitcoin"
            params = {
                'localization': 'false',
                'tickers': 'false',
                'market_data': 'true',
                'community_data': 'false',
                'developer_data': 'false'
            }
            
            response = requests.get(url, params=params, headers=get_coingecko_headers(), timeout=10)
            response.raise_for_status()
            data = response.json()
            
            market_data = data.get('market_data', {})
            
            # ì¶”ì • ê³„ì‚° (ì‹¤ì œ ì˜¨ì²´ì¸ ë°ì´í„° ëŒ€ì‹ )
            current_price = market_data.get('current_price', {}).get('usd', 50000)
            market_cap = market_data.get('market_cap', {}).get('usd', 0)
            volume_24h = market_data.get('total_volume', {}).get('usd', 0)
            
            # í™œì„± ì£¼ì†Œ ì¶”ì • (ê±°ë˜ëŸ‰ ê¸°ë°˜)
            estimated_active_addresses = min(1000000, max(100000, int(volume_24h / current_price * 100)))
            
            return {
                'estimated_active_addresses': estimated_active_addresses,
                'new_addresses_trend': 'Increasing' if volume_24h > 20000000000 else 'Stable',
                'address_activity_score': min(100, int((volume_24h / 20000000000) * 100)),
                'whale_addresses_estimate': int(estimated_active_addresses * 0.001),  # 0.1% ì¶”ì •
                'retail_addresses_estimate': int(estimated_active_addresses * 0.95),   # 95% ì¶”ì •
                'timestamp': datetime.now().isoformat(),
                'source': 'coingecko_estimation',
                'cache_ttl': self.cache_ttl['addresses_metrics'],
                'note': 'ì‹¤ì œ ì˜¨ì²´ì¸ ë°ì´í„° ëŒ€ì‹  ê±°ë˜ëŸ‰ ê¸°ë°˜ ì¶”ì •ì¹˜'
            }
            
        except Exception as e:
            logger.error(f"ì£¼ì†Œ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_counts['address_metrics'] += 1
            return self._get_cached_address_metrics()
    
    def get_holder_behavior_metrics(self) -> Dict:
        """ë³´ìœ ì í–‰ë™ ë¶„ì„ (ìºì‹± ê¶Œì¥: 1ì‹œê°„)"""
        try:
            # CoinGeckoì—ì„œ ì‹œì¥ ë°ì´í„°ë¡œ ë³´ìœ ì í–‰ë™ ì¶”ì •
            url = f"{self.data_sources['coingecko']['base_url']}/coins/bitcoin/market_chart"
            params = {
                'vs_currency': 'usd',
                'days': '30',
                'interval': 'daily'
            }
            
            response = requests.get(url, params=params, headers=get_coingecko_headers(), timeout=10)
            response.raise_for_status()
            data = response.json()
            
            prices = data.get('prices', [])
            volumes = data.get('total_volumes', [])
            
            if len(prices) >= 7 and len(volumes) >= 7:
                # ìµœê·¼ 7ì¼ í‰ê·  ëŒ€ë¹„ í˜„ì¬ ë³¼ë¥¨ ë¹„êµ
                recent_volumes = [v[1] for v in volumes[-7:]]
                avg_volume = sum(recent_volumes) / len(recent_volumes)
                current_volume = volumes[-1][1] if volumes else avg_volume
                
                # ê°€ê²© ë³€ë™ì„± ê³„ì‚°
                recent_prices = [p[1] for p in prices[-7:]]
                price_volatility = (max(recent_prices) - min(recent_prices)) / min(recent_prices) * 100
                
                # HODL ì¶”ì • ì§€í‘œ
                volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1
                hodl_strength = max(0, min(100, 100 - (volume_ratio * 50)))  # ê±°ë˜ëŸ‰ ë‚®ì„ìˆ˜ë¡ HODL ê°•í•¨
                
                return {
                    'hodl_strength_score': round(hodl_strength, 1),
                    'volume_trend': 'Increasing' if volume_ratio > 1.2 else 'Decreasing' if volume_ratio < 0.8 else 'Stable',
                    'price_volatility_7d': round(price_volatility, 2),
                    'holder_behavior': 'Strong HODL' if hodl_strength > 70 else 'Weak HODL' if hodl_strength < 30 else 'Mixed',
                    'selling_pressure': 'High' if volume_ratio > 1.5 else 'Low' if volume_ratio < 0.7 else 'Medium',
                    'accumulation_phase': hodl_strength > 60 and price_volatility < 10,
                    'timestamp': datetime.now().isoformat(),
                    'source': 'coingecko_analysis',
                    'cache_ttl': self.cache_ttl['holder_behavior']
                }
            else:
                raise ValueError("ì¶©ë¶„í•œ ê°€ê²©/ë³¼ë¥¨ ë°ì´í„° ì—†ìŒ")
                
        except Exception as e:
            logger.error(f"ë³´ìœ ì í–‰ë™ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.error_counts['holder_behavior'] += 1
            return self._get_cached_holder_behavior()
    
    def get_mining_metrics(self) -> Dict:
        """ì±„êµ´ ê´€ë ¨ ì§€í‘œ ìˆ˜ì§‘ (ìºì‹± ê¶Œì¥: 2ì‹œê°„)"""
        try:
            # Blockchain.infoì—ì„œ ì±„êµ´ ê´€ë ¨ ë°ì´í„°
            stats_data = self.get_blockchain_info_stats()
            
            hash_rate = stats_data.get('hash_rate', 0)
            difficulty = stats_data.get('difficulty', 0)
            
            # í•´ì‹œë ˆì´íŠ¸ ë‹¨ìœ„ ë³€í™˜ ìˆ˜ì • (ì •í™•í•œ ë³€í™˜)
            # Blockchain.infoëŠ” hash/s ë‹¨ìœ„ë¡œ ì œê³µ, EH/së¡œ ë³€í™˜
            hash_rate_eh = hash_rate / 1e18 if hash_rate > 0 else 300  # 1 EH/s = 10^18 H/s
            
            # ì±„êµ´ ì§€í‘œ ê³„ì‚°
            mining_difficulty_trend = 'Increasing' if difficulty > 50000000000000 else 'Stable'
            network_security = min(100, max(0, hash_rate_eh / 5 * 100))  # 500 EH/s = 100ì  ê¸°ì¤€
            
            return {
                'hash_rate_eh': round(hash_rate_eh, 2),
                'hash_rate_raw': hash_rate,  # ì›ë³¸ ë°ì´í„°ë„ í¬í•¨
                'difficulty': difficulty,
                'mining_difficulty_trend': mining_difficulty_trend,
                'network_security_score': round(network_security, 1),
                'hash_rate_trend': 'Strong' if hash_rate_eh > 400 else 'Weak' if hash_rate_eh < 200 else 'Moderate',
                'miner_capitulation_risk': 'Low' if hash_rate_eh > 300 else 'High',
                'timestamp': datetime.now().isoformat(),
                'source': 'blockchain_info',
                'cache_ttl': self.cache_ttl['mining_metrics']
            }
            
        except Exception as e:
            logger.error(f"ì±„êµ´ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_counts['mining_metrics'] += 1
            return self._get_cached_mining_metrics()
    
    def _get_cached_blockchain_stats(self) -> Optional[Dict]:
        """MongoDBì—ì„œ ê³¼ê±° ë¸”ë¡ì²´ì¸ í†µê³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            from pymongo import MongoClient
            from datetime import datetime, timezone, timedelta
            
            client = MongoClient("mongodb://mongodb:27017")
            db = client["bitcoin"]
            cache_collection = db["data_cache"]
            
            # ìµœê·¼ 4ì‹œê°„ ì´ë‚´ ë°ì´í„° ì°¾ê¸°
            four_hours_ago = datetime.now(timezone.utc) - timedelta(hours=4)
            
            cached_data = cache_collection.find_one({
                "task_name": "onchain_data",
                "created_at": {"$gte": four_hours_ago}
            }, sort=[("created_at", -1)])
            
            if cached_data and cached_data.get('data'):
                onchain_data = cached_data['data']
                if 'metrics' in onchain_data:
                    metrics = onchain_data['metrics']
                    return {
                        'total_btc_supply': metrics.get('total_btc_supply'),
                        'market_cap_usd': metrics.get('market_cap_usd'),
                        'hash_rate': metrics.get('hash_rate'),
                        'difficulty': metrics.get('difficulty'),
                        'mempool_size': metrics.get('mempool_size'),
                        'total_transactions': metrics.get('transaction_count'),
                        'blocks_count': metrics.get('blocks_count'),
                        'avg_block_size': metrics.get('avg_block_size'),
                        'minutes_between_blocks': metrics.get('minutes_between_blocks'),
                        'trade_volume_btc': metrics.get('trade_volume_btc'),
                        'trade_volume_usd': metrics.get('trade_volume_usd'),
                        'timestamp': cached_data['created_at'].isoformat(),
                        'source': 'cached_data',
                        'cache_ttl': self.cache_ttl['network_metrics']
                    }
            
            logger.warning("ë¸”ë¡ì²´ì¸ í†µê³„: ìºì‹œëœ ë°ì´í„° ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œëœ ë¸”ë¡ì²´ì¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_cached_address_metrics(self) -> Optional[Dict]:
        """MongoDBì—ì„œ ê³¼ê±° ì£¼ì†Œ ì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            from pymongo import MongoClient
            from datetime import datetime, timezone, timedelta
            
            client = MongoClient("mongodb://mongodb:27017")
            db = client["bitcoin"]
            cache_collection = db["data_cache"]
            
            # ìµœê·¼ 2ì‹œê°„ ì´ë‚´ ë°ì´í„° ì°¾ê¸°
            two_hours_ago = datetime.now(timezone.utc) - timedelta(hours=2)
            
            cached_data = cache_collection.find_one({
                "task_name": "onchain_data",
                "created_at": {"$gte": two_hours_ago}
            }, sort=[("created_at", -1)])
            
            if cached_data and cached_data.get('data', {}).get('address_metrics'):
                address_data = cached_data['data']['address_metrics']
                return {
                    'estimated_active_addresses': address_data.get('estimated_active_addresses'),
                    'new_addresses_trend': address_data.get('new_addresses_trend'),
                    'address_activity_score': address_data.get('address_activity_score'),
                    'whale_addresses_estimate': address_data.get('whale_addresses_estimate'),
                    'retail_addresses_estimate': address_data.get('retail_addresses_estimate'),
                    'timestamp': cached_data['created_at'].isoformat(),
                    'source': 'cached_data',
                    'cache_ttl': self.cache_ttl['addresses_metrics']
                }
            
            logger.warning("ì£¼ì†Œ ì§€í‘œ: ìºì‹œëœ ë°ì´í„° ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œëœ ì£¼ì†Œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_cached_holder_behavior(self) -> Optional[Dict]:
        """MongoDBì—ì„œ ê³¼ê±° ë³´ìœ ì í–‰ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            from pymongo import MongoClient
            from datetime import datetime, timezone, timedelta
            
            client = MongoClient("mongodb://mongodb:27017")
            db = client["bitcoin"]
            cache_collection = db["data_cache"]
            
            # ìµœê·¼ 2ì‹œê°„ ì´ë‚´ ë°ì´í„° ì°¾ê¸°
            two_hours_ago = datetime.now(timezone.utc) - timedelta(hours=2)
            
            cached_data = cache_collection.find_one({
                "task_name": "onchain_data",
                "created_at": {"$gte": two_hours_ago}
            }, sort=[("created_at", -1)])
            
            if cached_data and cached_data.get('data', {}).get('holder_behavior'):
                holder_data = cached_data['data']['holder_behavior']
                return {
                    'hodl_strength_score': holder_data.get('hodl_strength_score'),
                    'volume_trend': holder_data.get('volume_trend'),
                    'price_volatility_7d': holder_data.get('price_volatility_7d'),
                    'holder_behavior': holder_data.get('holder_behavior'),
                    'selling_pressure': holder_data.get('selling_pressure'),
                    'accumulation_phase': holder_data.get('accumulation_phase'),
                    'timestamp': cached_data['created_at'].isoformat(),
                    'source': 'cached_data',
                    'cache_ttl': self.cache_ttl['holder_behavior']
                }
            
            logger.warning("ë³´ìœ ì í–‰ë™ ë°ì´í„°: ìºì‹œëœ ë°ì´í„° ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œëœ ë³´ìœ ì ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_cached_mining_metrics(self) -> Optional[Dict]:
        """MongoDBì—ì„œ ê³¼ê±° ì±„êµ´ ì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            from pymongo import MongoClient
            from datetime import datetime, timezone, timedelta
            
            client = MongoClient("mongodb://mongodb:27017")
            db = client["bitcoin"]
            cache_collection = db["data_cache"]
            
            # ìµœê·¼ 4ì‹œê°„ ì´ë‚´ ë°ì´í„° ì°¾ê¸°
            four_hours_ago = datetime.now(timezone.utc) - timedelta(hours=4)
            
            cached_data = cache_collection.find_one({
                "task_name": "onchain_data",
                "created_at": {"$gte": four_hours_ago}
            }, sort=[("created_at", -1)])
            
            if cached_data and cached_data.get('data', {}).get('mining'):
                mining_data = cached_data['data']['mining']
                return {
                    'hash_rate_eh': mining_data.get('hash_rate_eh'),
                    'hash_rate_raw': mining_data.get('hash_rate_raw'),
                    'difficulty': mining_data.get('difficulty'),
                    'mining_difficulty_trend': mining_data.get('mining_difficulty_trend'),
                    'network_security_score': mining_data.get('network_security_score'),
                    'hash_rate_trend': mining_data.get('hash_rate_trend'),
                    'miner_capitulation_risk': mining_data.get('miner_capitulation_risk'),
                    'timestamp': cached_data['created_at'].isoformat(),
                    'source': 'cached_data',
                    'cache_ttl': self.cache_ttl['mining_metrics']
                }
            
            logger.warning("ì±„êµ´ ì§€í‘œ: ìºì‹œëœ ë°ì´í„° ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œëœ ì±„êµ´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def collect_onchain_data(self) -> Dict:
        """ì˜¨ì²´ì¸ ë°ì´í„° ì¢…í•© ìˆ˜ì§‘"""
        try:
            logger.info("ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìˆ˜ì§‘
            onchain_data = {}
            success_count = 0
            total_categories = 5
            
            # 1. ë„¤íŠ¸ì›Œí¬ ê¸°ë³¸ í†µê³„
            try:
                network_stats = self.get_blockchain_info_stats()
                onchain_data['network_stats'] = network_stats
                if 'error' not in network_stats:
                    success_count += 1
                logger.info("âœ… ë„¤íŠ¸ì›Œí¬ í†µê³„ ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                onchain_data['network_stats'] = self._get_cached_blockchain_stats()
            
            # 2. ë©”ëª¨ë¦¬í’€ ë°ì´í„°
            try:
                mempool_data = self.get_mempool_data()
                onchain_data['mempool'] = mempool_data
                if 'error' not in mempool_data:
                    success_count += 1
                logger.info("âœ… ë©”ëª¨ë¦¬í’€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë©”ëª¨ë¦¬í’€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                onchain_data['mempool'] = None
            
            # 3. ì£¼ì†Œ í™œì„±ë„
            try:
                address_metrics = self.get_address_metrics()
                onchain_data['addresses'] = address_metrics
                if 'error' not in address_metrics:
                    success_count += 1
                logger.info("âœ… ì£¼ì†Œ ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì£¼ì†Œ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                onchain_data['addresses'] = self._get_cached_address_metrics()
            
            # 4. ë³´ìœ ì í–‰ë™
            try:
                holder_behavior = self.get_holder_behavior_metrics()
                onchain_data['holder_behavior'] = holder_behavior
                if 'error' not in holder_behavior:
                    success_count += 1
                logger.info("âœ… ë³´ìœ ì í–‰ë™ ë¶„ì„ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë³´ìœ ì í–‰ë™ ë¶„ì„ ì‹¤íŒ¨: {e}")
                onchain_data['holder_behavior'] = self._get_cached_holder_behavior()
            
            # 5. ì±„êµ´ ì§€í‘œ
            try:
                mining_metrics = self.get_mining_metrics()
                onchain_data['mining'] = mining_metrics
                if 'error' not in mining_metrics:
                    success_count += 1
                logger.info("âœ… ì±„êµ´ ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì±„êµ´ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                onchain_data['mining'] = self._get_cached_mining_metrics()
            
            # ìˆ˜ì§‘ í†µê³„
            success_rate = (success_count / total_categories) * 100
            onchain_data['collection_stats'] = {
                'timestamp': datetime.now().isoformat(),
                'total_categories': total_categories,
                'successful_categories': success_count,
                'success_rate': round(success_rate, 1),
                'data_sources': ['blockchain_info', 'coingecko'],
                'cache_recommendations': {
                    'network_stats': f"{self.cache_ttl['network_metrics']}ì´ˆ",
                    'mempool': f"{self.cache_ttl['network_metrics']}ì´ˆ",
                    'addresses': f"{self.cache_ttl['addresses_metrics']}ì´ˆ",
                    'holder_behavior': f"{self.cache_ttl['holder_behavior']}ì´ˆ",
                    'mining': f"{self.cache_ttl['mining_metrics']}ì´ˆ"
                }
            }
            
            logger.info(f"ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_categories} ì„±ê³µ ({success_rate:.1f}%)")
            return onchain_data
            
        except Exception as e:
            logger.error(f"ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return self._get_cached_complete_data()
    
    def check_data_availability(self) -> bool:
        """ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        failed_sources = sum(1 for count in self.error_counts.values() if count >= self.max_errors)
        if failed_sources >= 3:  # 5ê°œ ì†ŒìŠ¤ ì¤‘ 3ê°œ ì´ìƒ ì‹¤íŒ¨ì‹œ ë¶ˆê°€
            return False
        return True
    
    def _get_cached_complete_data(self) -> Optional[Dict]:
        """ì „ì²´ ì‹¤íŒ¨ì‹œ ìºì‹œëœ ë°ì´í„° ì‚¬ìš©"""
        network_stats = self._get_cached_blockchain_stats()
        addresses = self._get_cached_address_metrics()
        holder_behavior = self._get_cached_holder_behavior()
        mining = self._get_cached_mining_metrics()
        
        # ìºì‹œëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if any([network_stats, addresses, holder_behavior, mining]):
            return {
                'network_stats': network_stats,
                'mempool': None,
                'addresses': addresses,
                'holder_behavior': holder_behavior,
                'mining': mining,
                'collection_stats': {
                    'timestamp': datetime.now().isoformat(),
                    'total_categories': 5,
                    'successful_categories': 0,
                    'success_rate': 0.0,
                    'note': 'ëŒ€ë¶€ë¶„ API ì‹¤íŒ¨ - ìºì‹œëœ ë°ì´í„° ì‚¬ìš©'
                }
            }
        return None
    
    def analyze_onchain_signals(self, onchain_data: Dict) -> Dict:
        """ì˜¨ì²´ì¸ ë°ì´í„° ì‹ í˜¸ ë¶„ì„ (ê·œì¹™ ê¸°ë°˜)"""
        try:
            # ë„¤íŠ¸ì›Œí¬ ê±´ê°•ë„ ë¶„ì„ - ì˜¬ë°”ë¥¸ ë°ì´í„° ì ‘ê·¼
            network_stats = onchain_data.get('network_stats', {})
            mining_data = onchain_data.get('mining', {})
            
            # ìˆ˜ì •ëœ ë°ì´í„° ì ‘ê·¼
            hash_rate_eh = mining_data.get('hash_rate_eh', 350)  # miningì—ì„œ ê°€ì ¸ì˜¤ê¸°
            network_security_score = mining_data.get('network_security_score', 85)
            difficulty = mining_data.get('difficulty', 70000000000000)
            
            # ë©”ëª¨ë¦¬í’€ í˜¼ì¡ë„
            mempool = onchain_data.get('mempool', {})
            unconfirmed_txs = mempool.get('unconfirmed_transactions', 25000)
            congestion_level = mempool.get('congestion_level', 'Medium')
            
            # ë³´ìœ ì í–‰ë™
            holder_behavior = onchain_data.get('holder_behavior', {})
            hodl_strength = holder_behavior.get('hodl_strength_score', 65)
            selling_pressure = holder_behavior.get('selling_pressure', 'Medium')
            
            # ì£¼ì†Œ í™œì„±ë„
            addresses = onchain_data.get('addresses', {})
            active_addresses = addresses.get('estimated_active_addresses', 800000)
            activity_score = addresses.get('address_activity_score', 65)
            
            # ì±„êµ´ ì§€í‘œ - ì˜¬ë°”ë¥¸ ì ‘ê·¼
            miner_risk = mining_data.get('miner_capitulation_risk', 'Low')
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (0-100) - ìˆ˜ì •ëœ ë¡œì§
            onchain_score = 0
            
            # ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ (30ì  ë§Œì ) - ìˆ˜ì •ëœ ê¸°ì¤€
            if network_security_score > 80:
                onchain_score += 30
            elif network_security_score > 60:
                onchain_score += 20
            elif network_security_score > 40:
                onchain_score += 15
            elif network_security_score > 20:
                onchain_score += 10
            else:
                onchain_score += 5  # ìµœì†Œ ì ìˆ˜
            
            # HODL ê°•ë„ (25ì  ë§Œì )
            if hodl_strength > 80:
                onchain_score += 25
            elif hodl_strength > 60:
                onchain_score += 18
            elif hodl_strength > 40:
                onchain_score += 12
            elif hodl_strength > 20:
                onchain_score += 6
            else:
                onchain_score += 2
            
            # ë„¤íŠ¸ì›Œí¬ í™œì„±ë„ (20ì  ë§Œì )
            if activity_score > 80:
                onchain_score += 20
            elif activity_score > 60:
                onchain_score += 15
            elif activity_score > 40:
                onchain_score += 10
            elif activity_score > 20:
                onchain_score += 5
            else:
                onchain_score += 2
            
            # ë©”ëª¨ë¦¬í’€ ìƒíƒœ (15ì  ë§Œì )
            if congestion_level == 'Low':
                onchain_score += 15
            elif congestion_level == 'Medium':
                onchain_score += 10
            else:  # High
                onchain_score += 5
            
            # ì±„êµ´ì ë¦¬ìŠ¤í¬ (10ì  ë§Œì )
            if miner_risk == 'Low':
                onchain_score += 10
            elif miner_risk == 'Medium':
                onchain_score += 7
            else:  # High
                onchain_score += 3
            
            # ì‹ í˜¸ ê°•ë„ ë¶„ë¥˜
            if onchain_score >= 85:
                signal_strength = "ë§¤ìš° ê°•í•¨"
                btc_signal = "Strong Buy"
            elif onchain_score >= 70:
                signal_strength = "ê°•í•¨"
                btc_signal = "Buy"
            elif onchain_score >= 50:
                signal_strength = "ì¤‘ë¦½"
                btc_signal = "Hold"
            elif onchain_score >= 30:
                signal_strength = "ì•½í•¨"
                btc_signal = "Weak Sell"
            else:
                signal_strength = "ë§¤ìš° ì•½í•¨"
                btc_signal = "Sell"
            
            return {
                'onchain_score': round(onchain_score, 1),
                'signal_strength': signal_strength,
                'btc_signal': btc_signal,
                'network_health': {
                    'security_level': 'High' if network_security_score > 70 else 'Medium' if network_security_score > 40 else 'Low',
                    'hash_rate_trend': 'Strong' if hash_rate_eh > 400 else 'Weak' if hash_rate_eh < 200 else 'Moderate',
                    'hash_rate_eh': hash_rate_eh,  # ì‹¤ì œ ê°’ í¬í•¨
                    'miner_sentiment': 'Bullish' if miner_risk == 'Low' else 'Bearish' if miner_risk == 'High' else 'Neutral'
                },
                'user_behavior': {
                    'hodl_sentiment': 'Strong' if hodl_strength > 70 else 'Weak' if hodl_strength < 40 else 'Mixed',
                    'selling_pressure_level': selling_pressure,
                    'accumulation_signal': hodl_strength > 60 and selling_pressure in ['Low', 'Medium']
                },
                'network_activity': {
                    'activity_level': 'High' if activity_score > 75 else 'Low' if activity_score < 50 else 'Medium',
                    'congestion_status': congestion_level,
                    'transaction_demand': 'High' if unconfirmed_txs > 40000 else 'Low' if unconfirmed_txs < 15000 else 'Medium'
                },
                'key_signals': self._identify_key_signals(onchain_data, onchain_score),
                'risk_factors': self._identify_onchain_risks(onchain_data),
                'bullish_factors': self._identify_bullish_factors(onchain_data),
                'data_reliability': {
                    'success_rate': onchain_data.get('collection_stats', {}).get('success_rate', 0),
                    'confidence': 'High' if onchain_data.get('collection_stats', {}).get('success_rate', 0) > 80 else 'Medium' if onchain_data.get('collection_stats', {}).get('success_rate', 0) > 50 else 'Low'
                },
                'debug_info': {  # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                    'hash_rate_eh': hash_rate_eh,
                    'network_security_score': network_security_score,
                    'score_breakdown': {
                        'security_points': 30 if network_security_score > 80 else 20 if network_security_score > 60 else 15 if network_security_score > 40 else 10 if network_security_score > 20 else 5,
                        'hodl_points': 25 if hodl_strength > 80 else 18 if hodl_strength > 60 else 12 if hodl_strength > 40 else 6 if hodl_strength > 20 else 2,
                        'activity_points': 20 if activity_score > 80 else 15 if activity_score > 60 else 10 if activity_score > 40 else 5 if activity_score > 20 else 2,
                        'mempool_points': 15 if congestion_level == 'Low' else 10 if congestion_level == 'Medium' else 5,
                        'miner_points': 10 if miner_risk == 'Low' else 7 if miner_risk == 'Medium' else 3
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"ì˜¨ì²´ì¸ ì‹ í˜¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'onchain_score': 50.0,
                'signal_strength': 'ì¤‘ë¦½',
                'btc_signal': 'Hold',
                'error': str(e),
                'data_reliability': {'success_rate': 0, 'confidence': 'None'}
            }

    def _identify_key_signals(self, onchain_data: Dict, score: float) -> List[str]:
        """ì£¼ìš” ì˜¨ì²´ì¸ ì‹ í˜¸ë“¤ ì‹ë³„"""
        signals = []
        
        holder_behavior = onchain_data.get('holder_behavior', {})
        hodl_strength = holder_behavior.get('hodl_strength_score', 65)
        
        if hodl_strength > 75:
            signals.append(f"ê°•í•œ HODL íŒ¨í„´ ({hodl_strength:.1f}) - ì¥ê¸° ë³´ìœ  ì¦ê°€")
        
        mining = onchain_data.get('mining', {})
        hash_rate_eh = mining.get('hash_rate_eh', 350)
        if hash_rate_eh > 400:
            signals.append(f"ë†’ì€ í•´ì‹œë ˆì´íŠ¸ ({hash_rate_eh:.1f} EH/s) - ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ê°•í™”")
        
        addresses = onchain_data.get('addresses', {})
        activity_score = addresses.get('address_activity_score', 65)
        if activity_score > 80:
            signals.append(f"ë†’ì€ ë„¤íŠ¸ì›Œí¬ í™œì„±ë„ ({activity_score}) - ì‚¬ìš©ì ì°¸ì—¬ ì¦ê°€")
        
        mempool = onchain_data.get('mempool', {})
        congestion = mempool.get('congestion_level', 'Medium')
        if congestion == 'Low':
            signals.append("ë‚®ì€ ë„¤íŠ¸ì›Œí¬ í˜¼ì¡ë„ - ê±°ë˜ íš¨ìœ¨ì„± ì–‘í˜¸")
        
        if score > 70:
            signals.append("ì˜¨ì²´ì¸ ì§€í‘œ ì¢…í•©: ê°•ì„¸ ì‹ í˜¸ ìš°ì„¸")
        
        return signals if signals else ["í˜„ì¬ ëšœë ·í•œ ì˜¨ì²´ì¸ ì‹ í˜¸ ì—†ìŒ"]
    
    def _identify_onchain_risks(self, onchain_data: Dict) -> List[str]:
        """ì˜¨ì²´ì¸ ë¦¬ìŠ¤í¬ ìš”ì¸ë“¤"""
        risks = []
        
        mining = onchain_data.get('mining', {})
        miner_risk = mining.get('miner_capitulation_risk', 'Low')
        if miner_risk == 'High':
            risks.append("ì±„êµ´ì í•­ë³µ ìœ„í—˜ - í•´ì‹œë ˆì´íŠ¸ ê¸‰ë½ ê°€ëŠ¥ì„±")
        
        holder_behavior = onchain_data.get('holder_behavior', {})
        selling_pressure = holder_behavior.get('selling_pressure', 'Medium')
        if selling_pressure == 'High':
            risks.append("ë†’ì€ ë§¤ë„ ì••ë ¥ - ëŒ€ëŸ‰ ë¬¼ëŸ‰ ì¶œíšŒ ìœ„í—˜")
        
        mempool = onchain_data.get('mempool', {})
        unconfirmed = mempool.get('unconfirmed_transactions', 25000)
        if unconfirmed > 50000:
            risks.append(f"ì‹¬ê°í•œ ë„¤íŠ¸ì›Œí¬ í˜¼ì¡ ({unconfirmed:,}ê±´) - ê±°ë˜ ì§€ì—°")
        
        network_stats = onchain_data.get('network_stats', {})
        hash_rate_eh = mining.get('hash_rate_eh', 350)
        if hash_rate_eh < 200:
            risks.append(f"ë‚®ì€ í•´ì‹œë ˆì´íŠ¸ ({hash_rate_eh:.1f} EH/s) - ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ì•½í™”")
        
        addresses = onchain_data.get('addresses', {})
        activity_score = addresses.get('address_activity_score', 65)
        if activity_score < 40:
            risks.append(f"ë‚®ì€ ë„¤íŠ¸ì›Œí¬ í™œì„±ë„ ({activity_score}) - ì‚¬ìš©ì ì´íƒˆ")
        
        return risks if risks else ["í˜„ì¬ íŠ¹ë³„í•œ ì˜¨ì²´ì¸ ë¦¬ìŠ¤í¬ ì—†ìŒ"]
    
    def _identify_bullish_factors(self, onchain_data: Dict) -> List[str]:
        """ê°•ì„¸ ìš”ì¸ë“¤"""
        bullish = []
        
        holder_behavior = onchain_data.get('holder_behavior', {})
        accumulation = holder_behavior.get('accumulation_phase', False)
        if accumulation:
            bullish.append("ì¶•ì  ë‹¨ê³„ ì§„ì… - ì¥ê¸° íˆ¬ìì ë§¤ìˆ˜ ì¦ê°€")
        
        mining = onchain_data.get('mining', {})
        security_score = mining.get('network_security_score', 85)
        if security_score > 90:
            bullish.append(f"ë§¤ìš° ë†’ì€ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ({security_score:.1f}) - ì‹ ë¢°ë„ ì¦ê°€")
        
        addresses = onchain_data.get('addresses', {})
        new_addresses_trend = addresses.get('new_addresses_trend', 'Stable')
        if new_addresses_trend == 'Increasing':
            bullish.append("ì‹ ê·œ ì£¼ì†Œ ì¦ê°€ - ìƒˆë¡œìš´ ì‚¬ìš©ì ìœ ì…")
        
        holder_behavior = onchain_data.get('holder_behavior', {})
        hodl_strength = holder_behavior.get('hodl_strength_score', 65)
        if hodl_strength > 80:
            bullish.append(f"ê·¹ê°• HODL ì‹¬ë¦¬ ({hodl_strength:.1f}) - ê³µê¸‰ ë¶€ì¡± ì‹¬í™”")
        
        network_stats = onchain_data.get('network_stats', {})
        hash_rate_eh = mining.get('hash_rate_eh', 350)
        if hash_rate_eh > 450:
            bullish.append(f"ì‚¬ìƒ ìµœê³  í•´ì‹œë ˆì´íŠ¸ ({hash_rate_eh:.1f} EH/s) - ì±„êµ´ì ì‹ ë¢°")
        
        return bullish if bullish else ["í˜„ì¬ íŠ¹ë³„í•œ ê°•ì„¸ ìš”ì¸ ì—†ìŒ"]
    
    async def analyze_with_ai(self, onchain_data: Dict) -> Dict:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¨ì²´ì¸ ë°ì´í„° ì¢…í•© ë¶„ì„"""
        if self.client is None:
            self.client, self.model_name = self.get_model()
        
        if self.client is None:
            logger.warning("AI ëª¨ë¸ì´ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.rule_based_analysis(onchain_data)
        
        try:
            prompt = CONFIG["prompts"]["onchain_analysis"].format(
                onchain_data=json.dumps(onchain_data, ensure_ascii=False, indent=2)
            )
            
            # AI ëª¨ë¸ì— ì§ˆì˜
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(thinking_budget=-1)
                )
            )
            
            # JSON íŒŒì‹±
            result_text = response.text
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                result_json = json.loads(json_match.group(0))
                
                # ë¶„ì„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                result_json['analysis_metadata'] = {
                    'analysis_type': 'ai_based',
                    'data_timestamp': datetime.now(timezone.utc).isoformat(),
                    'model_used': self.model_name,
                    'data_success_rate': onchain_data.get('collection_stats', {}).get('success_rate', 0),
                    'cache_info': onchain_data.get('collection_stats', {}).get('cache_recommendations', {}),
                    'raw_data': onchain_data
                }
                
                return result_json
            else:
                logger.error("AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self.rule_based_analysis(onchain_data)
                
        except Exception as e:
            logger.error(f"AI ì˜¨ì²´ì¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self.rule_based_analysis(onchain_data)
    
    def rule_based_analysis(self, onchain_data: Dict) -> Dict:
        """ê·œì¹™ ê¸°ë°˜ ì˜¨ì²´ì¸ ë¶„ì„ (AI ëª¨ë¸ ì—†ì„ ë•Œ ë°±ì—…)"""
        try:
            # ì˜¨ì²´ì¸ ì‹ í˜¸ ë¶„ì„
            signal_analysis = self.analyze_onchain_signals(onchain_data)
            
            onchain_score = signal_analysis.get('onchain_score', 50.0)
            btc_signal = signal_analysis.get('btc_signal', 'Hold')
            
            # íˆ¬ì ì‹ í˜¸ ë§¤í•‘
            signal_mapping = {
                'Strong Buy': 'Strong Buy',
                'Buy': 'Buy', 
                'Hold': 'Hold',
                'Weak Sell': 'Sell',
                'Sell': 'Strong Sell'
            }
            
            investment_signal = signal_mapping.get(btc_signal, 'Hold')
            
            # ë„¤íŠ¸ì›Œí¬ ê±´ê°•ë„ ë¶„ì„
            network_health = signal_analysis.get('network_health', {})
            security_level = network_health.get('security_level', 'Medium')
            
            # ë³´ìœ ì ì‹¬ë¦¬ ë¶„ì„
            user_behavior = signal_analysis.get('user_behavior', {})
            hodl_sentiment = user_behavior.get('hodl_sentiment', 'Mixed')
            
            # ì±„êµ´ í™˜ê²½ ë¶„ì„
            miner_sentiment = network_health.get('miner_sentiment', 'Neutral')
            
            # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
            key_insights = []
            key_insights.extend(signal_analysis.get('key_signals', [])[:3])
            key_insights.extend(signal_analysis.get('bullish_factors', [])[:2])
            
            result = {
                "onchain_health_score": onchain_score,
                "investment_signal": investment_signal,
                "network_security_analysis": f"ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ìˆ˜ì¤€: {security_level}, ì±„êµ´ì ì‹¬ë¦¬: {miner_sentiment}",
                "holder_sentiment": f"HODL ì‹¬ë¦¬: {hodl_sentiment}, ë§¤ë„ì••ë ¥: {user_behavior.get('selling_pressure_level', 'Medium')}",
                "mining_outlook": f"ì±„êµ´ í™˜ê²½: {network_health.get('hash_rate_trend', 'Moderate')}, ìœ„í—˜ë„: {signal_analysis.get('network_health', {}).get('miner_sentiment', 'Neutral')}",
                "liquidity_flow": f"ë„¤íŠ¸ì›Œí¬ í™œì„±ë„: {signal_analysis.get('network_activity', {}).get('activity_level', 'Medium')}, ê±°ë˜ ìˆ˜ìš”: {signal_analysis.get('network_activity', {}).get('transaction_demand', 'Medium')}",
                "key_insights": key_insights[:5] if key_insights else ["ì˜¨ì²´ì¸ ë°ì´í„°ì—ì„œ íŠ¹ë³„í•œ ì‹ í˜¸ ì—†ìŒ"],
                "risk_assessment": "; ".join(signal_analysis.get('risk_factors', ['í˜„ì¬ íŠ¹ë³„í•œ ë¦¬ìŠ¤í¬ ì—†ìŒ'])[:3]),
                "opportunity_analysis": "; ".join(signal_analysis.get('bullish_factors', ['í˜„ì¬ íŠ¹ë³„í•œ ê¸°íšŒ ì—†ìŒ'])[:3]),
                "confidence": max(50, min(95, int(onchain_score + signal_analysis.get('data_reliability', {}).get('success_rate', 0) * 0.3))),
                "analysis_summary": f"ì˜¨ì²´ì¸ ê±´ê°•ë„ {onchain_score:.1f}ì ìœ¼ë¡œ '{signal_analysis.get('signal_strength', 'ì¤‘ë¦½')}' ì‹ í˜¸. íˆ¬ì ê¶Œì¥: {investment_signal}"
            }
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['analysis_metadata'] = {
                'analysis_type': 'rule_based',
                'data_timestamp': datetime.now(timezone.utc).isoformat(),
                'model_used': 'rule_based_fallback',
                'data_success_rate': onchain_data.get('collection_stats', {}).get('success_rate', 0),
                'cache_info': onchain_data.get('collection_stats', {}).get('cache_recommendations', {}),
                'raw_data': onchain_data
            }
            
            return result
            
        except Exception as e:
            logger.error(f"ê·œì¹™ ê¸°ë°˜ ì˜¨ì²´ì¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "onchain_health_score": 50.0,
                "investment_signal": "Hold",
                "error": str(e),
                "confidence": 0,
                "analysis_summary": f"ì˜¨ì²´ì¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    async def analyze_onchain_data(self) -> Dict:
        """ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
        try:
            logger.info("ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ ì‹œì‘")
            
            # 1. ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘
            onchain_raw_data = self.collect_onchain_data()
            
            # 2. ì˜¨ì²´ì¸ ì‹ í˜¸ ë¶„ì„
            signal_analysis = self.analyze_onchain_signals(onchain_raw_data)
            
            # 3. ë°ì´í„° í†µí•©
            comprehensive_data = {
                'raw_data': onchain_raw_data,
                'signal_analysis': signal_analysis,
                'analysis_timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            # 4. AI ì¢…í•© ë¶„ì„
            analysis_result = await self.analyze_with_ai(comprehensive_data)
            
            logger.info("ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
            
            return {
                "success": True,
                "result": analysis_result,
                "analysis_type": "onchain_data",
                "data_quality": {
                    "success_rate": onchain_raw_data.get('collection_stats', {}).get('success_rate', 0),
                    "total_categories": onchain_raw_data.get('collection_stats', {}).get('total_categories', 0),
                    "successful_categories": onchain_raw_data.get('collection_stats', {}).get('successful_categories', 0),
                    "data_sources": ['blockchain_info', 'coingecko', 'estimated_calculations'],
                    "cache_recommendations": onchain_raw_data.get('collection_stats', {}).get('cache_recommendations', {}),
                    "error_counts": self.error_counts.copy()
                }
            }
            
        except Exception as e:
            logger.error(f"ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "analysis_type": "onchain_data"
            }

# ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜
async def analyze_onchain_data() -> Dict:
    """ì˜¨ì²´ì¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
    analyzer = OnchainAnalyzer()
    return await analyzer.analyze_onchain_data()

# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    import asyncio
    
    async def test():
        print("ğŸ” ì˜¨ì²´ì¸ ë°ì´í„° ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        result = await analyze_onchain_data()
        
        if result['success']:
            print("âœ… ì˜¨ì²´ì¸ ë¶„ì„ ì„±ê³µ!")
            print(f"ë°ì´í„° ì„±ê³µë¥ : {result['data_quality']['success_rate']:.1f}%")
            print(f"ì˜¨ì²´ì¸ ê±´ê°•ë„: {result['result']['onchain_health_score']:.1f}")
            print(f"íˆ¬ì ì‹ í˜¸: {result['result']['investment_signal']}")
            print(f"ìºì‹± ê¶Œì¥ì‚¬í•­:")
            for category, ttl in result['data_quality']['cache_recommendations'].items():
                print(f"  {category}: {ttl}")
        else:
            print("âŒ ì˜¨ì²´ì¸ ë¶„ì„ ì‹¤íŒ¨:")
            print(result['error'])
        
        print("\n" + "="*50)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    asyncio.run(test())