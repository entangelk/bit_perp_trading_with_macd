"""
AI íˆ¬ì ì‹œìŠ¤í…œìš© ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
ê° ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ ìµœì í™”ëœ ìˆ˜ì§‘ ì£¼ê¸° ê´€ë¦¬
"""

import asyncio
import time
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, Optional, Callable, Any
from dataclasses import dataclass
import json
from pymongo import MongoClient

logger = logging.getLogger("data_scheduler")

# AI API ìƒíƒœ ì „ì—­ ë³€ìˆ˜
_ai_api_status = {
    'is_working': True,
    'last_success_time': None,
    'last_failure_time': None,
    'consecutive_failures': 0,
    'last_check_time': None
}

@dataclass
class DataTask:
    """ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… ì •ì˜"""
    name: str
    func: Callable
    interval_minutes: int
    last_run: Optional[datetime] = None
    cache_duration_minutes: int = 0  # 0ì´ë©´ ìºì‹œ ì‚¬ìš© ì•ˆí•¨
    is_running: bool = False
    error_count: int = 0
    max_errors: int = 3
    last_error_time: Optional[datetime] = None  # ë§ˆì§€ë§‰ ì—ëŸ¬ ë°œìƒ ì‹œê°„
    auto_recovery_enabled: bool = True  # ìë™ ë³µêµ¬ í™œì„±í™”
    recovery_interval_hours: int = 2  # ë³µêµ¬ ì‹œë„ ê°„ê²© (ì‹œê°„)

class DataScheduler:
    """ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, main_interval_minutes: int = 15):
        self.main_interval = main_interval_minutes
        self.tasks: Dict[str, DataTask] = {}
        self.running = False
        
        # MongoDB ì—°ê²° ì„¤ì •
        self._setup_mongodb()
        
        # ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ë“¤ ë“±ë¡
        self._register_default_tasks()
    
    def _setup_mongodb(self):
        """MongoDB ì—°ê²° ë° ìºì‹œ ì»¬ë ‰ì…˜ ì„¤ì •"""
        try:
            self.mongo_client = MongoClient("mongodb://mongodb:27017")
            self.database = self.mongo_client["bitcoin"]
            self.cache_collection = self.database["data_cache"]
            
            # ë§Œë£Œ ì‹œê°„ì„ ìœ„í•œ TTL ì¸ë±ìŠ¤ ìƒì„± (expire_at í•„ë“œì— ëŒ€í•´)
            try:
                self.cache_collection.create_index("expire_at", expireAfterSeconds=0)
                logger.info("ë°ì´í„° ìºì‹œ ì»¬ë ‰ì…˜ TTL ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.debug(f"TTL ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")
                
            logger.info("MongoDB ë°ì´í„° ìºì‹œ ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            logger.error(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.mongo_client = None
            self.database = None
            self.cache_collection = None
    
    def _register_default_tasks(self):
        """ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ë“¤ ë“±ë¡"""
        
        # 1. ì°¨íŠ¸ ë°ì´í„° - 15ë¶„ë§ˆë‹¤ (ë©”ì¸ ì£¼ê¸°ì™€ ë™ì¼)
        self.register_task(
            name="chart_data",
            func=self._collect_chart_data,
            interval_minutes=15,
            cache_duration_minutes=5  # 5ë¶„ê°„ ìºì‹œ
        )
        
        # 2. ê³µí¬/íƒìš• ì§€ìˆ˜ - 4ì‹œê°„ë§ˆë‹¤ (í•˜ë£¨ 6ë²ˆ)
        self.register_task(
            name="fear_greed_index",
            func=self._collect_fear_greed_data,
            interval_minutes=240,  # 4ì‹œê°„
            cache_duration_minutes=120  # 2ì‹œê°„ ìºì‹œ
        )
        
        # 3. ë‰´ìŠ¤ ë°ì´í„° - 30ë¶„ë§ˆë‹¤
        self.register_task(
            name="crypto_news",
            func=self._collect_news_data,
            interval_minutes=30,
            cache_duration_minutes=15  # 15ë¶„ ìºì‹œ
        )
        
        # 4. ê±°ì‹œê²½ì œ ë°ì´í„° - 6ì‹œê°„ë§ˆë‹¤ (í•˜ë£¨ 4ë²ˆ)
        self.register_task(
            name="macro_economic",
            func=self._collect_macro_data,
            interval_minutes=360,  # 6ì‹œê°„
            cache_duration_minutes=180  # 3ì‹œê°„ ìºì‹œ
        )
        
        # 5. ì˜¨ì²´ì¸ ë°ì´í„° - 1ì‹œê°„ë§ˆë‹¤
        self.register_task(
            name="onchain_data",
            func=self._collect_onchain_data,
            interval_minutes=60,
            cache_duration_minutes=30  # 30ë¶„ ìºì‹œ
        )
        
        # 6. ê¸°ê´€ íˆ¬ì ë°ì´í„° - 2ì‹œê°„ë§ˆë‹¤
        self.register_task(
            name="institutional_data",
            func=self._collect_institutional_data,
            interval_minutes=120,
            cache_duration_minutes=60  # 1ì‹œê°„ ìºì‹œ
        )
        
        # 7. í¬ì§€ì…˜/ì”ê³  ë°ì´í„° - ì‹¤ì‹œê°„ (ìºì‹œ ì—†ìŒ)
        self.register_task(
            name="position_data",
            func=self._collect_position_data,
            interval_minutes=0,  # í•­ìƒ ì‹¤ì‹œê°„
            cache_duration_minutes=0  # ìºì‹œ ì—†ìŒ
        )
        
        # ========= AI ë¶„ì„ ê²°ê³¼ ì‘ì—…ë“¤ =========
        
        # 8. ì‹œì¥ ê°ì • AI ë¶„ì„ - 30ë¶„ë§ˆë‹¤
        self.register_task(
            name="ai_sentiment_analysis",
            func=self._collect_ai_sentiment_analysis,
            interval_minutes=30,
            cache_duration_minutes=25  # 25ë¶„ ìºì‹œ
        )
        
        # 9. ê¸°ìˆ ì  ë¶„ì„ AI ë¶„ì„ - 15ë¶„ë§ˆë‹¤ (ë©”ì¸ ì£¼ê¸°ì™€ ë™ì¼)
        self.register_task(
            name="ai_technical_analysis",
            func=self._collect_ai_technical_analysis,
            interval_minutes=15,
            cache_duration_minutes=10  # 10ë¶„ ìºì‹œ
        )
        
        # 10. ê±°ì‹œê²½ì œ AI ë¶„ì„ - 6ì‹œê°„ë§ˆë‹¤
        self.register_task(
            name="ai_macro_analysis",
            func=self._collect_ai_macro_analysis,
            interval_minutes=360,  # 6ì‹œê°„
            cache_duration_minutes=300  # 5ì‹œê°„ ìºì‹œ
        )
        
        # 11. ì˜¨ì²´ì¸ AI ë¶„ì„ - 1ì‹œê°„ë§ˆë‹¤
        self.register_task(
            name="ai_onchain_analysis",
            func=self._collect_ai_onchain_analysis,
            interval_minutes=60,
            cache_duration_minutes=50  # 50ë¶„ ìºì‹œ
        )
        
        # 12. ê¸°ê´€íˆ¬ì AI ë¶„ì„ - 2ì‹œê°„ë§ˆë‹¤
        self.register_task(
            name="ai_institutional_analysis",
            func=self._collect_ai_institutional_analysis,
            interval_minutes=120,
            cache_duration_minutes=100  # 100ë¶„ ìºì‹œ
        )
    
    def register_task(self, name: str, func: Callable, interval_minutes: int, 
                     cache_duration_minutes: int = 0):
        """ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… ë“±ë¡"""
        self.tasks[name] = DataTask(
            name=name,
            func=func,
            interval_minutes=interval_minutes,
            cache_duration_minutes=cache_duration_minutes
        )
        logger.info(f"ë°ì´í„° ì‘ì—… ë“±ë¡: {name} (ìˆ˜ì§‘ì£¼ê¸°: {interval_minutes}ë¶„, ìºì‹œ: {cache_duration_minutes}ë¶„)")
    
    def should_run_task(self, task: DataTask) -> bool:
        """ì‘ì—… ì‹¤í–‰ ì—¬ë¶€ íŒë‹¨"""
        if task.is_running:
            return False
        
        # ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” í•­ìƒ ì‹¤í–‰
        if task.interval_minutes == 0:
            return True
        
        # ì²« ì‹¤í–‰
        if task.last_run is None:
            return True
        
        # ì£¼ê¸° í™•ì¸
        time_since_last = datetime.now(timezone.utc) - task.last_run
        return time_since_last.total_seconds() >= task.interval_minutes * 60
    
    def get_cached_data(self, task_name: str) -> Optional[Any]:
        """MongoDBì—ì„œ ìºì‹œëœ ë°ì´í„° ë°˜í™˜"""
        if task_name not in self.tasks:
            return None
        
        task = self.tasks[task_name]
        
        # ìºì‹œ ì‚¬ìš© ì•ˆí•˜ëŠ” ê²½ìš°
        if task.cache_duration_minutes == 0 or self.cache_collection is None:
            return None
        
        try:
            # MongoDBì—ì„œ ìºì‹œ ë°ì´í„° ì¡°íšŒ
            cache_doc = self.cache_collection.find_one({
                "task_name": task_name,
                "expire_at": {"$gt": datetime.now(timezone.utc)}
            })
            
            if cache_doc:
                logger.debug(f"MongoDB ìºì‹œëœ ë°ì´í„° ì‚¬ìš©: {task_name}")
                return cache_doc.get("data")
            
            return None
        except Exception as e:
            logger.error(f"ìºì‹œ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    async def run_task(self, task: DataTask) -> Optional[Any]:
        """ê°œë³„ ì‘ì—… ì‹¤í–‰"""
        try:
            task.is_running = True
            logger.debug(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {task.name}")
            
            start_time = datetime.now()
            result = await task.func()
            end_time = datetime.now()
            
            duration = (end_time - start_time).total_seconds()
            
            # ì„±ê³µ ì‹œ ìºì‹œ ì—…ë°ì´íŠ¸
            if result is not None:
                self._update_cache(task, result)
                task.last_run = datetime.now(timezone.utc)
                task.error_count = 0  # ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                logger.debug(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {task.name} ({duration:.2f}ì´ˆ)")
            else:
                task.error_count += 1
                task.last_error_time = datetime.now(timezone.utc)
                logger.warning(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {task.name} (ì˜¤ë¥˜ {task.error_count}/{task.max_errors})")
            
            return result
            
        except Exception as e:
            task.error_count += 1
            task.last_error_time = datetime.now(timezone.utc)
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {task.name} - {str(e)}")
            return None
        finally:
            task.is_running = False
    
    async def get_data(self, task_name: str) -> Optional[Any]:
        """ë°ì´í„° ìš”ì²­ (ìºì‹œ ìš°ì„ , í•„ìš”ì‹œ ìˆ˜ì§‘)"""
        if task_name not in self.tasks:
            logger.error(f"ë“±ë¡ë˜ì§€ ì•Šì€ ë°ì´í„° ì‘ì—…: {task_name}")
            return None
        
        task = self.tasks[task_name]
        
        # ì—ëŸ¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìŠ¤í‚µ (AI ë¶„ì„ ì‘ì—…ì— ëŒ€í•´ì„œëŠ” ë” ì—„ê²©í•˜ê²Œ ì²˜ë¦¬)
        if task.error_count >= task.max_errors:
            # ìë™ ë³µêµ¬ ì‹œë„
            if self._should_attempt_auto_recovery(task):
                logger.info(f"ìë™ ë³µêµ¬ ì‹œë„: {task_name} (ì—ëŸ¬ í›„ {self._get_hours_since_last_error(task):.1f}ì‹œê°„ ê²½ê³¼)")
                task.error_count = max(0, task.error_count - 1)  # ì—ëŸ¬ ì¹´ìš´íŠ¸ 1 ê°ì†Œ
                task.last_error_time = datetime.now(timezone.utc)  # ë³µêµ¬ ì‹œë„ ì‹œê°„ ê¸°ë¡
                
                # ë³µêµ¬ ì‹œë„ í›„ ë‹¤ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë¯€ë¡œ ê³„ì† ì§„í–‰
            else:
                if task_name.startswith('ai_'):
                    hours_since_error = self._get_hours_since_last_error(task)
                    logger.warning(f"AI ë¶„ì„ ì‘ì—… ë¹„í™œì„±í™” (ì—°ì† {task.error_count}íšŒ ì‹¤íŒ¨): {task_name} - ë‹¤ìŒ ìë™ ë³µêµ¬ê¹Œì§€ {task.recovery_interval_hours - hours_since_error:.1f}ì‹œê°„")
                    # AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‹¤íŒ¨ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
                    return {
                        'analysis_result': {
                            'success': False,
                            'skip_reason': 'analyzer_disabled',
                            'error': f'ì—°ì† {task.error_count}íšŒ ì‹¤íŒ¨ë¡œ ë¶„ì„ê¸° ë¹„í™œì„±í™”',
                            'error_count': task.error_count,
                            'max_errors': task.max_errors,
                            'next_recovery_in_hours': task.recovery_interval_hours - hours_since_error
                        },
                        'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                        'disabled': True
                    }
                else:
                    logger.warning(f"ë°ì´í„° ì‘ì—… ìŠ¤í‚µ (ìµœëŒ€ ì˜¤ë¥˜ íšŸìˆ˜ ì´ˆê³¼): {task_name}")
                    return self.get_cached_data(task_name)  # ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„°ë¼ë„ ë°˜í™˜
        
        # ìºì‹œëœ ë°ì´í„° í™•ì¸
        cached_data = self.get_cached_data(task_name)
        if cached_data is not None:
            return cached_data
        
        # ìˆ˜ì§‘ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self.should_run_task(task):
            return await self.run_task(task)
        else:
            # ìˆ˜ì§‘ ì£¼ê¸°ê°€ ì•„ë‹ˆë©´ ë§ˆì§€ë§‰ ë°ì´í„° ë°˜í™˜
            return self.get_cached_data(task_name)
    
    async def run_scheduled_collections(self):
        """ì˜ˆì •ëœ ìˆ˜ì§‘ ì‘ì—…ë“¤ ì‹¤í–‰"""
        logger.info("ì˜ˆì •ëœ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰")
        
        tasks_to_run = []
        disabled_tasks = []
        
        for task_name, task in self.tasks.items():
            if task.interval_minutes > 0:  # ìŠ¤ì¼€ì¤„ë§ëœ ì‘ì—…ë§Œ
                if task.error_count >= task.max_errors:
                    disabled_tasks.append(task_name)
                elif self.should_run_task(task):
                    tasks_to_run.append((task_name, task))
        
        if disabled_tasks:
            logger.warning(f"ë¹„í™œì„±í™”ëœ ì‘ì—…ë“¤ (ìµœëŒ€ ì˜¤ë¥˜ ì´ˆê³¼): {disabled_tasks}")
        
        if not tasks_to_run:
            logger.debug("ì‹¤í–‰í•  ì˜ˆì • ì‘ì—… ì—†ìŒ")
            return
        
        logger.info(f"ì‹¤í–‰í•  ì‘ì—…: {[name for name, _ in tasks_to_run]}")
        
        # ë³‘ë ¬ ì‹¤í–‰
        await asyncio.gather(*[self.run_task(task) for _, task in tasks_to_run])
    
    def _update_cache(self, task: DataTask, data: Any):
        """MongoDBì— ìºì‹œ ë°ì´í„° ì €ì¥"""
        if task.cache_duration_minutes == 0 or self.cache_collection is None:
            return
        
        try:
            expire_at = datetime.now(timezone.utc) + timedelta(minutes=task.cache_duration_minutes)
            
            # upsertë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆë¡œ ì‚½ì…
            self.cache_collection.replace_one(
                {"task_name": task.name},
                {
                    "task_name": task.name,
                    "data": data,
                    "created_at": datetime.now(timezone.utc),
                    "expire_at": expire_at
                },
                upsert=True
            )
            logger.debug(f"MongoDB ìºì‹œ ì—…ë°ì´íŠ¸: {task.name}")
        except Exception as e:
            logger.error(f"ìºì‹œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def get_task_status(self) -> Dict:
        """ëª¨ë“  ì‘ì—…ì˜ ìƒíƒœ ë°˜í™˜"""
        status = {}
        for task_name, task in self.tasks.items():
            has_cache = False
            cache_age_minutes = 0
            
            # MongoDBì—ì„œ ìºì‹œ ìƒíƒœ í™•ì¸
            if self.cache_collection is not None:
                try:
                    cache_doc = self.cache_collection.find_one({"task_name": task_name})
                    if cache_doc:
                        has_cache = True
                        if cache_doc.get("created_at"):
                            cache_age = datetime.now(timezone.utc) - cache_doc["created_at"]
                            cache_age_minutes = cache_age.total_seconds() / 60
                except Exception as e:
                    logger.error(f"ìºì‹œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            
            # ë³µêµ¬ ì •ë³´ ê³„ì‚°
            hours_since_error = self._get_hours_since_last_error(task)
            is_disabled = task.error_count >= task.max_errors
            can_auto_recover = self._should_attempt_auto_recovery(task)
            
            status[task_name] = {
                'interval_minutes': task.interval_minutes,
                'last_run': task.last_run.isoformat() if task.last_run else None,
                'has_cache': has_cache,
                'is_running': task.is_running,
                'error_count': task.error_count,
                'max_errors': task.max_errors,
                'is_disabled': is_disabled,
                'cache_age_minutes': cache_age_minutes,
                'auto_recovery': {
                    'enabled': task.auto_recovery_enabled,
                    'last_error_time': task.last_error_time.isoformat() if task.last_error_time else None,
                    'hours_since_error': round(hours_since_error, 1),
                    'recovery_interval_hours': task.recovery_interval_hours,
                    'can_recover_now': can_auto_recover,
                    'next_recovery_in_hours': max(0, task.recovery_interval_hours - hours_since_error) if is_disabled else 0
                }
            }
        
        return status
    
    def reset_task_errors(self, task_name: str) -> bool:
        """íŠ¹ì • ì‘ì—…ì˜ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹ (ìˆ˜ë™ ë³µêµ¬ìš©)"""
        if task_name in self.tasks:
            old_count = self.tasks[task_name].error_count
            self.tasks[task_name].error_count = 0
            logger.info(f"ì‘ì—… ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {task_name} ({old_count} â†’ 0)")
            return True
        return False
    
    def reset_all_errors(self) -> int:
        """ëª¨ë“  ì‘ì—…ì˜ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        reset_count = 0
        for task_name, task in self.tasks.items():
            if task.error_count > 0:
                task.error_count = 0
                reset_count += 1
        logger.info(f"ëª¨ë“  ì‘ì—… ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {reset_count}ê°œ ì‘ì—…")
        return reset_count
    
    def _should_attempt_auto_recovery(self, task: DataTask) -> bool:
        """ìë™ ë³µêµ¬ ì‹œë„ ì—¬ë¶€ íŒë‹¨"""
        if not task.auto_recovery_enabled:
            return False
        
        if task.last_error_time is None:
            return False
        
        hours_since_error = self._get_hours_since_last_error(task)
        return hours_since_error >= task.recovery_interval_hours
    
    def _get_hours_since_last_error(self, task: DataTask) -> float:
        """ë§ˆì§€ë§‰ ì—ëŸ¬ ì´í›„ ê²½ê³¼ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)"""
        if task.last_error_time is None:
            return 0
        
        time_diff = datetime.now(timezone.utc) - task.last_error_time
        return time_diff.total_seconds() / 3600
    
    def get_recovery_status(self) -> Dict:
        """ìë™ ë³µêµ¬ ìƒíƒœ í™•ì¸"""
        recovery_info = {
            'disabled_tasks': [],
            'recovering_tasks': [],
            'healthy_tasks': [],
            'next_recovery_times': {}
        }
        
        for task_name, task in self.tasks.items():
            if task.error_count >= task.max_errors:
                hours_since_error = self._get_hours_since_last_error(task)
                time_to_recovery = max(0, task.recovery_interval_hours - hours_since_error)
                
                if time_to_recovery <= 0:
                    recovery_info['recovering_tasks'].append(task_name)
                else:
                    recovery_info['disabled_tasks'].append(task_name)
                    recovery_info['next_recovery_times'][task_name] = time_to_recovery
            else:
                recovery_info['healthy_tasks'].append(task_name)
        
        return recovery_info
    
    def force_recovery_attempt(self, task_name: str) -> bool:
        """íŠ¹ì • ì‘ì—…ì˜ ê°•ì œ ë³µêµ¬ ì‹œë„"""
        if task_name not in self.tasks:
            return False
        
        task = self.tasks[task_name]
        if task.error_count >= task.max_errors:
            old_count = task.error_count
            task.error_count = max(0, task.error_count - 2)  # ê°•ì œ ë³µêµ¬ëŠ” 2 ê°ì†Œ
            task.last_error_time = datetime.now(timezone.utc)
            logger.info(f"ê°•ì œ ë³µêµ¬ ì‹œë„: {task_name} (ì—ëŸ¬ ì¹´ìš´íŠ¸: {old_count} â†’ {task.error_count})")
            return True
        
        return False
    
    # ============= ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ =============
    
    async def _collect_chart_data(self):
        """ì°¨íŠ¸ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ê¸°ì¡´ ì°¨íŠ¸ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ì‚¬ìš©
            from docs.get_chart import chart_update_one
            result, server_time, execution_time = chart_update_one('15m', 'BTCUSDT')
            return {
                'success': result is not None,
                'server_time': server_time,
                'execution_time': execution_time,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"ì°¨íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def _collect_fear_greed_data(self):
        """ê³µí¬/íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘"""
        try:
            import requests
            response = requests.get("https://api.alternative.me/fng/?limit=7", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return {
                    'data': data,
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
        except Exception as e:
            logger.error(f"ê³µí¬/íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return None
    
    async def _collect_news_data(self):
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            import feedparser
            
            news_sources = {
                'cointelegraph': 'https://cointelegraph.com/rss',
                'coindesk': 'https://www.coindesk.com/arc/outboundfeeds/rss/',
            }
            
            all_news = []
            for source_name, rss_url in news_sources.items():
                try:
                    feed = feedparser.parse(rss_url)
                    for entry in feed.entries[:5]:  # ìµœì‹  5ê°œë§Œ
                        title = entry.get('title', '').lower()
                        if any(keyword in title for keyword in ['bitcoin', 'btc', 'crypto']):
                            all_news.append({
                                'title': entry.get('title', ''),
                                'summary': entry.get('summary', '')[:200],
                                'source': source_name,
                                'published_time': getattr(entry, 'published', ''),
                                'link': entry.get('link', '')
                            })
                except Exception as e:
                    logger.warning(f"{source_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
            return {
                'news': all_news,
                'count': len(all_news),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return None
    
    async def _collect_macro_data(self):
        """ê±°ì‹œê²½ì œ ë°ì´í„° ìˆ˜ì§‘ (ë”ë¯¸ ë°ì´í„°)"""
        # ì‹¤ì œë¡œëŠ” ê²½ì œ ì§€í‘œ APIë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
        return {
            'indicators': {
                'dxy': 103.5,  # ë‹¬ëŸ¬ì§€ìˆ˜
                'gold': 2650,  # ê¸ˆ ê°€ê²©
                'sp500': 4500,  # S&P 500
                'interest_rate': 5.25  # ê¸°ì¤€ê¸ˆë¦¬
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    async def _collect_onchain_data(self):
        """ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ (ë”ë¯¸ ë°ì´í„°)"""
        # ì‹¤ì œë¡œëŠ” ì˜¨ì²´ì¸ ë¶„ì„ APIë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
        return {
            'metrics': {
                'hash_rate': 450000000,  # TH/s
                'difficulty': 72000000000000,
                'active_addresses': 980000,
                'transaction_count': 350000
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    async def _collect_institutional_data(self):
        """ê¸°ê´€ íˆ¬ì ë°ì´í„° ìˆ˜ì§‘ (ë”ë¯¸ ë°ì´í„°)"""
        # ì‹¤ì œë¡œëŠ” ê¸°ê´€ íˆ¬ì ê´€ë ¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
        return {
            'flows': {
                'etf_inflow': 150000000,  # ë‹¬ëŸ¬
                'institutional_holdings': 800000,  # BTC
                'corporate_treasury': 250000  # BTC
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    async def _collect_position_data(self):
        """í¬ì§€ì…˜/ì”ê³  ë°ì´í„° ìˆ˜ì§‘"""
        try:
            from docs.get_current import fetch_investment_status
            balance, positions_json, ledger = fetch_investment_status()
            
            if balance == 'error':
                return None
            
            return {
                'balance': balance,
                'positions': positions_json,
                'ledger': ledger[:10] if ledger else [],  # ìµœê·¼ 10ê°œë§Œ
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"í¬ì§€ì…˜ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return None
    
    # ============= AI ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ =============
    
    async def _collect_ai_sentiment_analysis(self):
        """ì‹œì¥ ê°ì • AI ë¶„ì„ ìˆ˜ì§‘ ë° ì €ì¥"""
        try:
            from docs.investment_ai.analyzers.sentiment_analyzer import analyze_market_sentiment
            
            # ì›ì‹œ ë°ì´í„° í™•ì¸ (ë‰´ìŠ¤, ê³µí¬/íƒìš• ì§€ìˆ˜)
            news_data = self.get_cached_data("crypto_news")
            fear_greed_data = self.get_cached_data("fear_greed_index")
            
            # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
            available_data_sources = 0
            if news_data:
                available_data_sources += 1
            if fear_greed_data:
                available_data_sources += 1
            
            if available_data_sources == 0:
                logger.warning("ê°ì • ë¶„ì„: ëª¨ë“  ì›ì‹œ ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨ - AI ë¶„ì„ ìŠ¤í‚µ")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'insufficient_raw_data',
                        'error': 'ëª¨ë“  ì›ì‹œ ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨ (ë‰´ìŠ¤, ê³µí¬/íƒìš• ì§€ìˆ˜)'
                    },
                    'raw_data_used': {
                        'has_news': False,
                        'has_fear_greed': False
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'skipped': True
                }
            
            # ë°ì´í„° í’ˆì§ˆ í™•ì¸
            data_quality_issues = []
            if not news_data:
                data_quality_issues.append("ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ")
            if not fear_greed_data:
                data_quality_issues.append("ê³µí¬/íƒìš• ì§€ìˆ˜ ì—†ìŒ")
            
            # AI ë¶„ì„ ì‹¤í–‰ (ìµœì†Œ 1ê°œ ë°ì´í„° ì†ŒìŠ¤ ìˆì„ ë•Œë§Œ)
            logger.info(f"ê°ì • ë¶„ì„ ì‹¤í–‰: {available_data_sources}ê°œ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš©")
            analysis_result = await analyze_market_sentiment()
            
            if analysis_result and analysis_result.get('success', False):
                return {
                    'analysis_result': analysis_result,
                    'raw_data_used': {
                        'has_news': news_data is not None,
                        'has_fear_greed': fear_greed_data is not None,
                        'available_sources': available_data_sources,
                        'quality_issues': data_quality_issues
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'data_freshness': {
                        'news_age_minutes': self._get_data_age_minutes("crypto_news"),
                        'fear_greed_age_minutes': self._get_data_age_minutes("fear_greed_index")
                    }
                }
            else:
                logger.error("ê°ì • ë¶„ì„ AI í˜¸ì¶œ ì‹¤íŒ¨")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'ai_analysis_failed',
                        'error': 'AI ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨'
                    },
                    'raw_data_used': {
                        'has_news': news_data is not None,
                        'has_fear_greed': fear_greed_data is not None
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'failed': True
                }
                
        except Exception as e:
            logger.error(f"AI ê°ì • ë¶„ì„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'analysis_result': {
                    'success': False,
                    'skip_reason': 'exception',
                    'error': f'ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'
                },
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'failed': True
            }
    
    async def _collect_ai_technical_analysis(self):
        """ê¸°ìˆ ì  ë¶„ì„ AI ë¶„ì„ ìˆ˜ì§‘ ë° ì €ì¥"""
        try:
            from docs.investment_ai.analyzers.technical_analyzer import analyze_technical_indicators
            
            # ì°¨íŠ¸ ë°ì´í„° í™•ì¸ (í•„ìˆ˜)
            chart_data = self.get_cached_data("chart_data")
            if not chart_data:
                logger.warning("ê¸°ìˆ ì  ë¶„ì„: ì°¨íŠ¸ ë°ì´í„° ì—†ìŒ - AI ë¶„ì„ ìŠ¤í‚µ")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'insufficient_raw_data',
                        'error': 'ì°¨íŠ¸ ë°ì´í„° ì—†ìŒ - ê¸°ìˆ ì  ë¶„ì„ ë¶ˆê°€'
                    },
                    'raw_data_used': {
                        'has_chart': False
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'skipped': True
                }
            
            # ì°¨íŠ¸ ë°ì´í„° í’ˆì§ˆ í™•ì¸
            chart_age = self._get_data_age_minutes("chart_data")
            if chart_age > 30:  # 30ë¶„ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„°
                logger.warning(f"ê¸°ìˆ ì  ë¶„ì„: ì°¨íŠ¸ ë°ì´í„°ê°€ {chart_age:.1f}ë¶„ ì „ ë°ì´í„°ì„")
            
            # AI ë¶„ì„ ì‹¤í–‰
            logger.info("ê¸°ìˆ ì  ë¶„ì„ ì‹¤í–‰: ì°¨íŠ¸ ë°ì´í„° ì‚¬ìš©")
            analysis_result = await analyze_technical_indicators('BTCUSDT', '15m', 300)
            
            if analysis_result and analysis_result.get('success', False):
                return {
                    'analysis_result': analysis_result,
                    'raw_data_used': {
                        'has_chart': True,
                        'chart_quality': 'fresh' if chart_age <= 30 else 'stale'
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'data_freshness': {
                        'chart_age_minutes': chart_age
                    }
                }
            else:
                logger.error("ê¸°ìˆ ì  ë¶„ì„ AI í˜¸ì¶œ ì‹¤íŒ¨")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'ai_analysis_failed',
                        'error': 'AI ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨'
                    },
                    'raw_data_used': {
                        'has_chart': True
                    },
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'failed': True
                }
                
        except Exception as e:
            logger.error(f"AI ê¸°ìˆ ì  ë¶„ì„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'analysis_result': {
                    'success': False,
                    'skip_reason': 'exception',
                    'error': f'ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'
                },
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'failed': True
            }
    
    async def _collect_ai_macro_analysis(self):
        """ê±°ì‹œê²½ì œ AI ë¶„ì„ ìˆ˜ì§‘ ë° ì €ì¥"""
        try:
            from docs.investment_ai.analyzers.macro_analyzer import analyze_macro_economics
            
            # ê±°ì‹œê²½ì œ ë°ì´í„° í™•ì¸ (í•„ìˆ˜)
            macro_data = self.get_cached_data("macro_economic")
            if not macro_data:
                logger.warning("ê±°ì‹œê²½ì œ ë¶„ì„: ë°ì´í„° ì—†ìŒ - AI ë¶„ì„ ìŠ¤í‚µ")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'insufficient_raw_data',
                        'error': 'ê±°ì‹œê²½ì œ ë°ì´í„° ì—†ìŒ'
                    },
                    'raw_data_used': {'has_macro': False},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'skipped': True
                }
            
            # AI ë¶„ì„ ì‹¤í–‰
            logger.info("ê±°ì‹œê²½ì œ ë¶„ì„ ì‹¤í–‰")
            analysis_result = await analyze_macro_economics()
            
            if analysis_result and analysis_result.get('success', False):
                return {
                    'analysis_result': analysis_result,
                    'raw_data_used': {'has_macro': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'data_freshness': {
                        'macro_age_minutes': self._get_data_age_minutes("macro_economic")
                    }
                }
            else:
                logger.error("ê±°ì‹œê²½ì œ ë¶„ì„ AI í˜¸ì¶œ ì‹¤íŒ¨")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'ai_analysis_failed',
                        'error': 'AI ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨'
                    },
                    'raw_data_used': {'has_macro': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'failed': True
                }
                
        except Exception as e:
            logger.error(f"AI ê±°ì‹œê²½ì œ ë¶„ì„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'analysis_result': {
                    'success': False,
                    'skip_reason': 'exception',
                    'error': f'ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'
                },
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'failed': True
            }
    
    async def _collect_ai_onchain_analysis(self):
        """ì˜¨ì²´ì¸ AI ë¶„ì„ ìˆ˜ì§‘ ë° ì €ì¥"""
        try:
            from docs.investment_ai.analyzers.onchain_analyzer import analyze_onchain_data
            
            # ì˜¨ì²´ì¸ ë°ì´í„° í™•ì¸ (í•„ìˆ˜)
            onchain_data = self.get_cached_data("onchain_data")
            if not onchain_data:
                logger.warning("ì˜¨ì²´ì¸ ë¶„ì„: ë°ì´í„° ì—†ìŒ - AI ë¶„ì„ ìŠ¤í‚µ")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'insufficient_raw_data',
                        'error': 'ì˜¨ì²´ì¸ ë°ì´í„° ì—†ìŒ'
                    },
                    'raw_data_used': {'has_onchain': False},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'skipped': True
                }
            
            # AI ë¶„ì„ ì‹¤í–‰
            logger.info("ì˜¨ì²´ì¸ ë¶„ì„ ì‹¤í–‰")
            analysis_result = await analyze_onchain_data()
            
            if analysis_result and analysis_result.get('success', False):
                return {
                    'analysis_result': analysis_result,
                    'raw_data_used': {'has_onchain': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'data_freshness': {
                        'onchain_age_minutes': self._get_data_age_minutes("onchain_data")
                    }
                }
            else:
                logger.error("ì˜¨ì²´ì¸ ë¶„ì„ AI í˜¸ì¶œ ì‹¤íŒ¨")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'ai_analysis_failed',
                        'error': 'AI ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨'
                    },
                    'raw_data_used': {'has_onchain': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'failed': True
                }
                
        except Exception as e:
            logger.error(f"AI ì˜¨ì²´ì¸ ë¶„ì„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'analysis_result': {
                    'success': False,
                    'skip_reason': 'exception',
                    'error': f'ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'
                },
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'failed': True
            }
    
    async def _collect_ai_institutional_analysis(self):
        """ê¸°ê´€íˆ¬ì AI ë¶„ì„ ìˆ˜ì§‘ ë° ì €ì¥"""
        try:
            from docs.investment_ai.analyzers.institution_analyzer import analyze_institutional_flow
            
            # ê¸°ê´€íˆ¬ì ë°ì´í„° í™•ì¸ (í•„ìˆ˜)
            institutional_data = self.get_cached_data("institutional_data")
            if not institutional_data:
                logger.warning("ê¸°ê´€íˆ¬ì ë¶„ì„: ë°ì´í„° ì—†ìŒ - AI ë¶„ì„ ìŠ¤í‚µ")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'insufficient_raw_data',
                        'error': 'ê¸°ê´€íˆ¬ì ë°ì´í„° ì—†ìŒ'
                    },
                    'raw_data_used': {'has_institutional': False},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'skipped': True
                }
            
            # AI ë¶„ì„ ì‹¤í–‰
            logger.info("ê¸°ê´€íˆ¬ì ë¶„ì„ ì‹¤í–‰")
            analysis_result = await analyze_institutional_flow()
            
            if analysis_result and analysis_result.get('success', False):
                return {
                    'analysis_result': analysis_result,
                    'raw_data_used': {'has_institutional': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'data_freshness': {
                        'institutional_age_minutes': self._get_data_age_minutes("institutional_data")
                    }
                }
            else:
                logger.error("ê¸°ê´€íˆ¬ì ë¶„ì„ AI í˜¸ì¶œ ì‹¤íŒ¨")
                return {
                    'analysis_result': {
                        'success': False,
                        'skip_reason': 'ai_analysis_failed',
                        'error': 'AI ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨'
                    },
                    'raw_data_used': {'has_institutional': True},
                    'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                    'failed': True
                }
                
        except Exception as e:
            logger.error(f"AI ê¸°ê´€íˆ¬ì ë¶„ì„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'analysis_result': {
                    'success': False,
                    'skip_reason': 'exception',
                    'error': f'ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'
                },
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'failed': True
            }
    
    def _get_data_age_minutes(self, task_name: str) -> float:
        """íŠ¹ì • ë°ì´í„°ì˜ ìƒì„± ì‹œê°„ìœ¼ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ ê³„ì‚° (ë¶„ ë‹¨ìœ„)"""
        try:
            if self.cache_collection is None:
                return 0
            
            cache_doc = self.cache_collection.find_one({"task_name": task_name})
            if cache_doc and cache_doc.get("created_at"):
                age = datetime.now(timezone.utc) - cache_doc["created_at"]
                return age.total_seconds() / 60
            return 0
        except Exception:
            return 0

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
_global_scheduler: Optional[DataScheduler] = None

def get_data_scheduler() -> DataScheduler:
    """ì „ì—­ ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ë°˜í™˜"""
    global _global_scheduler
    if _global_scheduler is None:
        _global_scheduler = DataScheduler()
    return _global_scheduler

# í¸ì˜ í•¨ìˆ˜ë“¤
async def get_chart_data():
    """ì°¨íŠ¸ ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("chart_data")

async def get_fear_greed_data():
    """ê³µí¬/íƒìš• ì§€ìˆ˜ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("fear_greed_index")

async def get_news_data():
    """ë‰´ìŠ¤ ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("crypto_news")

async def get_macro_data():
    """ê±°ì‹œê²½ì œ ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("macro_economic")

async def get_onchain_data():
    """ì˜¨ì²´ì¸ ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("onchain_data")

async def get_institutional_data():
    """ê¸°ê´€ íˆ¬ì ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("institutional_data")

async def get_position_data():
    """í¬ì§€ì…˜ ë°ì´í„° ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("position_data")

# AI ë¶„ì„ ê²°ê³¼ ìš”ì²­ í•¨ìˆ˜ë“¤
async def get_ai_sentiment_analysis():
    """AI ì‹œì¥ ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("ai_sentiment_analysis")

async def get_ai_technical_analysis():
    """AI ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("ai_technical_analysis")

async def get_ai_macro_analysis():
    """AI ê±°ì‹œê²½ì œ ë¶„ì„ ê²°ê³¼ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("ai_macro_analysis")

async def get_ai_onchain_analysis():
    """AI ì˜¨ì²´ì¸ ë¶„ì„ ê²°ê³¼ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("ai_onchain_analysis")

async def get_ai_institutional_analysis():
    """AI ê¸°ê´€íˆ¬ì ë¶„ì„ ê²°ê³¼ ìš”ì²­"""
    scheduler = get_data_scheduler()
    return await scheduler.get_data("ai_institutional_analysis")

async def run_scheduled_data_collection():
    """ì˜ˆì •ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    scheduler = get_data_scheduler()
    await scheduler.run_scheduled_collections()

def get_data_status():
    """ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ í™•ì¸"""
    scheduler = get_data_scheduler()
    return scheduler.get_task_status()

def get_recovery_status():
    """ìë™ ë³µêµ¬ ìƒíƒœ í™•ì¸"""
    scheduler = get_data_scheduler()
    return scheduler.get_recovery_status()

def force_recovery(task_name: str = None):
    """ê°•ì œ ë³µêµ¬ ì‹¤í–‰ (íŠ¹ì • ì‘ì—… ë˜ëŠ” ì „ì²´)"""
    scheduler = get_data_scheduler()
    if task_name:
        return scheduler.force_recovery_attempt(task_name)
    else:
        return scheduler.reset_all_errors()

def reset_errors(task_name: str = None):
    """ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹ (íŠ¹ì • ì‘ì—… ë˜ëŠ” ì „ì²´)"""
    scheduler = get_data_scheduler()
    if task_name:
        return scheduler.reset_task_errors(task_name)
    else:
        return scheduler.reset_all_errors()

# AI API ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def check_ai_api_status() -> Dict:
    """AI API ìƒíƒœ í™•ì¸"""
    global _ai_api_status
    return _ai_api_status.copy()

def mark_ai_api_success():
    """AI API ì„±ê³µ ì‹œ í˜¸ì¶œ"""
    global _ai_api_status
    _ai_api_status.update({
        'is_working': True,
        'last_success_time': datetime.now(timezone.utc),
        'consecutive_failures': 0,
        'last_check_time': datetime.now(timezone.utc)
    })
    logger.info("AI API ì‘ë™ ìƒíƒœ: ì •ìƒ")

def mark_ai_api_failure():
    """AI API ì‹¤íŒ¨ ì‹œ í˜¸ì¶œ"""
    global _ai_api_status
    _ai_api_status.update({
        'last_failure_time': datetime.now(timezone.utc),
        'consecutive_failures': _ai_api_status['consecutive_failures'] + 1,
        'last_check_time': datetime.now(timezone.utc)
    })
    
    # 3íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ ë¹„ì‘ë™ ìƒíƒœë¡œ ë³€ê²½
    if _ai_api_status['consecutive_failures'] >= 3:
        _ai_api_status['is_working'] = False
        logger.error(f"AI API ë¹„ì‘ë™ ìƒíƒœ: {_ai_api_status['consecutive_failures']}íšŒ ì—°ì† ì‹¤íŒ¨")
    else:
        logger.warning(f"AI API ì‹¤íŒ¨: {_ai_api_status['consecutive_failures']}/3íšŒ")

async def test_ai_api_connection() -> bool:
    """AI API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        # ê°„ë‹¨í•œ AI API í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        from docs.investment_ai.analyzers.sentiment_analyzer import SentimentAnalyzer
        analyzer = SentimentAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ë°ì´í„°ë¡œ AI í˜¸ì¶œ ì‹œë„
        if analyzer.client is None:
            return False
            
        # ë§¤ìš° ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ AI ì—°ê²° í™•ì¸
        test_prompt = "Hello, respond with just 'OK'"
        response = analyzer.client.generate_content(test_prompt)
        
        if response and response.text:
            mark_ai_api_success()
            return True
        else:
            mark_ai_api_failure()
            return False
            
    except Exception as e:
        logger.error(f"AI API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        mark_ai_api_failure()
        return False

def get_ai_api_status_summary() -> Dict:
    """AI API ìƒíƒœ ìš”ì•½ ì •ë³´"""
    status = check_ai_api_status()
    
    return {
        'is_working': status['is_working'],
        'consecutive_failures': status['consecutive_failures'],
        'last_success': status['last_success_time'].isoformat() if status['last_success_time'] else None,
        'last_failure': status['last_failure_time'].isoformat() if status['last_failure_time'] else None,
        'status_text': 'AI API ì •ìƒ ì‘ë™' if status['is_working'] else f'AI API ë¹„ì‘ë™ ({status["consecutive_failures"]}íšŒ ì‹¤íŒ¨)'
    }

# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    async def test():
        print("ğŸ“Š ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        scheduler = get_data_scheduler()
        
        # ìƒíƒœ í™•ì¸
        print("\n=== ì´ˆê¸° ìƒíƒœ ===")
        status = scheduler.get_task_status()
        for task_name, info in status.items():
            print(f"{task_name}: ì£¼ê¸° {info['interval_minutes']}ë¶„, ìºì‹œ {info['cache_age_minutes']:.1f}ë¶„")
        
        # ì°¨íŠ¸ ë°ì´í„° í…ŒìŠ¤íŠ¸
        print("\n=== ì°¨íŠ¸ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
        chart_data = await get_chart_data()
        print(f"ì°¨íŠ¸ ë°ì´í„°: {chart_data is not None}")
        
        # ê³µí¬/íƒìš• ì§€ìˆ˜ í…ŒìŠ¤íŠ¸
        print("\n=== ê³µí¬/íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
        fg_data = await get_fear_greed_data()
        print(f"ê³µí¬/íƒìš• ë°ì´í„°: {fg_data is not None}")
        
        # ì˜ˆì •ëœ ìˆ˜ì§‘ ì‹¤í–‰
        print("\n=== ì˜ˆì •ëœ ìˆ˜ì§‘ ì‹¤í–‰ ===")
        await run_scheduled_data_collection()
        
        # ìµœì¢… ìƒíƒœ
        print("\n=== ìµœì¢… ìƒíƒœ ===")
        final_status = get_data_status()
        for task_name, info in final_status.items():
            cache_status = "ìºì‹œë¨" if info['has_cache'] else "ì—†ìŒ"
            print(f"{task_name}: {cache_status}, ì˜¤ë¥˜ {info['error_count']}íšŒ")
    
    asyncio.run(test())